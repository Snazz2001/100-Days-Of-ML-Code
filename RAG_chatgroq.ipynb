{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHSUr2CDYJJ2hATGkwuK5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Snazz2001/100-Days-Of-ML-Code/blob/master/RAG_chatgroq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNDF6lwgSSM0",
        "outputId": "a2729f36-b2d0-4f21-9bb2-e02640b64f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.4.2-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.2/328.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, pymupdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, groq, dataclasses-json, langchain-text-splitters, langchain-groq, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.1 groq-0.37.1 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-groq-1.1.1 langchain-text-splitters-1.1.0 marshmallow-3.26.1 mypy-extensions-1.1.0 pymupdf-1.26.7 pypdf-6.4.2 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-core langchain-community pypdf pymupdf faiss-cpu sentence-transformers  langchain-groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-text-splitters langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "-iy4UsjSTAWt",
        "outputId": "638a1cea-5e39-4300-f1a4-da0d95c04c88"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.2.1)\n",
            "Collecting langchain-core<2.0.0,>=1.2.0 (from langchain-text-splitters)\n",
            "  Downloading langchain_core-1.2.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.4-py3-none-any.whl (477 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.4/477.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "Successfully installed langchain-core-1.2.4 langchain_openai-1.1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core"
                ]
              },
              "id": "3ab23e0faebc48cfaef270ece06a473d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, textwrap, gdown, torch\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqFs-O4BSY7W",
        "outputId": "27009496-997b-4b72-826b-52e29744e023"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = userdata.get('GROQ_API_KEY')\n",
        "if key:\n",
        "    os.environ['GROQ_API_KEY'] = key"
      ],
      "metadata": {
        "id": "_m1Csmw5SkVL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_name = \"pdf_files\"\n",
        "os.makedirs(directory_name, exist_ok=True)\n",
        "print(f\"Directory '{directory_name}' is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjMWrPTIUmne",
        "outputId": "daa03e5b-fa73-4ba1-a381-39419b331ecb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'pdf_files' is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "links = [\n",
        "    # You can replace with yours\n",
        "     \"https://drive.google.com/file/d/17ujrBMddGf_kOx4yjUKbPa0gUJt8POZT/view?usp=sharing\",  # file\n",
        "     \"https://drive.google.com/file/d/1SYi854OUQoCOWJ2mV-GKDndI8sGTAP4p/view?usp=sharing\",  # file\n",
        "]"
      ],
      "metadata": {
        "id": "RZcpYC9zVa9C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, url in enumerate(links, start=1):\n",
        "    output_path = os.path.join(directory_name, f\"file_{i}.pdf\")  # saves as file_1.pdf, file_2.pdf...\n",
        "    gdown.download(url=url, output=output_path, quiet=False, fuzzy=True)\n",
        "    print(f\"Downloaded: {output_path}\")\n",
        "\n",
        "print(\"✅ All PDF files downloaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVbRyUwBVbEf",
        "outputId": "c1c02ad9-098e-4f58-a1cc-dd96aa7fce8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17ujrBMddGf_kOx4yjUKbPa0gUJt8POZT\n",
            "To: /content/pdf_files/file_1.pdf\n",
            "100%|██████████| 2.27M/2.27M [00:00<00:00, 172MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: pdf_files/file_1.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SYi854OUQoCOWJ2mV-GKDndI8sGTAP4p\n",
            "To: /content/pdf_files/file_2.pdf\n",
            "100%|██████████| 1.35M/1.35M [00:00<00:00, 108MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: pdf_files/file_2.pdf\n",
            "✅ All PDF files downloaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_pdfs(pdf_directory):\n",
        "    \"\"\"Load and extract all PDFs in a directory using LangChain's PyPDFLoader\"\"\"\n",
        "    pdf_dir = Path(pdf_directory)\n",
        "    pdf_files = list(pdf_dir.rglob(\"*.pdf\"))\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF file(s).\")\n",
        "    all_documents = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "        try:\n",
        "            loader = PyPDFLoader(str(pdf_file))\n",
        "            documents = loader.load()  # one Document per page\n",
        "            #metadata\n",
        "            for doc in documents:\n",
        "                doc.metadata.update({\n",
        "                    \"source_file\": pdf_file.name,\n",
        "                    \"file_type\": \"pdf\"\n",
        "                })\n",
        "            all_documents.extend(documents)\n",
        "            print(f\"  ✓ Loaded {len(documents)} page(s)\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error reading {pdf_file.name}: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
        "    return all_documents\n",
        "\n",
        "# PDF Files Directory\n",
        "all_pdf_documents = process_all_pdfs(\"/content/pdf_files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfZnf8mTVbHt",
        "outputId": "ced4541d-241e-421b-cf1f-b43c8035c3f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 PDF file(s).\n",
            "\n",
            "Processing: file_2.pdf\n",
            "  ✓ Loaded 21 page(s)\n",
            "\n",
            "Processing: file_1.pdf\n",
            "  ✓ Loaded 15 page(s)\n",
            "\n",
            "Total documents loaded: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pdf_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRlyzr9fVbK8",
        "outputId": "86429ac5-b853-4941-f87f-bd887be98041"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang1 Hui Chen2∗ Lihao Liu1 Kai Chen1\\nZijia Lin1 Jungong Han3 Guiguang Ding1∗\\n1School of Software, Tsinghua University 2BNRist, Tsinghua University\\n3Department of Automation, Tsinghua University\\nwa22@mails.tsinghua.edu.cn huichen@mail.tsinghua.edu.cn linzijia07@tsinghua.org.cn\\n{louisliu2048,chenkai2010.9,jungonghan77}@gmail.com dinggg@tsinghua.edu.cn\\n/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\\n/uni00000016/uni0000001a/uni00000011/uni00000018\\n/uni00000017/uni00000013/uni00000011/uni00000013\\n/uni00000017/uni00000015/uni00000011/uni00000018\\n/uni00000017/uni00000018/uni00000011/uni00000013\\n/uni00000017/uni0000001a/uni00000011/uni00000018\\n/uni00000018/uni00000013/uni00000011/uni00000013\\n/uni00000018/uni00000015/uni00000011/uni00000018\\n/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c\\n/uni00000016/uni0000001a/uni00000011/uni00000018\\n/uni00000017/uni00000013/uni00000011/uni00000013\\n/uni00000017/uni00000015/uni00000011/uni00000018\\n/uni00000017/uni00000018/uni00000011/uni00000013\\n/uni00000017/uni0000001a/uni00000011/uni00000018\\n/uni00000018/uni00000013/uni00000011/uni00000013\\n/uni00000018/uni00000015/uni00000011/uni00000018\\n/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training\\nof YOLOs, which brings the competitive performance and low inference latency\\nsimultaneously. Moreover, we introduce the holistic efficiency-accuracy driven\\nmodel design strategy for YOLOs. We comprehensively optimize various compo-\\nnents of YOLOs from both the efficiency and accuracy perspectives, which greatly\\nreduces the computational overhead and enhances the capability. The outcome\\nof our effort is a new generation of YOLO series for real-time end-to-end object\\ndetection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves\\nthe state-of-the-art performance and efficiency across various model scales. For\\n∗Corresponding author.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\\narXiv:2405.14458v2  [cs.CV]  30 Oct 2024'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='example, our YOLOv10-S is 1.8 × faster than RT-DETR-R18 under the simi-\\nlar AP on COCO, meanwhile enjoying 2.8× smaller number of parameters and\\nFLOPs. Compared with YOLOv9-C, YOLOv10-B has 46% less latency and 25%\\nfewer parameters for the same performance. Code and models are available at\\nhttps://github.com/THU-MIG/yolov10.\\n1 Introduction\\nReal-time object detection has always been a focal point of research in the area of computer vision,\\nwhich aims to accurately predict the categories and positions of objects in an image under low\\nlatency. It is widely adopted in various practical applications, including autonomous driving [ 3],\\nrobot navigation [12], and object tracking [72], etc. In recent years, researchers have concentrated\\non devising CNN-based object detectors to achieve real-time detection [ 19, 23, 48, 49, 50, 57,\\n13]. Among them, YOLOs have gained increasing popularity due to their adept balance between\\nperformance and efficiency [2, 20, 29, 20, 21, 65, 60, 70, 8, 71, 17, 29]. The detection pipeline of\\nYOLOs consists of two parts: the model forward process and the NMS post-processing. However,\\nboth of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.\\nSpecifically, YOLOs usually employ one-to-many label assignment strategy during training, whereby\\none ground-truth object corresponds to multiple positive samples. Despite yielding superior perfor-\\nmance, this approach necessitates NMS to select the best positive prediction during inference. This\\nslows down the inference speed and renders the performance sensitive to the hyperparameters of NMS,\\nthereby preventing YOLOs from achieving optimal end-to-end deployment [78]. One line to tackle\\nthis issue is to adopt the recently introduced end-to-end DETR architectures [4, 81, 73, 30, 36, 42, 67].\\nFor example, RT-DETR [78] presents an efficient hybrid encoder and uncertainty-minimal query\\nselection, propelling DETRs into the realm of real-time applications. Nevertheless, when considering\\nonly the forward process of model during deployment, the efficiency of the DETRs still has room\\nfor improvements compared with YOLOs. Another line is to explore end-to-end detection for CNN-\\nbased detectors, which typically leverages one-to-one assignment strategies to suppress the redundant\\npredictions [6, 55, 66, 80, 17]. However, they usually introduce additional inference overhead or\\nachieve suboptimal performance for YOLOs.\\nFurthermore, the model architecture design remains a fundamental challenge for YOLOs, which\\nexhibits an important impact on the accuracy and speed [50, 17, 71, 8]. To achieve more efficient\\nand effective model architectures, researchers have explored different design strategies. Various\\nprimary computational units are presented for the backbone to enhance the feature extraction ability,\\nincluding DarkNet [48, 49, 50], CSPNet [2], EfficientRep [29] and ELAN [62, 64], etc. For the neck,\\nPAN [37], BiC [29], GD [60] and RepGFPN [71], etc., are explored to enhance the multi-scale feature\\nfusion. Besides, model scaling strategies [62, 61] and re-parameterization [11, 29] techniques are also\\ninvestigated. While these efforts have achieved notable advancements, a comprehensive inspection for\\nvarious components in YOLOs from both the efficiency and accuracy perspectives is still lacking. As\\na result, there still exists considerable computational redundancy within YOLOs, leading to inefficient\\nparameter utilization and suboptimal efficiency. Besides, the resulting constrained model capability\\nalso leads to inferior performance, leaving ample room for accuracy improvements.\\nIn this work, we aim to address these issues and further advance the accuracy-speed boundaries of\\nYOLOs. We target both the post-processing and the model architecture throughout the detection\\npipeline. To this end, we first tackle the problem of redundant predictions in the post-processing\\nby presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label\\nassignments and consistent matching metric. It allows the model to enjoy rich and harmonious\\nsupervision during training while eliminating the need for NMS during inference, leading to com-\\npetitive performance with high efficiency. Secondly, we propose the holistic efficiency-accuracy\\ndriven model design strategy for the model architecture by performing the comprehensive inspection\\nfor various components in YOLOs. For efficiency, we propose the lightweight classification head,\\nspatial-channel decoupled downsampling, and rank-guided block design, to reduce the manifested\\ncomputational redundancy and achieve more efficient architecture. For accuracy, we explore the\\nlarge-kernel convolution and present the effective partial self-attention module to enhance the model\\ncapability, harnessing the potential for performance improvements under low cost.\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Based on these approaches, we succeed in achieving a new family of real-time end-to-end detectors\\nwith different model scales, i.e., YOLOv10-N / S / M / B / L / X. Extensive experiments on standard\\nbenchmarks for object detection, i.e., COCO [35], demonstrate that our YOLOv10 can significantly\\noutperform previous state-of-the-art models in terms of computation-accuracy trade-offs across\\nvarious model scales. As shown in Fig. 1, our YOLOv10-S / X are 1.8× / 1.3× faster than RT-DETR-\\nR18 / R101, respectively, under the similar performance. Compared with YOLOv9-C, YOLOv10-B\\nachieves a 46% reduction in latency with the same performance. Moreover, YOLOv10 exhibits highly\\nefficient parameter utilization. Our YOLOv10-L / X outperforms YOLOv8-L / X by 0.3 AP and\\n0.5 AP, with 1.8× and 2.3× smaller number of parameters, respectively. YOLOv10-M achieves the\\nsimilar AP compared with YOLOv9-M / YOLO-MS, with 23% / 31% fewer parameters, respectively.\\nWe hope that our work can inspire further studies and advancements in the field.\\n2 Related Work\\nReal-time object detectors. Real-time object detection aims to classify and locate objects under low\\nlatency, which is crucial for real-world applications. Over the past years, substantial efforts have been\\ndirected towards developing efficient detectors [19, 57, 48, 34, 79, 75, 32, 31, 41]. Particularly, the\\nYOLO series [48, 49, 50, 2, 20, 29, 62, 21, 65] stand out as the mainstream ones. YOLOv1, YOLOv2,\\nand YOLOv3 identify the typical detection architecture consisting of three parts, i.e., backbone, neck,\\nand head [48, 49, 50]. YOLOv4 [2] and YOLOv5 [20] introduce the CSPNet [63] design to replace\\nDarkNet [47], coupled with data augmentation strategies, enhanced PAN, and a greater variety of\\nmodel scales, etc. YOLOv6 [29] presents BiC and SimCSPSPPF for neck and backbone, respectively,\\nwith anchor-aided training and self-distillation strategy. YOLOv7 [62] introduces E-ELAN for rich\\ngradient flow path and explores several trainable bag-of-freebies methods. YOLOv8 [21] presents C2f\\nbuilding block for effective feature extraction and fusion. Gold-YOLO [60] provides the advanced\\nGD mechanism to boost the multi-scale feature fusion capability. YOLOv9 [65] proposes GELAN to\\nimprove the architecture and PGI to augment the training process.\\nEnd-to-end object detectors. End-to-end object detection has emerged as a paradigm shift from\\ntraditional pipelines, offering streamlined architectures [53]. DETR [4] introduces the transformer\\narchitecture and adopts Hungarian loss to achieve one-to-one matching prediction, thereby eliminating\\nhand-crafted components and post-processing. Since then, various DETR variants have been proposed\\nto enhance its performance and efficiency [42, 67, 56, 30, 36, 28, 5, 77, 82]. Deformable-DETR [81]\\nleverages multi-scale deformable attention module to accelerate the convergence speed. DINO [73]\\nintegrates contrastive denoising, mix query selection, and look forward twice scheme into DETRs.\\nRT-DETR [78] further designs the efficient hybrid encoder and proposes the uncertainty-minimal\\nquery selection to improve both the accuracy and latency. Another line to achieve end-to-end object\\ndetection is based CNN detectors. Learnable NMS [24] and relation networks [26] present another\\nnetwork to remove duplicated predictions for detectors. OneNet [55] and DeFCN [66] propose one-\\nto-one matching strategies to enable end-to-end object detection with fully convolutional networks.\\nFCOSpss [80] introduces a positive sample selector to choose the optimal sample for prediction.\\n3 Methodology\\n3.1 Consistent Dual Assignments for NMS-free Training\\nDuring training, YOLOs [21, 65, 29, 70] usually leverage TAL [15] to allocate multiple positive sam-\\nples for each instance. The adoption of one-to-many assignment yields plentiful supervisory signals,\\nfacilitating the optimization and achieving superior performance. However, it necessitates YOLOs\\nto rely on the NMS post-processing, which causes the suboptimal inference efficiency for deploy-\\nment. While previous works [55, 66, 80, 6] explore one-to-one matching to suppress the redundant\\npredictions, they usually introduce additional inference overhead or yield suboptimal performance.\\nIn this work, we present a NMS-free training strategy for YOLOs with dual label assignments and\\nconsistent matching metric, achieving both high efficiency and competitive performance.\\nDual label assignments. Unlike one-to-many assignment, one-to-one matching assigns only one\\nprediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak\\nsupervision, which causes suboptimal accuracy and convergence speed [ 82]. Fortunately, this\\ndeficiency can be compensated for by the one-to-many assignment [6]. To achieve this, we introduce\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Backbone PAN\\nRegression\\nClassification\\nOne-to-many Head\\nDual Label Assignments Consistent Match. Metric\\n𝑚 = 𝑠 ⋅ 𝑝𝛼 ⋅ IoU \\u0de0𝑏, 𝑏\\n𝛽\\n \\nRegression\\nClassification\\nOne-to-one Head\\nInput\\n(a) (b)\\n1\\n23\\n4\\n6\\n5\\nFigure 2: (a) Consistent dual assignments for NMS-free training. (b) Frequency of one-to-one\\nassignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs αo2m=0.5 and\\nβo2m=6 by default [21]. For consistency, αo2o=0.5; βo2o=6. For inconsistency, αo2o=0.5; βo2o =2.\\ndual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown\\nin Fig. 2.(a), we incorporate another one-to-one head for YOLOs. It retains the identical structure\\nand adopts the same optimization objectives as the original one-to-many branch but leverages the\\none-to-one matching to obtain label assignments. During training, two heads are jointly optimized\\nwith the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-\\nto-many assignment. During inference, we discard the one-to-many head and utilize the one-to-one\\nhead to make predictions. This enables YOLOs for the end-to-end deployment without incurring any\\nadditional inference cost. Besides, in the one-to-one matching, we adopt the top one selection, which\\nachieves the same performance as Hungarian matching [4] with less extra training time.\\nConsistent matching metric. During assignments, both one-to-one and one-to-many approaches\\nleverage a metric to quantitatively assess the level of concordance between predictions and instances.\\nTo achieve prediction aware matching for both branches, we employ a uniform matching metric,i.e.,\\nm(α, β) = s · pα · IoU(ˆb, b)β, (1)\\nwhere p is the classification score, ˆb and b denote the bounding box of prediction and instance,\\nrespectively. s represents the spatial prior indicating whether the anchor point of prediction is within\\nthe instance [21, 65, 29, 70]. α and β are two important hyperparameters that balance the impact\\nof the semantic prediction task and the location regression task. We denote the one-to-many and\\none-to-one metrics as mo2m=m(αo2m, βo2m) and mo2o=m(αo2o, βo2o), respectively. These metrics\\ninfluence the label assignments and supervision information for the two heads.\\nIn dual label assignments, the one-to-many branch provides much richer supervisory signals than\\none-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that\\nof one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many\\nhead’s optimization. As a result, the one-to-one head can provide improved quality of samples during\\ninference, leading to better performance. To this end, we first analyze the supervision gap between the\\ntwo heads. Due to the randomness during training, we initiate our examination in the beginning with\\ntwo heads initialized with the same values and producing the same predictions, i.e., one-to-one head\\nand one-to-many head generate the same p and IoU for each prediction-instance pair. We note that the\\nregression targets of two branches do not conflict, as matched predictions share the same targets and\\nunmatched predictions are ignored. The supervision gap thus lies in the different classification targets.\\nGiven an instance, we denote its largest IoU with predictions as u∗, and the largest one-to-many and\\none-to-one matching scores as m∗\\no2m and m∗\\no2o, respectively. Suppose that one-to-many branch yields\\nthe positive samples Ω and one-to-one branch selects i-th prediction with the metric mo2o,i=m∗\\no2o, we\\ncan then derive the classification target to2m,j=u∗ · mo2m,j\\nm∗\\no2m\\n≤ u∗ for j ∈ Ω and to2o,i=u∗ · mo2o,i\\nm∗\\no2o\\n=u∗\\nfor task aligned loss as in [21, 65, 29, 70, 15]. The supervision gap between two branches can thus\\nbe derived by the 1-Wasserstein distance [46] of different classification objectives,i.e.,\\nA = to2o,i − I(i ∈ Ω)to2m,i +\\nX\\nk∈Ω\\\\{i}\\nto2m,k, (2)\\nWe can observe that the gap decreases as to2m,i increases, i.e., i ranks higher within Ω. It reaches the\\nminimum when to2m,i=u∗, i.e., i is the best positive sample in Ω, as shown in Fig. 2.(a). To achieve\\nthis, we present the consistent matching metric, i.e., αo2o=r · αo2m and βo2o=r · βo2m, which implies\\nmo2o=mr\\no2m. Therefore, the best positive sample for one-to-many head is also the best for one-to-one\\nhead. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, we\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='take r=1, by default, i.e., αo2o=αo2m and βo2o=βo2m. To verify the improved supervision alignment,\\nwe count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results\\nafter training. As shown in Fig. 2.(b), the alignment is improved under the consistent matching metric.\\nFor a more comprehensive understanding of the mathematical proof, please refer to the appendix.\\nDiscussion with other counter-parts. Similarly, previous works [28, 5, 77, 54, 6, 82, 45] explore the\\ndifferent assignments to accelerate the training convergence and improve the performance for different\\nnetworks. For example, H-DETR [28], Group-DETR [5], and MS-DETR [77] introduce one-to-many\\nmatching in conjunction with the original one-to-one matching by hybrid or multiple group label\\nassignments, to improve upon DETR. Differently, to achieve the one-to-many matching, they usually\\nintroduce extra queries or repeat ground truths for bipartite matching, or select top several queries\\nfrom the matching scores, while we adopt the prediction aware assignment that incorporates the\\nspatial prior. Besides, LRANet [54] employs the dense assignment and sparse assignment branches\\nfor training, which all belong to the one-to-many assignment, while we adopt the one-to-many and\\none-to-one branches. DEYO [ 45, 43, 44] investigates the step-by-step training with one-to-many\\nmatching in the first stage for convolutional encoder and one-to-one matching in the second stage for\\ntransformer decoder, while we avoid the transformer decoder for end-to-end inference. Compared\\nwith works [6, 80] which incorporate dual assignments for CNN-based detectors, we further analyze\\nthe supervision gap between the two heads and present the consistent matching metric for YOLOs to\\nreduce the theoretical supervision gap. It improves performance through better supervision alignment\\nand eliminates the need for hyper-parameter tuning.\\n3.2 Holistic Efficiency-Accuracy Driven Model Design\\nIn addition to the post-processing, the model architectures of YOLOs also pose great challenges to the\\nefficiency-accuracy trade-offs [50, 8, 29]. Although previous works explore various design strategies,\\nthe comprehensive inspection for various components in YOLOs is still lacking. Consequently, the\\nmodel architecture exhibits non-negligible computational redundancy and constrained capability,\\nwhich impedes its potential for achieving high efficiency and performance. Here, we aim to holistically\\nperform model designs for YOLOs from both efficiency and accuracy perspectives.\\nEfficiency driven model design. The components in YOLO consist of the stem, downsampling\\nlayers, stages with basic building blocks, and the head. The stem incurs few computational cost and\\nwe thus perform efficiency driven model design for other three parts.\\n(1) Lightweight classification head. The classification and regression heads usually share the same\\narchitecture in YOLOs. However, they exhibit notable disparities in computational overhead. For\\nexample, the FLOPs and parameter count of the classification head (5.95G/1.51M) are 2.5× and 2.4×\\nthose of the regression head (2.34G/0.64M) in YOLOv8-S, respectively. However, after analyzing\\nthe impact of classification error and the regression error (seeing Tab. 6), we find that the regression\\nhead undertakes more significance for the performance of YOLOs. Consequently, we can reduce the\\noverhead of classification head without worrying about hurting the performance greatly. Therefore,\\nwe simply adopt a lightweight architecture for the classification head, which consists of two depthwise\\nseparable convolutions [25, 9] with the kernel size of 3×3 followed by a 1×1 convolution.\\n(2) Spatial-channel decoupled downsampling. YOLOs typically leverage regular 3 ×3 standard\\nconvolutions with stride of 2, achieving spatial downsampling (fromH × W to H\\n2 × W\\n2 ) and channel\\ntransformation (from C to 2C) simultaneously. This introduces non-negligible computational cost of\\nO(9\\n2 HW C2) and parameter count ofO(18C2). Instead, we propose to decouple the spatial reduction\\nand channel increase operations, enabling more efficient downsampling. Specifically, we firstly\\nleverage the pointwise convolution to modulate the channel dimension and then utilize the depthwise\\nconvolution to perform spatial downsampling. This reduces the computational cost to O(2HW C2 +\\n9\\n2 HW C) and the parameter count to O(2C2 +18C). Meanwhile, it maximizes information retention\\nduring downsampling, leading to competitive performance with latency reduction.\\n(3) Rank-guided block design. YOLOs usually employ the same basic building block for all stages [29,\\n65], e.g., the bottleneck block in YOLOv8 [21]. To thoroughly examine such homogeneous design for\\nYOLOs, we utilize the intrinsic rank [33, 16] to analyze the redundancy2 of each stage. Specifically,\\nwe calculate the numerical rank of the last convolution in the last basic block in each stage, which\\ncounts the number of singular values larger than a threshold. Fig. 3.(a) presents the results of\\n2A lower rank implies greater redundancy, while a higher rank signifies more condensed information.\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='1×1\\nC\\n×𝑁 +\\n3×3 DW\\n1×1\\n3×3 DW\\n1×1\\n3×3 DW\\nSplit\\n1×1\\nCIB\\nCIB\\n1×1\\nSplit\\nC\\nMHSA\\n+\\nFFN\\n+\\n1×1\\n×𝑁𝑝𝑠𝑎\\n(a) (b) (c)\\nFigure 3: (a) The intrinsic ranks across stages and models in YOLOv8. The stage in the backbone\\nand neck is numbered in the order of model forward process. The numerical rank r is normalized\\nto r/Co for y-axis and its threshold is set to λmax/2, by default, where Co denotes the number of\\noutput channels and λmax is the largest singular value. It can be observed that deep stages and large\\nmodels exhibit lower intrinsic rank values. (b) The compact inverted block (CIB). (c) The partial\\nself-attention module (PSA).\\nYOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This\\nobservation suggests that simply applying the same block design for all stages is suboptimal for the\\nbest capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme\\nwhich aims to decrease the complexity of stages that are shown to be redundant using compact\\narchitecture design. We first present a compact inverted block (CIB) structure, which adopts the cheap\\ndepthwise convolutions for spatial mixing and cost-effective pointwise convolutions for channel\\nmixing, as shown in Fig. 3.(b). It can serve as the efficient basic building block, e.g., embedded in the\\nELAN structure [64, 21] (Fig. 3.(b)). Then, we advocate a rank-guided block allocation strategy to\\nachieve the best efficiency while maintaining competitive capacity. Specifically, given a model, we\\nsort its all stages based on their intrinsic ranks in ascending order. We further inspect the performance\\nvariation of replacing the basic block in the leading stage with CIB. If there is no performance\\ndegradation compared with the given model, we proceed with the replacement of the next stage and\\nhalt the process otherwise. Consequently, we can implement adaptive compact block designs across\\nstages and model scales, achieving higher efficiency without compromising performance. Due to the\\npage limit, we provide the details of the algorithm in the appendix.\\nAccuracy driven model design. We further explore the large-kernel convolution and self-attention\\nfor accuracy driven design, aiming to boost the performance under minimal cost.\\n(1) Large-kernel convolution. Employing large-kernel depthwise convolution is an effective way\\nto enlarge the receptive field and enhance the model’s capability [ 10, 40, 39]. However, simply\\nleveraging them in all stages may introduce contamination in shallow features used for detecting\\nsmall objects, while also introducing significantI/O overhead and latency in high-resolution stages [8].\\nTherefore, we propose to leverage the large-kernel depthwise convolutions in CIB within the deep\\nstages. Specifically, we increase the kernel size of the second 3×3 depthwise convolution in the CIB to\\n7×7, following [39]. Additionally, we employ the structural reparameterization technique [11, 10, 59]\\nto bring another 3 ×3 depthwise convolution branch to alleviate the optimization issue without\\ninference overhead. Furthermore, as the model size increases, its receptive field naturally expands,\\nwith the benefit of using large-kernel convolutions diminishing. Therefore, we only adopt large-kernel\\nconvolution for small model scales.\\n(2) Partial self-attention (PSA). Self-attention [58] is widely employed in various visual tasks due\\nto its remarkable global modeling capability [38, 14, 76]. However, it exhibits high computational\\ncomplexity and memory footprint. To address this, in light of the prevalent attention head redun-\\ndancy [69], we present an efficient partial self-attention (PSA) module design, as shown in Fig. 3.(c).\\nSpecifically, we evenly partition the features across channels into two parts after the 1×1 convolution.\\nWe only feed one part into the NPSA blocks comprised of multi-head self-attention module (MHSA)\\nand feed-forward network (FFN). Two parts are then concatenated and fused by a 1×1 convolution.\\nBesides, we follow [22] to assign the dimensions of the query and key to half of that of the value in\\nMHSA and replace the LayerNorm [1] with BatchNorm [27] for fast inference. Furthermore, PSA is\\nonly placed after the Stage 4 with the lowest resolution, avoiding the excessive overhead from the\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models.\\nLatencyf denotes the latency in the forward process of model without post-processing. † means the\\nresults of YOLOv10 with the original one-to-many training using NMS. All results below are without\\nthe additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\\nModel #Param.(M) FLOPs(G) AP val(%) Latency(ms) Latency f (ms)\\nYOLOv6-3.0-N [29] 4.7 11.4 37.0 2.69 1.76\\nGold-YOLO-N [60] 5.6 12.1 39.6 2.92 1.82\\nYOLOv8-N [21] 3.2 8.7 37.3 6.16 1.77\\nYOLOv10-N (Ours) 2.3 6.7 38.5 / 39.5† 1.84 1.79\\nYOLOv6-3.0-S [29] 18.5 45.3 44.3 3.42 2.35\\nGold-YOLO-S [60] 21.5 46.0 45.4 3.82 2.73\\nYOLO-MS-XS [8] 4.5 17.4 43.4 8.23 2.80\\nYOLO-MS-S [8] 8.1 31.2 46.2 10.12 4.83\\nYOLOv8-S [21] 11.2 28.6 44.9 7.07 2.33\\nYOLOv9-S [65] 7.1 26.4 46.7 - -\\nRT-DETR-R18 [78] 20.0 60.0 46.5 4.58 4.49\\nYOLOv10-S (Ours) 7.2 21.6 46.3 / 46.8† 2.49 2.39\\nYOLOv6-3.0-M [29] 34.9 85.8 49.1 5.63 4.56\\nGold-YOLO-M [60] 41.3 87.5 49.8 6.38 5.45\\nYOLO-MS [8] 22.2 80.2 51.0 12.41 7.30\\nYOLOv8-M [21] 25.9 78.9 50.6 9.50 5.09\\nYOLOv9-M [65] 20.0 76.3 51.1 - -\\nRT-DETR-R34 [78] 31.0 92.0 48.9 6.32 6.21\\nRT-DETR-R50m [78] 36.0 100.0 51.3 6.90 6.84\\nYOLOv10-M (Ours) 15.4 59.1 51.1 / 51.3† 4.74 4.63\\nYOLOv6-3.0-L [29] 59.6 150.7 51.8 9.02 7.90\\nGold-YOLO-L [60] 75.1 151.7 51.8 10.65 9.78\\nYOLOv9-C [65] 25.3 102.1 52.5 10.57 6.13\\nYOLOv10-B (Ours) 19.1 92.0 52.5 / 52.7† 5.74 5.67\\nYOLOv8-L [21] 43.7 165.2 52.9 12.39 8.06\\nRT-DETR-R50 [78] 42.0 136.0 53.1 9.20 9.07\\nYOLOv10-L (Ours) 24.4 120.3 53.2 / 53.4† 7.28 7.21\\nYOLOv8-X [21] 68.2 257.8 53.9 16.86 12.83\\nRT-DETR-R101 [78] 76.0 259.0 54.3 13.71 13.58\\nYOLOv10-X (Ours) 29.5 160.4 54.4 / 54.4† 10.70 10.60\\nquadratic computational complexity of self-attention. In this way, the global representation learning\\nability can be incorporated into YOLOs with low computational costs, which well enhances the\\nmodel’s capability and leads to improved performance.\\n4 Experiments\\n4.1 Implementation Details\\nWe select YOLOv8 [21] as our baseline model, due to its commendable latency-accuracy balance\\nand its availability in various model sizes. We employ the consistent dual assignments for NMS-free\\ntraining and perform holistic efficiency-accuracy driven model design based on it, which brings our\\nYOLOv10 models. YOLOv10 has the same variants as YOLOv8, i.e., N / S / M / L / X. Besides, we\\nderive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We\\nverify the proposed detector on COCO [35] under the same training-from-scratch setting [21, 65, 62].\\nMoreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following [78].\\n4.2 Comparison with state-of-the-arts\\nAs shown in Tab. 1, our YOLOv10 achieves the state-of-the-art performance and end-to-end latency\\nacross various model scales. We first compare YOLOv10 with our baseline models, i.e., YOLOv8.\\nOn N / S / M / L / X five variants, our YOLOv10 achieves 1.2% / 1.4% / 0.5% / 0.3% / 0.5% AP\\nimprovements, with 28% / 36% / 41% / 44% / 57% fewer parameters, 23% / 24% / 25% / 27% / 38%\\nless calculations, and 70% / 65% / 50% / 41% / 37% lower latencies. Compared with other YOLOs,\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\\n# Model NMS-free. Efficiency. Accuracy. #Param.(M) FLOPs(G) AP val(%) Latency(ms)\\n1\\nYOLOv10-S\\n11.2 28.6 44.9 7.07\\n2 ✓ 11.2 28.6 44.3 2.44\\n3 ✓ ✓ 6.2 20.8 44.5 2.31\\n4 ✓ ✓ ✓ 7.2 21.6 46.3 2.49\\n5\\nYOLOv10-M\\n25.9 78.9 50.6 9.50\\n6 ✓ 25.9 78.9 50.3 5.22\\n7 ✓ ✓ 14.1 58.1 50.4 4.57\\n8 ✓ ✓ ✓ 15.4 59.1 51.1 4.74\\nTable 3: Dual assign.\\no2m o2o AP Latency\\n✓ 44.9 7.07\\n✓ 43.4 2.44\\n✓ ✓ 44.3 2.44\\nTable 4: Matching metric.\\nαo2o βo2o APval αo2o βo2o APval\\n0.5 2.0 42.7 0.25 3.0 44.3\\n0.5 4.0 44.2 0.25 6.0 43.5\\n0.5 6.0 44.3 1.0 6.0 43.9\\n0.5 8.0 44.0 1.0 12.0 44.3\\nTable 5: Efficiency. for YOLOv10-S/M.\\n# Model #Param FLOPs AP val Latency\\n1 base. 11.2/25.9 28.6/78.9 44.3/50.3 2.44/5.22\\n2 +cls. 9.9/23.2 23.5/67.7 44.2/50.2 2.39/5.07\\n3 +downs. 8.0/19.7 22.2/65.0 44.4/50.4 2.36/4.97\\n4 +block. 6.2/14.1 20.8/58.1 44.5/50.4 2.31/4.57\\nYOLOv10 also exhibits superior trade-offs between accuracy and computational cost. Specifically,\\nfor lightweight and small models, YOLOv10-N / S outperforms YOLOv6-3.0-N / S by 1.5 AP and 2.0\\nAP, with 51% / 61% fewer parameters and 41% / 52% less computations, respectively. For medium\\nmodels, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46% / 62% latency\\nreduction under the same or better performance, respectively. For large models, compared with\\nGold-YOLO-L, our YOLOv10-L shows 68% fewer parameters and 32% lower latency, along with\\na significant improvement of 1.4% AP. Furthermore, compared with RT-DETR, YOLOv10 obtains\\nsignificant performance and latency improvements. Notably, YOLOv10-S / X achieves 1.8 × and\\n1.3× faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance.\\nThese results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach.\\nWe consider the performance and the latency of model forward process (Latencyf ) in this situation,\\nfollowing [62, 21, 60]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance\\nand efficiency across different model scales, indicating the effectiveness of our architectural designs.\\n4.3 Model Analyses\\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It\\ncan be observed that our NMS-free training with consistent dual assignments significantly reduces\\nthe end-to-end latency of YOLOv10-S by 4.63ms, while maintaining competitive performance of\\n44.3% AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters\\nand 20.8 GFlOPs, with a considerable latency reduction of 0.65ms for YOLOv10-M, well showing\\nits effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements\\nof 1.8 AP and 0.7 AP for YOLOv10-S and YOLOv10-M, alone with only 0.18ms and 0.17ms latency\\noverhead, respectively, which well demonstrates its superiority.\\nAnalyses for NMS-free training.\\n• Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can\\nbring both rich supervision of one-to-many (o2m) branch during training and high efficiency of\\none-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., #1 in\\nTab. 2. Specifically, we introduce baselines for training with only o2m branch and only o2o branch,\\nrespectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\\n• Consistent matching metric. We introduce consistent matching metric to make the one-to-one head\\nmore harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., #1 in\\nTab. 2, under differentαo2o and βo2o. As shown in Tab. 4, the proposed consistent matching metric,\\ni.e., αo2o=r · αo2m and βo2o=r · βo2m, can achieve the optimal performance, where αo2m=0.5 and\\nβo2m=6.0 in the one-to-many head [21]. Such an improvement can be attributed to the reduction\\nof the supervision gap (Eq. (2)), which provides improved supervision alignment between two\\nbranches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive\\nhyper-parameter tuning, which is appealing in practical scenarios.\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10-N\\n YOLOv10-S\\n YOLOv10-M\\n YOLOv10-B\\n YOLOv10-L\\n YOLOv10-X\\n0.2\\n0.3\\n0.4\\nFigure 4: The average cosine similarity of each anchor point’s extracted features with all others.\\nTable 6: cls. results.\\nbase. +cls.\\nAPval 44.3 44.2\\nAPval\\nw/o c 59.9 59.9\\nAPval\\nw/o r 64.5 64.2\\nTable 7: Results of d.s.\\nModel AP val Latency\\nbase. 43.7 2.33\\nours 44.4 2.36\\nTable 8: Results of CIB.\\nModel AP val Latency\\nIRB 43.7 2.30\\nIRB-DW 44.2 2.30\\nours 44.5 2.31\\nTable 9: Rank-guided.\\nStages with CIB AP val\\nempty 44.4\\n8 44.5\\n8,4, 44.5\\n8,4,7 44.3\\n• Performance gap compared with one-to-many training. Although achieving superior end-to-end\\nperformance under NMS-free training, we observe that there still exists the performance gap\\ncompared with the original one-to-many training using NMS, as shown in Tab. 3 and Tab. 1.\\nBesides, we note that the gap diminishes as the model size increases. Therefore, we reasonably\\nconcludes that such a gap can be attributed to the limitations in the model capability. Notably,\\nunlike the original one-to-many training using NMS, the NMS-free training necessitates more\\ndiscriminative features for one-to-one matching. In the case of the YOLOv10-N model, its limited\\ncapacity results in extracted features that lack sufficient discriminability, leading to a more notable\\nperformance gap of 1.0% AP. In contrast, the YOLOv10-X model, which possesses stronger\\ncapability and more discriminative features, shows no performance gap between two training\\nstrategies. In Fig. 4, we visualize the average cosine similarity of each anchor point’s extracted\\nfeatures with those of all other anchor points on the COCO val set. We observe that as the model\\nsize increases, the feature similarity between anchor points exhibits a downward trend, which\\nbenefits the one-to-one matching. Based on this insight, we will explore approaches to further\\nreduce the gap and achieve higher end-to-end performance in the future work.\\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the\\nefficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model\\nwithout efficiency-accuracy driven model design,i.e., #2/#6 in Tab. 2. As shown in Tab. 5, each design\\ncomponent, including lightweight classification head, spatial-channel decoupled downsampling, and\\nrank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency.\\nImportantly, these improvements are achieved while maintaining competitive performance.\\n• Lightweight classification head. We analyze the impact of category and localization errors of\\npredictions on the performance, based on the YOLOv10-S of #1 and #2 in Tab. 5, like [ 7].\\nSpecifically, we match the predictions to the instances by the one-to-one assignment. Then,\\nwe substitute the predicted category score with instance labels, resulting in AP val\\nw/o c with no\\nclassification errors. Similarly, we replace the predicted locations with those of instances, yielding\\nAPval\\nw/o rwith no regression errors. As shown in Tab. 6, AP val\\nw/o ris much higher than AP val\\nw/o c,\\nrevealing that eliminating the regression errors achieves greater improvement. The performance\\nbottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification\\nhead can allow higher efficiency without compromising the performance.\\n• Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency,\\nwhere the channel dimensions are first increased by pointwise convolution (PW) and the resolution\\nis then reduced by depthwise convolution (DW) for maximal information retention. We compare it\\nwith the baseline way of spatial reduction by DW followed by channel modulation by PW, based\\non the YOLOv10-S of #3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the\\n0.7% AP improvement by enjoying less information loss during downsampling.\\n• Compact inverted block (CIB).We introduce CIB as the compact basic building block. We verify its\\neffectiveness based on the YOLOv10-S of #4 in the Tab. 5. Specifically, we introduce the inverted\\nresidual block [51] (IRB) as the baseline, which achieves the suboptimal 43.7% AP, as shown in\\nTab. 8. We then append a 3×3 depthwise convolution (DW) after it, denoted as “IRB-DW”, which\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 10: Accuracy. for S/M.\\n# Model AP val Latency\\n1 base. 44.5/50.4 2.31/4.57\\n2 +L.k. 44.9/- 2.34/-\\n3 +PSA 46.3/51.1 2.49/4.74\\nTable 11: L.k. results.\\nModel AP val Latency\\nk.s.=5 44.7 2.32\\nk.s.=7 44.9 2.34\\nk.s.=9 44.9 2.37\\nw/o rep. 44.8 2.34\\nTable 12: L.k. usage.\\nw/o L.k. w/ L.k.\\nN 36.3 36.6\\nS 44.5 44.9\\nM 50.4 50.4\\nTable 13: PSA results.\\nModel AP val Latency\\nPSA 46.3 2.49\\nTrans. 46.0 2.54\\nNPSA = 1 46.3 2.49\\nNPSA = 2 46.5 2.59\\nbrings 0.5% AP improvement. Compared with “IRB-DW”, our CIB further achieves 0.3% AP\\nimprovement by prepending another DW with minimal overhead, indicating its superiority.\\n• Rank-guided block design. We introduce the rank-guided block design to adaptively integrate\\ncompact block design for improving the model efficiency. We verify its benefit based on the\\nYOLOv10-S of #3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks\\nare Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the\\nbottleneck block in each stage with the efficient CIB, we observe the performance degradation\\nstarting from Stage 7. In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can\\nthus adopt the efficient block design without compromising the performance. These results indicate\\nthat rank-guided block design can serve as an effective strategy for higher model efficiency.\\nAnalyses for accuracy driven model design. We present the results of gradually integrating the\\naccuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model\\nafter incorporating efficiency driven design, i.e., #3/#7 in Tab. 2. As shown in Tab. 10, the adoption\\nof large-kernel convolution and PSA module leads to the considerable performance improvements\\nof 0.4% AP and 1.4% AP for YOLOv10-S under minimal latency increase of 0.03ms and 0.15ms,\\nrespectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\\n• Large-kernel convolution. We first investigate the effect of different kernel sizes based on the\\nYOLOv10-S of #2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size\\nincreases and stagnates around the kernel size of 7×7, indicating the benefit of large perception field.\\nBesides, removing the reparameterization branch during training achieves 0.1% AP degradation,\\nshowing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel\\nconvolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no\\nimprovements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We\\nthus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\\n• Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the\\nglobal modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10-\\nS of #3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN,\\nas the baseline, denoted as “Trans.”. As shown in Tab. 13, compared with it, PSA brings 0.3% AP\\nimprovement with 0.05ms latency reduction. The performance enhancement may be attributed to\\nthe alleviation of optimization problem [68, 10] in self-attention, by mitigating the redundancy\\nin attention heads. Moreover, we investigate the impact of different NPSA. As shown in Tab. 13,\\nincreasing NPSA to 2 obtains 0.2% AP improvement but with 0.1ms latency overhead. Therefore,\\nwe set NPSA to 1, by default, to enhance the model capability while maintaining high efficiency.\\n5 Conclusion\\nIn this paper, we target both the post-processing and model architecture throughout the detection\\npipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMS-\\nfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the\\nholistic efficiency-accuracy driven model design strategy, improving the performance-efficiency trade-\\noffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments\\nshow that YOLOv10 achieves the state-of-the-art performance and latency compared with other\\nadvanced detectors, well demonstrating its superiority.\\n6 Acknowledgments\\nThis work was supported by National Natural Science Foundation of China (Nos. 61925107,\\n62271281) and Beijing Natural Science Foundation (No. L223023).\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed\\nand accuracy of object detection, 2020.\\n[3] Daniel Bogdoll, Maximilian Nitsche, and J Marius Zöllner. Anomaly detection in autonomous\\ndriving: A survey. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 4488–4499, 2022.\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In European conference on\\ncomputer vision, pages 213–229. Springer, 2020.\\n[5] Qiang Chen, Xiaokang Chen, Jian Wang, Shan Zhang, Kun Yao, Haocheng Feng, Junyu Han,\\nErrui Ding, Gang Zeng, and Jingdong Wang. Group detr: Fast detr training with group-wise one-\\nto-many assignment. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision, pages 6633–6642, 2023.\\n[6] Yiqun Chen, Qiang Chen, Qinghao Hu, and Jian Cheng. Date: Dual assignment for end-to-end\\nfully convolutional object detection. arXiv preprint arXiv:2211.13859, 2022.\\n[7] Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, and Jian Cheng. Enhancing\\nyour trained detrs with box refinement. arXiv preprint arXiv:2307.11828, 2023.\\n[8] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng.\\nYolo-ms: rethinking multi-scale representation learning for real-time object detection. arXiv\\npreprint arXiv:2308.05480, 2023.\\n[9] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceed-\\nings of the IEEE conference on computer vision and pattern recognition , pages 1251–1258,\\n2017.\\n[10] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to\\n31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition, pages 11963–11975, 2022.\\n[11] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun.\\nRepvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pages 13733–13742, 2021.\\n[12] Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel\\nFernando Tello Gamarra. Mobile robot navigation using an object recognition software with\\nrgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):1290–1305, 2019.\\n[13] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet:\\nKeypoint triplets for object detection. In Proceedings of the IEEE/CVF international conference\\non computer vision, pages 6569–6578, 2019.\\n[14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\\nimage synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 12873–12883, 2021.\\n[15] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. Tood: Task-aligned\\none-stage object detection. In 2021 IEEE/CVF International Conference on Computer Vision\\n(ICCV), pages 3490–3499. IEEE Computer Society, 2021.\\n[16] Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha.\\nRank diminishing in deep neural networks. Advances in Neural Information Processing Systems,\\n35:33054–33065, 2022.\\n[17] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in\\n2021. arXiv preprint arXiv:2107.08430, 2021.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[18] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V\\nLe, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance\\nsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 2918–2928, 2021.\\n[19] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer\\nvision, pages 1440–1448, 2015.\\n[20] Jocher Glenn. Yolov5 release v7.0.https: // github. com/ ultralytics/ yolov5/ tree/\\nv7. 0, 2022.\\n[21] Jocher Glenn. Yolov8. https: // github. com/ ultralytics/ ultralytics/ tree/\\nmain , 2023.\\n[22] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé\\nJégou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference.\\nIn Proceedings of the IEEE/CVF international conference on computer vision, pages 12259–\\n12269, 2021.\\n[23] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of\\nthe IEEE international conference on computer vision, pages 2961–2969, 2017.\\n[24] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition , pages\\n4507–4515, 2017.\\n[25] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias\\nWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural\\nnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\\n[26] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 3588–3597, 2018.\\n[27] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\\nby reducing internal covariate shift. In International conference on machine learning, pages\\n448–456. pmlr, 2015.\\n[28] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang,\\nand Han Hu. Detrs with hybrid matching. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition, pages 19702–19712, 2023.\\n[29] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming\\nXu, and Xiangxiang Chu. Yolov6 v3.0: A full-scale reloading.arXiv preprint arXiv:2301.05586,\\n2023.\\n[30] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate\\ndetr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition, pages 13619–13627, 2022.\\n[31] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss\\nv2: Learning reliable localization quality estimation for dense object detection. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 11632–11641,\\n2021.\\n[32] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang.\\nGeneralized focal loss: Learning qualified and distributed bounding boxes for dense object\\ndetection. Advances in Neural Information Processing Systems, 33:21002–21012, 2020.\\n[33] Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design\\nfor gpu-efficient networks. arXiv preprint arXiv:2006.14090, 2020.\\n[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense\\nobject detection. In Proceedings of the IEEE international conference on computer vision ,\\npages 2980–2988, 2017.\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13, pages 740–755. Springer, 2014.\\n[36] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang.\\nDab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329,\\n2022.\\n[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for\\ninstance segmentation. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 8759–8768, 2018.\\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\\nof the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\\n[39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\\nXie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition, pages 11976–11986, 2022.\\n[40] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive\\nfield in deep convolutional neural networks. Advances in neural information processing systems,\\n29, 2016.\\n[41] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong\\nZhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors.\\narXiv preprint arXiv:2212.07784, 2022.\\n[42] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and\\nJingdong Wang. Conditional detr for fast training convergence. InProceedings of the IEEE/CVF\\ninternational conference on computer vision, pages 3651–3660, 2021.\\n[43] Haodong Ouyang. Deyov2: Rank feature with greedy matching for end-to-end object detection.\\narXiv preprint arXiv:2306.09165, 2023.\\n[44] Haodong Ouyang. Deyov3: Detr with yolo for real-time object detection. arXiv preprint\\narXiv:2309.11851, 2023.\\n[45] Haodong Ouyang. Deyo: Detr with yolo for end-to-end object detection. arXiv preprint\\narXiv:2402.16370, 2024.\\n[46] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review\\nof statistics and its application, 6:405–431, 2019.\\n[47] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/\\ndarknet/, 2013–2016.\\n[48] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\\nreal-time object detection. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), June 2016.\\n[49] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), July 2017.\\n[50] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.\\n[51] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\\nMobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 4510–4520, 2018.\\n[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and\\nJian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings\\nof the IEEE/CVF international conference on computer vision, pages 8430–8439, 2019.\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[53] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in\\ncrowded scenes. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 2325–2333, 2016.\\n[54] Yuchen Su, Zhineng Chen, Zhiwen Shao, Yuning Du, Zhilong Ji, Jinfeng Bai, Yong Zhou,\\nand Yu-Gang Jiang. Lranet: Towards accurate and efficient scene text detection with low-rank\\napproximation network. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 38, pages 4979–4987, 2024.\\n[55] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What\\nmakes for end-to-end object detection? In International Conference on Machine Learning,\\npages 9934–9944. PMLR, 2021.\\n[56] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka,\\nLei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with\\nlearnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 14454–14463, 2021.\\n[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object\\ndetector. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):1922–1933,\\n2020.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems, 30, 2017.\\n[59] Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, and Guiguang Ding. Repvit: Revisiting mobile\\ncnn from vit perspective. arXiv preprint arXiv:2307.09283, 2023.\\n[60] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai\\nHan. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in\\nNeural Information Processing Systems, 36, 2024.\\n[61] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling\\ncross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and\\npattern recognition, pages 13029–13038, 2021.\\n[62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-\\nfreebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 7464–7475, 2023.\\n[63] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and\\nI-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. InProceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition workshops , pages\\n390–391, 2020.\\n[64] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh. Designing network design strategies\\nthrough gradient path analysis. arXiv preprint arXiv:2211.04800, 2022.\\n[65] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to\\nlearn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024.\\n[66] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end\\nobject detection with fully convolutional network. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pages 15849–15858, 2021.\\n[67] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for\\ntransformer-based detector. In Proceedings of the AAAI conference on artificial intelligence,\\nvolume 36, pages 2567–2575, 2022.\\n[68] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\\nIntroducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international\\nconference on computer vision, pages 22–31, 2021.\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[69] Haiyang Xu, Zhichao Zhou, Dongliang He, Fu Li, and Jingdong Wang. Vision transformer with\\nattention map hallucination and ffn compaction. arXiv preprint arXiv:2306.10875, 2023.\\n[70] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong\\nWang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. Pp-yoloe: An evolved version of yolo.\\narXiv preprint arXiv:2203.16250, 2022.\\n[71] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, and Xiuyu Sun. Damo-yolo:\\nA report on real-time object detection design. arXiv preprint arXiv:2211.15444, 2022.\\n[72] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr:\\nEnd-to-end multiple-object tracking with transformer. In European Conference on Computer\\nVision, pages 659–675. Springer, 2022.\\n[73] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-\\nYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.\\narXiv preprint arXiv:2203.03605, 2022.\\n[74] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\\n[75] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between\\nanchor-based and anchor-free detection via adaptive training sample selection. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 9759–9768,\\n2020.\\n[76] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu,\\nGang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic\\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 12083–12093, 2022.\\n[77] Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, and Jingdong\\nWang. Ms-detr: Efficient detr training with mixed supervision. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 17027–17036, 2024.\\n[78] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu,\\nand Jie Chen. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069,\\n2023.\\n[79] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou\\nloss: Faster and better learning for bounding box regression. In Proceedings of the AAAI\\nconference on artificial intelligence, volume 34, pages 12993–13000, 2020.\\n[80] Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms.\\nIEEE Transactions on Multimedia, 2023.\\n[81] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\\n2020.\\n[82] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training.\\nIn Proceedings of the IEEE/CVF international conference on computer vision, pages 6748–6758,\\n2023.\\n15'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='A Appendix\\nA.1 Implementation Details\\nFollowing [21, 62, 65], all YOLOv10 models are trained from scratch using the SGD optimizer for\\n500 epochs. The SGD momentum and weight decay are set to 0.937 and 5×10−4, respectively. The\\ninitial learning rate is 1×10−2 and it decays linearly to 1×10−4. For data augmentation, we adopt the\\nMosaic [2, 20], Mixup [74] and copy-paste augmentation [18], etc., like [21, 65]. Tab. 14 presents the\\ndetailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase\\nthe width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the\\nSPPF module [21] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion\\nratio of 2 for the inverted bottleneck block structure. Following [65, 62], we report the standard mean\\naverage precision (AP) across different object scales and IoU thresholds on the COCO dataset [35].\\nMoreover, we follow [78] to establish the end-to-end speed benchmark. Since the execution time of\\nNMS is affected by the input, we thus measure the latency on the COCO val set with the batch size\\nof 1, like [78]. We adopt the same NMS hyperparameters used by the detectors during their validation.\\nThe TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is\\nomitted. We report the average latency across all images.\\nTable 14: Hyper-parameters of YOLOv10.\\nhyper-parameter YOLOv10-N/S/M/B/L/X\\nepochs 500\\noptimizer SGD\\nmomentum 0.937\\nweight decay 5 ×10−4\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\ninitial learning rate 10 −2\\nfinal learning rate 10 −4\\nlearning rate schedule linear decay\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\nHSV hue augmentation 0.015\\ntranslation augmentation 0.1\\nscale augmentation 0.5/0.5/0.9/0.9/0.9/0.9\\nmosaic augmentation 1.0\\nMixup augmentation 0.0/0.0/0.1/0.1/0.15/0.15\\ncopy-paste augmentation 0.0/0.0/0.1/0.1/0.3/0.3\\nclose mosaic epochs 10\\nA.2 Details of Consistent Matching Metric\\nWe provide the detailed derivation of consistent matching metric here.\\nAs mentioned in the paper, we suppose that the one-to-many positive samples isΩ and the one-to-\\none branch selects i-th prediction. We can then leverage the normalized metric [ 15] to obtain the\\nclassification target for task alignment learning [21, 15, 65, 29, 70], i.e., to2m,j = u∗ · mo2m,j\\nm∗\\no2m\\n≤ u∗\\nfor j ∈ Ω and to2o,i = u∗ · mo2o,i\\nm∗\\no2o\\n= u∗. We can thus derive the supervision gap between two\\nbranches by the 1-Wasserstein distance [46] of the different classification targets, i.e.,\\nA = |(1 − to2o,i) − (1 − I(i ∈ Ω)to2m,i)| +\\nX\\nk∈Ω\\\\{i}\\n|1 − (1 − to2m,k)|\\n= |to2o,i − I(i ∈ Ω)to2m,i| +\\nX\\nk∈Ω\\\\{i}\\nto2m,k\\n= to2o,i − I(i ∈ Ω)to2m,i +\\nX\\nk∈Ω\\\\{i}\\nto2m,k,\\n(3)\\n16'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='where I(·) is the indicator function. We denote the classification targets of the predictions in Ω as\\n{ˆt1, ˆt2, ...,ˆt|Ω|} in descending order, with ˆt1 ≥ ˆt2 ≥ ... ≥ ˆt|Ω|. We can then replace to2o,i with u∗\\nand obtain:\\nA = u∗ − I(i ∈ Ω)to2m,i +\\nX\\nk∈Ω\\\\{i}\\nto2m,k\\n= u∗ +\\nX\\nk∈Ω\\nto2m,k − 2 · I(i ∈ Ω)to2m,i\\n= u∗ +\\nX|Ω|\\nk=1\\nˆtk − 2 · I(i ∈ Ω)to2m,i\\n(4)\\nWe further discuss the supervision gap in two scenarios, i.e.,\\n1. Supposing i ̸∈ Ω, we can obtain:\\nA = u∗ +\\nX|Ω|\\nk=1\\nˆtk (5)\\n2. Supposing i ∈ Ω, we denote to2m,i = ˆtn and obtain:\\nA = u∗ +\\nX|Ω|\\nk=1\\nˆtk − 2 · ˆtn (6)\\nDue to ˆtn ≥ 0, the second case can lead to smaller supervision gap. Besides, we can observe that A\\ndecreases as ˆtn increases, indicating that n decreases and the ranking of i within Ω improves. Due\\nto ˆtn ≤ ˆt1, A thus achieves the minimum when ˆtn = ˆt1, i.e., i is the best positive sample in Ω with\\nmo2m,i = m∗\\no2m and to2m,i = u∗ · mo2m,i\\nm∗\\no2m\\n= u∗.\\nFurthermore, we prove that we can achieve the minimized supervision gap by the consistent matching\\nmetric. We suppose αo2m > 0 and βo2m > 0, which are common in [21, 65, 29, 15, 70]. Similarly,\\nwe assume αo2o > 0 and βo2o > 0. We can obtain r1 = αo2o\\nαo2m\\n> 0 and r2 = βo2o\\nβo2m\\n> 0, and then\\nderive mo2o by\\nmo2o = s · pαo2o · IoU(ˆb, b)βo2o\\n= s · pr1·αo2m · IoU(ˆb, b)r2·βo2m\\n= s · (pαo2m · IoU(ˆb, b)βo2m)r1 · IoU(ˆb, b)(r2−r1)·βo2m\\n= mr1\\no2m · IoU(ˆb, b)(r2−r1)·βo2m\\n(7)\\nTo achieve mo2m,i = m∗\\no2m and mo2o,i = m∗\\no2o, we can make mo2o monotonically increase with\\nmo2m by assigning (r2 − r1) = 0, i.e.,\\nmo2o = mr1\\no2m · IoU(ˆb, b)0·βo2m\\n= mr1\\no2m\\n(8)\\nSupposing r1 = r2 = r, we can thus derive the consistent matching metric, i.e., αo2o = r · αo2m and\\nβo2o = r · βo2m. By simply taking r = 1, we obtain αo2o = αo2m and βo2o = βo2m.\\nA.3 Details of Rank-Guided Block Design\\nWe present the details of the algorithm of rank-guided block design in Algo. 1. Besides, to calculate\\nthe numerical rank of the convolution, we reshape its weight to the shape of (Co, K2 ×Ci), where Co\\nand Ci denote the number of output and input channels, and K means the kernel size, respectively.\\nA.4 Training Cost Analyses\\nIn addition to the inference efficiency analyses, we also investigate the training cost of our YOLOv10\\nmodels. We compare with other YOLO variants and measure the training throughput on 8 NVIDIA\\n3090 GPUs using the official codebases. Tab. 15 presents the comparison results based on the medium\\nmodel scale. We observe that despite having 500 training epochs, YOLOv10 achieves a high training\\nthroughput, making its training cost affordable. We also note that the one-to-many head in the\\nNMS-free training will introduce the extra overhead for YOLOv10. To investigate this, we measure\\nthe training cost of YOLOv10 with only the one-to-one head, which is denoted as “YOLOv10-o2o”.\\nAs shown in Tab. 15, YOLOv10-M results in a small increase in the training time over “YOLOv10-\\nM-o2o”, about 18s each epoch, which is affordable. To fairly verify the benefit of the one-to-many\\nhead in NMS-free training, we also adopt longer 550 training epochs for “YOLOv10-M-o2o”, which\\n17'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Algorithm 1: Rank-guided block design\\nInput: Intrinsic ranks R for all stages S; Original Network Θ; CIB θcib;\\nOutput: New network Θ∗ with CIB for certain stages.\\n1 t ← 0;\\n2 Θ0 ← Θ; Θ∗ ← Θ0;\\n3 ap0 ← AP(T(Θ0)) ; // T:training the network; AP:evaluating the AP performance.\\n4 while S ̸= ∅ do\\n5 st ← argmins∈S R;\\n6 Θt+1 ← Replace(Θt, θcib, st) ; // Replace the block in Stage st of Θt with CIB θcib.\\n7 apt+1 ← AP(T(Θt+1));\\n8 if apt+1 ≥ ap0 then\\n9 Θ∗ ← Θt+1; S ← S \\\\ {st};\\n10 else\\n11 return Θ∗;\\n12 end\\n13 end\\n14 return Θ∗;\\nTable 15: Training cost analyses on 8 NVIDIA 3090 GPUs.\\nModel Epoch Speed (epoch/hour) Time (hour)\\nYOLOv6-3.0-M 300 7.2 41.7\\nYOLOv8-M 500 18.3 27.3\\nYOLOv9-M 500 12.3 40.7\\nGold-YOLO-M 300 4.7 63.8\\nYOLO-MS 300 7.1 42.3\\nYOLOv10-M-o2o 500 18.8 26.7\\nYOLOv10-M 500 17.2 29.1\\nTable 16: Latency with NMS.\\nModel Latency\\nYOLOv10-N 6.19ms\\nYOLOv10-S 7.15ms\\nYOLOv10-M 9.03ms\\nYOLOv10-B 10.04ms\\nYOLOv10-L 11.52ms\\nYOLOv10-X 14.67ms\\nleads to a similar training time (29.3 vs. 29.1 hours) but still yields inferior performance (48.9% vs.\\n51.1% AP) compared with YOLOv10-M.\\nA.5 More Results on COCO\\nWe measure the latency of YOLOv10 with the original one-to-many training using NMS and report\\nthe results on COCO in Tab. 16. Besides, we report the detailed performance of YOLOv10, including\\nAPval\\n50 and APval\\n75 at different IoU thresholds, as well as AP val\\nsmall, APval\\nmedium, and APval\\nlarge across\\ndifferent scales, in Tab. 17. We also present the comparisons with more lightweight detectors,\\nincluding DAMO-YOLO [ 71], YOLOv7 [ 62], and DEYO [ 45], in Tab. 18. It shows that our\\nYOLOv10 also achieves superior performance and efficiency trade-offs. Additionally, in experiments,\\nwe follow previous works [21, 65] to train the models for 500 epochs. We also conduct experiments\\nto train the models for 300 epochs and present the comparison results with YOLOv6 [ 29], Gold-\\nYOLO [60], and YOLO-MS [8] which adopt 300 epochs, in Tab. 19. We observe that our YOLOv10\\nalso exhibits better performance and inference latency. We also note that despite trained for 500\\nepochs, YOLOv10 has less training cost compared with these models as presented in Tab. 15.\\nA.6 Inference Efficiency Comparison on CPU\\nWe present the speed comparison results of YOLOv10 and others on CPU (Intel Xeon Skylake, IBRS)\\nusing OpenVINO in Fig. 5. We observe that YOLOv10 also shows state-of-the-art trade-offs in terms\\nof performance and efficiency.\\nA.7 More Analyses for Holistic Efficiency-Accuracy Driven Model Design\\nWe note that reducing the latency of YOLOv10-S (#2 in Tab. 2) is particularly challenging due to its\\nsmall model scale. However, as shown in Tab. 2, our efficiency driven model design still achieves a\\n5.3% reduction in latency without compromising performance. This provides substantial support for\\nthe further accuracy driven model design. YOLOv10-S achieves a better latency-accuracy trade-off\\n18'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 17: Detailed performance of YOLOv10 on COCO.\\nModel AP val(%) AP val\\n50 (%) AP val\\n75 (%) AP val\\nsmall(%) AP val\\nmedium(%) AP val\\nlarge(%)\\nYOLOv10-N 38.5 53.8 41.7 18.9 42.4 54.6\\nYOLOv10-S 46.3 63.0 50.4 26.8 51.0 63.8\\nYOLOv10-M 51.1 68.1 55.8 33.8 56.5 67.0\\nYOLOv10-B 52.5 69.6 57.2 35.1 57.8 68.5\\nYOLOv10-L 53.2 70.1 58.1 35.8 58.5 69.4\\nYOLOv10-X 54.4 71.3 59.3 37.0 59.8 70.9\\nTable 18: Comparisons with more lightweight detectors.\\nModel #Param.(M) FLOPs(G) AP val(%) Latency(ms)\\nDEYO-tiny [45] 4.0 8.0 37.6 2.01\\nYOLOv10-N 2.3 6.7 38.5 1.84\\nDAMO-YOLO-T [71] 8.5 18.1 42.0 2.21\\nDAMO-YOLO-S [71] 16.3 37.8 46.0 3.18\\nDEYO-S [45] 14.0 26.0 45.8 3.34\\nYOLOv10-S 7.2 21.6 46.3 2.49\\nDAMO-YOLO-M [71] 28.2 61.8 49.2 4.57\\nDAMO-YOLO-L [71] 42.1 97.3 50.8 6.48\\nDEYO-M [45] 33.0 78.0 50.7 7.14\\nYOLOv10-M 15.4 59.1 51.1 4.74\\nYOLOv7 [62] 36.9 104.7 51.2 17.03\\nYOLOv10-B 19.1 92.0 52.5 5.74\\nYOLOv7-X [62] 71.3 189.9 52.9 21.45\\nDEYO-L [45] 51.0 155.0 52.7 10.00\\nYOLOv10-L 24.4 120.3 53.2 7.28\\nDEYO-X [45] 78.0 242.0 53.7 15.38\\nYOLOv10-X 29.5 160.4 54.4 10.70\\nwith our holistic efficiency-accuracy driven model design, showing a 2.0% AP improvement with only\\n0.05ms latency overhead. Besides, for YOLOv10-M (#6 in Tab. 2), which has a larger model scale\\nand more redundancy, our efficiency driven model design results in a considerable 12.5% latency\\nreduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a\\nnotable 0.8% AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48ms.\\nThese results well demonstrate the effectiveness of our design strategy across different model scales.\\nTable 19: Performance comparisons under 300 training epochs.\\nModel #Param.(M) FLOPs(G) AP val(%) Latency(ms)\\nYOLOv6-3.0-N [29] 4.7 11.4 37.0 2.69\\nGold-YOLO-N [60] 5.6 12.1 39.6 2.92\\nYOLOv10-N (Ours) 2.3 6.7 37.7 1.84\\nYOLOv6-3.0-S [29] 18.5 45.3 44.3 3.42\\nGold-YOLO-S [60] 21.5 46.0 45.4 3.82\\nYOLO-MS-XS [8] 4.5 17.4 43.4 8.23\\nYOLOv10-S (Ours) 7.2 21.6 45.6 2.49\\nYOLOv6-3.0-M [29] 34.9 85.8 49.1 5.63\\nGold-YOLO-M [60] 41.3 87.5 49.8 6.38\\nYOLOv10-M (Ours) 15.4 59.1 50.3 4.74\\nYOLOv6-3.0-L [29] 59.6 150.7 51.8 9.02\\nGold-YOLO-L [60] 75.1 151.7 51.8 10.65\\nYOLO-MS [8] 22.2 80.2 51.0 12.41\\nYOLOv10-B (Ours) 19.1 92.0 51.6 5.74\\nYOLOv10-L (Ours) 24.4 120.3 52.4 7.28\\nYOLOv10-X (Ours) 29.5 160.4 53.6 10.70\\n19'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni00000052/uni00000051/uni00000003/uni00000026/uni00000033/uni00000038/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\\n/uni00000016/uni0000001a/uni00000011/uni00000018\\n/uni00000017/uni00000013/uni00000011/uni00000013\\n/uni00000017/uni00000015/uni00000011/uni00000018\\n/uni00000017/uni00000018/uni00000011/uni00000013\\n/uni00000017/uni0000001a/uni00000011/uni00000018\\n/uni00000018/uni00000013/uni00000011/uni00000013\\n/uni00000018/uni00000015/uni00000011/uni00000018\\n/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 5: Performance and efficiency comparisons on CPU.\\nFigure 6: Visualization results under complex and challenging scenarios.\\nA.8 Visualization Results\\nFig. 6 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It\\ncan be observed that YOLOv10 can achieve precise detection under various difficult conditions, such\\nas low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely\\npacked objects, such as bottle, cup, and person. These results indicate its superior performance.\\nA.9 Contribution, Limitation, and Broader Impact\\nContribution. In summary, our contributions are three folds as follows:\\n1. We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label\\nassignments way is designed to provide rich supervision by one-to-many branch during training\\nand high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious\\nsupervision between two branches, we innovatively propose the consistent matching metric, which\\ncan well reduce the theoretical supervision gap and lead to improved performance.\\n20'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='2. We propose a holistic efficiency-accuracy driven model design strategy for the model architecture\\nof YOLOs. We present novel lightweight classification head, spatial-channel decoupled down-\\nsampling, and rank-guided block design, which greatly reduce the computational redundancy and\\nachieve high efficiency. We further introduce the large-kernel convolution and innovative partial\\nself-attention module, which effectively enhance the performance under low cost.\\n3. Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object\\ndetector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art\\nperformance and efficiency trade-offs compared with other advanced detectors.\\nLimitation. Due to the limited computational resources, we do not investigate the pretraining\\nof YOLOv10 on large-scale datasets, e.g., Objects365 [ 52]. Besides, although we can achieve\\ncompetitive end-to-end performance using the one-to-one head under NMS-free training, there still\\nexists a performance gap compared with the original one-to-many training using NMS, especially\\nnoticeable in small models. For example, in YOLOv10-N and YOLOv10-S, the performance of\\none-to-many training with NMS outperforms that of NMS-free training by 1.0% AP and 0.5% AP,\\nrespectively. We will explore ways to further reduce the gap and achieve higher performance for\\nYOLOv10 in the future work.\\nBroader impact. The YOLOs can be widely applied in various real-world applications, including\\nmedical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these\\nfields and improve the efficiency. However, we acknowledge the potential for malicious use of our\\nmodels. We will make every effort to prevent this.\\n21'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\nReal-time object detection is a very important topic in\\ncomputer vision, as it is often a necessary component in\\ncomputer vision systems. For example, multi-object track-\\ning [94, 93], autonomous driving [40, 18], robotics [35, 58],\\nmedical image analysis [34, 46], etc. The computing de-\\nvices that execute real-time object detection is usually some\\nmobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute\\nstick (Intel), Jetson AI edge devices (Nvidia), the edge TPU\\n(Google), the neural processing engine (Qualcomm), the AI\\nprocessing unit (MediaTek), and the AI SoCs (Kneron), are\\nall NPUs. Some of the above mentioned edge devices focus\\non speeding up different operations such as vanilla convolu-\\ntion, depth-wise convolution, or MLP operations. In this pa-\\nper, the real-time object detector we proposed mainly hopes\\nthat it can support both mobile GPU and GPU devices from\\nthe edge to the cloud.\\nIn recent years, the real-time object detector is still de-\\nveloped for different edge device. For example, the devel-\\nFigure 1: Comparison with other real-time object detectors, our\\nproposed methods achieve state-of-the-arts performance.\\nopment of MCUNet [49, 48] and NanoDet [54] focused on\\nproducing low-power single-chip and improving the infer-\\nence speed on edge CPU. As for methods such as YOLOX\\n[21] and YOLOR [81], they focus on improving the infer-\\nence speed of various GPUs. More recently, the develop-\\nment of real-time object detector has focused on the de-\\nsign of efﬁcient architecture. As for real-time object de-\\ntectors that can be used on CPU [54, 88, 84, 83], their de-\\nsign is mostly based on MobileNet [28, 66, 27], ShufﬂeNet\\n[92, 55], or GhostNet [25]. Another mainstream real-time\\nobject detectors are developed for GPU [81, 21, 97], they\\nmostly use ResNet [26], DarkNet [63], or DLA [87], and\\nthen use the CSPNet [80] strategy to optimize the architec-\\nture. The development direction of the proposed methods in\\nthis paper are different from that of the current mainstream\\nreal-time object detectors. In addition to architecture op-\\ntimization, our proposed methods will focus on the opti-\\nmization of the training process. Our focus will be on some\\noptimized modules and optimization methods which may\\nstrengthen the training cost for improving the accuracy of\\nobject detection, but without increasing the inference cost.\\nWe call the proposed modules and optimization methods\\ntrainable bag-of-freebies.\\n1\\narXiv:2207.02696v1  [cs.CV]  6 Jul 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Recently, model re-parameterization [13, 12, 29] and dy-\\nnamic label assignment [20, 17, 42] have become important\\ntopics in network training and object detection. Mainly af-\\nter the above new concepts are proposed, the training of\\nobject detector evolves many new issues. In this paper, we\\nwill present some of the new issues we have discovered and\\ndevise effective methods to address them. For model re-\\nparameterization, we analyze the model re-parameterization\\nstrategies applicable to layers in different networks with the\\nconcept of gradient propagation path, and propose planned\\nre-parameterized model. In addition, when we discover that\\nwith dynamic label assignment technology, the training of\\nmodel with multiple output layers will generate new issues.\\nThat is: “How to assign dynamic targets for the outputs of\\ndifferent branches?” For this problem, we propose a new\\nlabel assignment method called coarse-to-ﬁne lead guided\\nlabel assignment.\\nThe contributions of this paper are summarized as fol-\\nlows: (1) we design several trainable bag-of-freebies meth-\\nods, so that real-time object detection can greatly improve\\nthe detection accuracy without increasing the inference\\ncost; (2) for the evolution of object detection methods, we\\nfound two new issues, namely how re-parameterized mod-\\nule replaces original module, and how dynamic label as-\\nsignment strategy deals with assignment to different output\\nlayers. In addition, we also propose methods to address the\\ndifﬁculties arising from these issues; (3) we propose “ex-\\ntend” and “compound scaling” methods for the real-time\\nobject detector that can effectively utilize parameters and\\ncomputation; and (4) the method we proposed can effec-\\ntively reduce about 40% parameters and 50% computation\\nof state-of-the-art real-time object detector, and has faster\\ninference speed and higher detection accuracy.\\n2. Related work\\n2.1. Real-time object detectors\\nCurrently state-of-the-art real-time object detectors are\\nmainly based on YOLO [61, 62, 63] and FCOS [76, 77],\\nwhich are [3, 79, 81, 21, 54, 85, 23]. Being able to become\\na state-of-the-art real-time object detector usually requires\\nthe following characteristics: (1) a faster and stronger net-\\nwork architecture; (2) a more effective feature integration\\nmethod [22, 97, 37, 74, 59, 30, 9, 45]; (3) a more accurate\\ndetection method [76, 77, 69]; (4) a more robust loss func-\\ntion [96, 64, 6, 56, 95, 57]; (5) a more efﬁcient label assign-\\nment method [99, 20, 17, 82, 42]; and (6) a more efﬁcient\\ntraining method. In this paper, we do not intend to explore\\nself-supervised learning or knowledge distillation methods\\nthat require additional data or large model. Instead, we will\\ndesign new trainable bag-of-freebies method for the issues\\nderived from the state-of-the-art methods associated with\\n(4), (5), and (6) mentioned above.\\n2.2. Model re-parameterization\\nModel re-parametrization techniques [71, 31, 75, 19, 33,\\n11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple compu-\\ntational modules into one at inference stage. The model\\nre-parameterization technique can be regarded as an en-\\nsemble technique, and we can divide it into two cate-\\ngories, i.e., module-level ensemble and model-level ensem-\\nble. There are two common practices for model-level re-\\nparameterization to obtain the ﬁnal inference model. One\\nis to train multiple identical models with different train-\\ning data, and then average the weights of multiple trained\\nmodels. The other is to perform a weighted average of the\\nweights of models at different iteration number. Module-\\nlevel re-parameterization is a more popular research issue\\nrecently. This type of method splits a module into multi-\\nple identical or different module branches during training\\nand integrates multiple branched modules into a completely\\nequivalent module during inference. However, not all pro-\\nposed re-parameterized module can be perfectly applied to\\ndifferent architectures. With this in mind, we have devel-\\noped new re-parameterization module and designed related\\napplication strategies for various architectures.\\n2.3. Model scaling\\nModel scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way\\nto scale up or down an already designed model and make\\nit ﬁt in different computing devices. The model scaling\\nmethod usually uses different scaling factors, such as reso-\\nlution (size of input image), depth (number of layer), width\\n(number of channel), and stage (number of feature pyra-\\nmid), so as to achieve a good trade-off for the amount of\\nnetwork parameters, computation, inference speed, and ac-\\ncuracy. Network architecture search (NAS) is one of the\\ncommonly used model scaling methods. NAS can automat-\\nically search for suitable scaling factors from search space\\nwithout deﬁning too complicated rules. The disadvantage\\nof NAS is that it requires very expensive computation to\\ncomplete the search for model scaling factors. In [15], the\\nresearcher analyzes the relationship between scaling factors\\nand the amount of parameters and operations, trying to di-\\nrectly estimate some rules, and thereby obtain the scaling\\nfactors required by model scaling. Checking the literature,\\nwe found that almost all model scaling methods analyze in-\\ndividual scaling factor independently, and even the methods\\nin the compound scaling category also optimized scaling\\nfactor independently. The reason for this is because most\\npopular NAS architectures deal with scaling factors that are\\nnot very correlated. We observed that all concatenation-\\nbased models, such as DenseNet [32] or V oVNet [39], will\\nchange the input width of some layers when the depth of\\nsuch models is scaled. Since the proposed architecture is\\nconcatenation-based, we have to design a new compound\\nscaling method for this model.\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 2: Extended efﬁcient layer aggregation networks. The proposed extended ELAN (E-ELAN) does not change the gradient transmis-\\nsion path of the original architecture at all, but use group convolution to increase the cardinality of the added features, and combine the\\nfeatures of different groups in a shufﬂe and merge cardinality manner. This way of operation can enhance the features learned by different\\nfeature maps and improve the use of parameters and calculations.\\n3. Architecture\\n3.1. Extended efﬁcient layer aggregation networks\\nIn most of the literature on designing the efﬁcient ar-\\nchitectures, the main considerations are no more than the\\nnumber of parameters, the amount of computation, and the\\ncomputational density. Starting from the characteristics of\\nmemory access cost, Ma et al. [55] also analyzed the in-\\nﬂuence of the input/output channel ratio, the number of\\nbranches of the architecture, and the element-wise opera-\\ntion on the network inference speed. Doll´ar et al. [15] addi-\\ntionally considered activation when performing model scal-\\ning, that is, to put more consideration on the number of el-\\nements in the output tensors of convolutional layers. The\\ndesign of CSPV oVNet [79] in Figure 2 (b) is a variation of\\nV oVNet [39]. In addition to considering the aforementioned\\nbasic designing concerns, the architecture of CSPV oVNet\\n[79] also analyzes the gradient path, in order to enable the\\nweights of different layers to learn more diverse features.\\nThe gradient analysis approach described above makes in-\\nferences faster and more accurate. ELAN [1] in Figure 2 (c)\\nconsiders the following design strategy – “How to design an\\nefﬁcient network?.” They came out with a conclusion: By\\ncontrolling the shortest longest gradient path, a deeper net-\\nwork can learn and converge effectively. In this paper, we\\npropose Extended-ELAN (E-ELAN) based on ELAN and\\nits main architecture is shown in Figure 2 (d).\\nRegardless of the gradient path length and the stacking\\nnumber of computational blocks in large-scale ELAN, it has\\nreached a stable state. If more computational blocks are\\nstacked unlimitedly, this stable state may be destroyed, and\\nthe parameter utilization rate will decrease. The proposed\\nE-ELAN uses expand, shufﬂe, merge cardinality to achieve\\nthe ability to continuously enhance the learning ability of\\nthe network without destroying the original gradient path.\\nIn terms of architecture, E-ELAN only changes the archi-\\ntecture in computational block, while the architecture of\\ntransition layer is completely unchanged. Our strategy is\\nto use group convolution to expand the channel and car-\\ndinality of computational blocks. We will apply the same\\ngroup parameter and channel multiplier to all the compu-\\ntational blocks of a computational layer. Then, the feature\\nmap calculated by each computational block will be shuf-\\nﬂed into g groups according to the set group parameter g,\\nand then concatenate them together. At this time, the num-\\nber of channels in each group of feature map will be the\\nsame as the number of channels in the original architec-\\nture. Finally, we add g groups of feature maps to perform\\nmerge cardinality. In addition to maintaining the original\\nELAN design architecture, E-ELAN can also guide differ-\\nent groups of computational blocks to learn more diverse\\nfeatures.\\n3.2. Model scaling for concatenation-based models\\nThe main purpose of model scaling is to adjust some at-\\ntributes of the model and generate models of different scales\\nto meet the needs of different inference speeds. For ex-\\nample the scaling model of EfﬁcientNet [72] considers the\\nwidth, depth, and resolution. As for the scaled-YOLOv4\\n[79], its scaling model is to adjust the number of stages. In\\n[15], Doll´ar et al. analyzed the inﬂuence of vanilla convolu-\\ntion and group convolution on the amount of parameter and\\ncomputation when performing width and depth scaling, and\\nused this to design the corresponding model scaling method.\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 3: Model scaling for concatenation-based models. From (a) to (b), we observe that when depth scaling is performed on\\nconcatenation-based models, the output width of a computational block also increases. This phenomenon will cause the input width\\nof the subsequent transmission layer to increase. Therefore, we propose (c), that is, when performing model scaling on concatenation-\\nbased models, only the depth in a computational block needs to be scaled, and the remaining of transmission layer is performed with\\ncorresponding width scaling.\\nThe above methods are mainly used in architectures such as\\nPlainNet or ResNet. When these architectures are in execut-\\ning scaling up or scaling down, the in-degree and out-degree\\nof each layer will not change, so we can independently an-\\nalyze the impact of each scaling factor on the amount of\\nparameters and computation. However, if these methods\\nare applied to the concatenation-based architecture, we will\\nﬁnd that when scaling up or scaling down is performed on\\ndepth, the in-degree of a translation layer which is immedi-\\nately after a concatenation-based computational block will\\ndecrease or increase, as shown in Figure 3 (a) and (b).\\nIt can be inferred from the above phenomenon that\\nwe cannot analyze different scaling factors separately for\\na concatenation-based model but must be considered to-\\ngether. Take scaling-up depth as an example, such an ac-\\ntion will cause a ratio change between the input channel and\\noutput channel of a transition layer, which may lead to a de-\\ncrease in the hardware usage of the model. Therefore, we\\nmust propose the corresponding compound model scaling\\nmethod for a concatenation-based model. When we scale\\nthe depth factor of a computational block, we must also cal-\\nculate the change of the output channel of that block. Then,\\nwe will perform width factor scaling with the same amount\\nof change on the transition layers, and the result is shown\\nin Figure 3 (c). Our proposed compound scaling method\\ncan maintain the properties that the model had at the initial\\ndesign and maintains the optimal structure.\\n4. Trainable bag-of-freebies\\n4.1. Planned re-parameterized convolution\\nAlthough RepConv [13] has achieved excellent perfor-\\nmance on the VGG [68], when we directly apply it to\\nResNet [26] and DenseNet [32] and other architectures,\\nits accuracy will be signiﬁcantly reduced. We use gradi-\\nent ﬂow propagation paths to analyze how re-parameterized\\nconvolution should be combined with different network.\\nWe also designed planned re-parameterized convolution ac-\\ncordingly.\\nFigure 4: Planned re-parameterized model. In the proposed\\nplanned re-parameterized model, we found that a layer with resid-\\nual or concatenation connections, its RepConv should not have\\nidentity connection. Under these circumstances, it can be replaced\\nby RepConvN that contains no identity connections.\\nRepConv actually combines 3 × 3 convolution, 1 × 1\\nconvolution, and identity connection in one convolutional\\nlayer. After analyzing the combination and correspond-\\ning performance of RepConv and different architectures,\\nwe ﬁnd that the identity connection in RepConv destroys\\nthe residual in ResNet and the concatenation in DenseNet,\\nwhich provides more diversity of gradients for different fea-\\nture maps. For the above reasons, we use RepConv with-\\nout identity connection (RepConvN) to design the architec-\\nture of planned re-parameterized convolution. In our think-\\ning, when a convolutional layer with residual or concate-\\nnation is replaced by re-parameterized convolution, there\\nshould be no identity connection. Figure 4 shows an exam-\\nple of our designed “planned re-parameterized convolution”\\nused in PlainNet and ResNet. As for the complete planned\\nre-parameterized convolution experiment in residual-based\\nmodel and concatenation-based model, it will be presented\\nin the ablation study session.\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 5: Coarse for auxiliary and ﬁne for lead head label assigner. Compare with normal model (a), the schema in (b) has auxiliary head.\\nDifferent from the usual independent label assigner (c), we propose (d) lead head guided label assigner and (e) coarse-to-ﬁne lead head\\nguided label assigner. The proposed label assigner is optimized by lead head prediction and the ground truth to get the labels of training\\nlead head and auxiliary head at the same time. The detailed coarse-to-ﬁne implementation method and constraint design details will be\\nelaborated in Apendix.\\n4.2. Coarse for auxiliary and ﬁne for lead loss\\nDeep supervision [38] is a technique that is often used\\nin training deep networks. Its main concept is to add\\nextra auxiliary head in the middle layers of the network,\\nand the shallow network weights with assistant loss as the\\nguide. Even for architectures such as ResNet [26] and\\nDenseNet [32] which usually converge well, deep supervi-\\nsion [70, 98, 67, 47, 82, 65, 86, 50] can still signiﬁcantly\\nimprove the performance of the model on many tasks. Fig-\\nure 5 (a) and (b) show, respectively, the object detector ar-\\nchitecture “without” and “with” deep supervision. In this\\npaper, we call the head responsible for the ﬁnal output as\\nthe lead head, and the head used to assist training is called\\nauxiliary head.\\nNext we want to discuss the issue of label assignment. In\\nthe past, in the training of deep network, label assignment\\nusually refers directly to the ground truth and generate hard\\nlabel according to the given rules. However, in recent years,\\nif we take object detection as an example, researchers often\\nuse the quality and distribution of prediction output by the\\nnetwork, and then consider together with the ground truth to\\nuse some calculation and optimization methods to generate\\na reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].\\nFor example, YOLO [61] use IoU of prediction of bounding\\nbox regression and ground truth as the soft label of object-\\nness. In this paper, we call the mechanism that considers\\nthe network prediction results together with the ground truth\\nand then assigns soft labels as “label assigner.”\\nDeep supervision needs to be trained on the target ob-\\njectives regardless of the circumstances of auxiliary head or\\nlead head. During the development of soft label assigner re-\\nlated techniques, we accidentally discovered a new deriva-\\ntive issue, i.e., “How to assign soft label to auxiliary head\\nand lead head ?” To the best of our knowledge, the relevant\\nliterature has not explored this issue so far. The results of\\nthe most popular method at present is as shown in Figure 5\\n(c), which is to separate auxiliary head and lead head, and\\nthen use their own prediction results and the ground truth\\nto execute label assignment. The method proposed in this\\npaper is a new label assignment method that guides both\\nauxiliary head and lead head by the lead head prediction.\\nIn other words, we use lead head prediction as guidance to\\ngenerate coarse-to-ﬁne hierarchical labels, which are used\\nfor auxiliary head and lead head learning, respectively. The\\ntwo proposed deep supervision label assignment strategies\\nare shown in Figure 5 (d) and (e), respectively.\\nLead head guided label assigner is mainly calculated\\nbased on the prediction result of the lead head and the\\nground truth, and generate soft label through the optimiza-\\ntion process. This set of soft labels will be used as the tar-\\nget training model for both auxiliary head and lead head.\\nThe reason to do this is because lead head has a relatively\\nstrong learning capability, so the soft label generated from it\\nshould be more representative of the distribution and corre-\\nlation between the source data and the target. Furthermore,\\nwe can view such learning as a kind of generalized residual\\nlearning. By letting the shallower auxiliary head directly\\nlearn the information that lead head has learned, lead head\\nwill be more able to focus on learning residual information\\nthat has not yet been learned.\\nCoarse-to-ﬁne lead head guided label assigner also\\nused the predicted result of the lead head and the ground\\ntruth to generate soft label. However, in the process we gen-\\nerate two different sets of soft label, i.e., coarse label and\\nﬁne label, where ﬁne label is the same as the soft label gen-\\nerated by lead head guided label assigner, and coarse label\\nis generated by allowing more grids to be treated as posi-\\ntive target by relaxing the constraints of the positive sample\\nassignment process. The reason for this is that the learning\\nability of an auxiliary head is not as strong as that of a lead\\nhead, and in order to avoid losing the information that needs\\nto be learned, we will focus on optimizing the recall of aux-\\niliary head in the object detection task. As for the output\\nof lead head, we can ﬁlter the high precision results from\\nthe high recall results as the ﬁnal output. However, we must\\nnote that if the additional weight of coarse label is close to\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparison of baseline object detectors.\\nModel #Param. FLOPs Size AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nYOLOv4 [3] 64.4M 142.8G 640 49.7% 68.2% 54.3% 32.9% 54.8% 63.7%\\nYOLOR-u5 (r6.1) [81] 46.5M 109.1G 640 50.2% 68.7% 54.6% 33.2% 55.5% 63.7%\\nYOLOv4-CSP [79] 52.9M 120.4G 640 50.3% 68.6% 54.9% 34.2% 55.6% 65.1%\\nYOLOR-CSP [81] 52.9M 120.4G 640 50.8% 69.5% 55.3% 33.7% 56.0% 65.4%\\nYOLOv7 36.9M 104.7G 640 51.2% 69.7% 55.5% 35.2% 56.0% 66.7%\\nimprovement -43% -15% - +0.4 +0.2 +0.2 +1.5 = +1.3\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 52.7% 71.3% 57.4% 36.3% 57.5% 68.3%\\nYOLOv7-X 71.3M 189.9G 640 52.9% 71.1% 57.5% 36.9% 57.7% 68.6%\\nimprovement -36% -19% - +0.2 -0.2 +0.1 +0.6 +0.2 +0.3\\nYOLOv4-tiny [79] 6.1 6.9 416 24.9% 42.1% 25.7% 8.7% 28.4% 39.2%\\nYOLOv7-tiny 6.2 5.8 416 35.2% 52.8% 37.3% 15.7% 38.0% 53.4%\\nimprovement +2% -19% - +10.3 +10.7 +11.6 +7.0 +9.6 +14.2\\nYOLOv4-tiny-3l [79] 8.7 5.2 320 30.8% 47.3% 32.2% 10.9% 31.9% 51.5%\\nYOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%\\nimprovement = -11% - +0.7 +0.5 +0.9 -1.6 +1.6 +0.7\\nthat of ﬁne label, it may produce bad prior at ﬁnal predic-\\ntion. Therefore, in order to make those extra coarse positive\\ngrids have less impact, we put restrictions in the decoder,\\nso that the extra coarse positive grids cannot produce soft\\nlabel perfectly. The mechanism mentioned above allows\\nthe importance of ﬁne label and coarse label to be dynam-\\nically adjusted during the learning process, and makes the\\noptimizable upper bound of ﬁne label always higher than\\ncoarse label.\\n4.3. Other trainable bag-of-freebies\\nIn this section we will list some trainable bag-of-\\nfreebies. These freebies are some of the tricks we used\\nin training, but the original concepts were not proposed\\nby us. The training details of these freebies will be elab-\\norated in the Appendix, including (1) Batch normalization\\nin conv-bn-activation topology: This part mainly connects\\nbatch normalization layer directly to convolutional layer.\\nThe purpose of this is to integrate the mean and variance\\nof batch normalization into the bias and weight of convolu-\\ntional layer at the inference stage. (2) Implicit knowledge\\nin YOLOR [81] combined with convolution feature map in\\naddition and multiplication manner: Implicit knowledge in\\nYOLOR can be simpliﬁed to a vector by pre-computing at\\nthe inference stage. This vector can be combined with the\\nbias and weight of the previous or subsequent convolutional\\nlayer. (3) EMA model: EMA is a technique used in mean\\nteacher [75], and in our system we use EMA model purely\\nas the ﬁnal inference model.\\n5. Experiments\\n5.1. Experimental setup\\nWe use Microsoft COCO dataset to conduct experiments\\nand validate our object detection method. All our experi-\\nments did not use pre-trained models. That is, all models\\nwere trained from scratch. During the development pro-\\ncess, we used train 2017 set for training, and then used val\\n2017 set for veriﬁcation and choosing hyperparameters. Fi-\\nnally, we show the performance of object detection on the\\ntest 2017 set and compare it with the state-of-the-art object\\ndetection algorithms. Detailed training parameter settings\\nare described in Appendix.\\nWe designed basic model for edge GPU, normal GPU,\\nand cloud GPU, and they are respectively called YOLOv7-\\ntiny, YOLOv7, and YOLOv7-W6. At the same time, we\\nalso use basic model for model scaling for different ser-\\nvice requirements and get different types of models. For\\nYOLOv7, we do stack scaling on neck, and use the pro-\\nposed compound scaling method to perform scaling-up of\\nthe depth and width of the entire model, and use this to ob-\\ntain YOLOv7-X. As for YOLOv7-W6, we use the newly\\nproposed compound scaling method to obtain YOLOv7-E6\\nand YOLOv7-D6. In addition, we use the proposed E-\\nELAN for YOLOv7-E6, and thereby complete YOLOv7-\\nE6E. Since YOLOv7-tiny is an edge GPU-oriented archi-\\ntecture, it will use leaky ReLU as activation function. As\\nfor other models we use SiLU as activation function. We\\nwill describe the scaling factor of each model in detail in\\nAppendix.\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Table 2: Comparison of state-of-the-art real-time object detectors.\\nModel #Param. FLOPs Size FPS APtest / APval APtest\\n50 APtest\\n75 APtest\\nS APtest\\nM APtest\\nL\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - - - - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - - - - -\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - - - - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - - - - -\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6% 23.2% 46.4% 56.9%\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0% 28.6% 52.9% 63.8%\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6% 31.4% 55.3% 66.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5% 33.3% 56.3% 66.4%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - - - - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - - - - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - - - - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - - - - -\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - - - - -\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7% 31.7% 55.3% 64.7%\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9% 33.7% 57.1% 66.8%\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7% 18.8% 42.4% 51.9%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9% 31.8% 55.5% 65.0%\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8% 33.8% 57.1% 67.4%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - - - - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - - - - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - - - - -\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - - - - -\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - - - - -\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9% 36.1% 57.7% 65.6%\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5% 37.7% 59.1% 67.1%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1% 38.4% 59.7% 67.7%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9% 38.9% 60.4% 68.7%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1% 37.3% 58.7% 67.1%\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2% 38.0% 59.9% 68.4%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8% 38.8% 60.1% 69.5%\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%\\n1 Our FLOPs is calaculated by rectangle input resolution like 640 × 640 or 1280 × 1280.\\n2 Our inference time is estimated by using letterbox resize input image to make its long side equals to 640 or 1280.\\n5.2. Baselines\\nWe choose previous version of YOLO [3, 79] and state-\\nof-the-art object detector YOLOR [81] as our baselines. Ta-\\nble 1 shows the comparison of our proposed YOLOv7 mod-\\nels and those baseline that are trained with the same settings.\\nFrom the results we see that if compared with YOLOv4,\\nYOLOv7 has 75% less parameters, 36% less computation,\\nand brings 1.5% higher AP. If compared with state-of-the-\\nart YOLOR-CSP, YOLOv7 has 43% fewer parameters, 15%\\nless computation, and 0.4% higher AP. In the performance\\nof tiny model, compared with YOLOv4-tiny-31, YOLOv7-\\ntiny reduces the number of parameters by 39% and the\\namount of computation by 49%, but maintains the same AP.\\nOn the cloud GPU model, our model can still have a higher\\nAP while reducing the number of parameters by 19% and\\nthe amount of computation by 33%.\\n5.3. Comparison with state-of-the-arts\\nWe compare the proposed method with state-of-the-art\\nobject detectors for general GPUs and Mobile GPUs, and\\nthe results are shown in Table 2. From the results in\\nTable 2 we know that the proposed method has the best\\nspeed-accuracy trade-off comprehensively. If we compare\\nYOLOv7-tiny-SiLU with YOLOv5-N (r6.1), our method\\nis 127 fps faster and 10.7% more accurate on AP. In ad-\\ndition, YOLOv7 has 51.4% AP at frame rate of 161 fps,\\nwhile PPYOLOE-L with the same AP has only 78 fps frame\\nrate. In terms of parameter usage, YOLOv7 is 41% less than\\nPPYOLOE-L. If we compare YOLOv7-X with 114 fps in-\\nference speed to YOLOv5-L (r6.1) with 99 fps inference\\nspeed, YOLOv7-X can improve AP by 3.9%. If YOLOv7-\\nX is compared with YOLOv5-X (r6.1) of similar scale, the\\ninference speed of YOLOv7-X is 31 fps faster. In addi-\\ntion, in terms of the amount of parameters and computation,\\nYOLOv7-X reduces 22% of parameters and 8% of compu-\\ntation compared to YOLOv5-X (r6.1), but improves AP by\\n2.2%.\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='If we compare YOLOv7 with YOLOR using the input\\nresolution 1280, the inference speed of YOLOv7-W6 is 8\\nfps faster than that of YOLOR-P6, and the detection rate is\\nalso increased by 1% AP. As for the comparison between\\nYOLOv7-E6 and YOLOv5-X6 (r6.1), the former has 0.9%\\nAP gain than the latter, 45% less parameters and 63% less\\ncomputation, and the inference speed is increased by 47%.\\nYOLOv7-D6 has close inference speed to YOLOR-E6, but\\nimproves AP by 0.8%. YOLOv7-E6E has close inference\\nspeed to YOLOR-D6, but improves AP by 0.3%.\\n5.4. Ablation study\\n5.4.1 Proposed compound scaling method\\nTable 3 shows the results obtained when using different\\nmodel scaling strategies for scaling up. Among them, our\\nproposed compound scaling method is to scale up the depth\\nof computational block by 1.5 times and the width of tran-\\nsition block by 1.25 times. If our method is compared with\\nthe method that only scaled up the width, our method can\\nimprove the AP by 0.5% with less parameters and amount\\nof computation. If our method is compared with the method\\nthat only scales up the depth, our method only needs to in-\\ncrease the number of parameters by 2.9% and the amount of\\ncomputation by 1.2%, which can improve the AP by 0.2%.\\nIt can be seen from the results of Table 3 that our proposed\\ncompound scaling strategy can utilize parameters and com-\\nputation more efﬁciently.\\nTable 3: Ablation study on proposed model scaling.\\nModel #Param. FLOPs Size AP val APval\\n50 APval\\n75\\nbase (v7-X light) 47.0M 125.5G 640 51.7% 70.1% 56.0%\\nwidth only (1.25 w) 73.4M 195.5G 640 52.4% 70.9% 57.1%\\ndepth only (2.0 d) 69.3M 187.6G 640 52.7% 70.8% 57.3%\\ncompound (v7-X) 71.3M 189.9G 640 52.9% 71.1% 57.5%\\nimprovement - - - +1.2 +1.0 +1.5\\n5.4.2 Proposed planned re-parameterized model\\nIn order to verify the generality of our proposed planed\\nre-parameterized model, we use it on concatenation-based\\nmodel and residual-based model respectively for veriﬁca-\\ntion. The concatenation-based model and residual-based\\nmodel we chose for veriﬁcation are 3-stacked ELAN and\\nCSPDarknet, respectively.\\nIn the experiment of concatenation-based model, we re-\\nplace the 3 ×3 convolutional layers in different positions in\\n3-stacked ELAN with RepConv, and the detailed conﬁgura-\\ntion is shown in Figure 6. From the results shown in Table 4\\nwe see that all higher AP values are present on our proposed\\nplanned re-parameterized model.\\nIn the experiment dealing with residual-based model,\\nsince the original dark block does not have a 3 × 3 con-\\nFigure 6: Planned RepConv 3-stacked ELAN. Blue circles are the\\nposition we replace Conv by RepConv.\\nTable 4: Ablation study on planned RepConcatenation model.\\nModel AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nbase (3-S ELAN) 52.26% 70.41% 56.77% 35.81% 57.00% 67.59%\\nFigure 6 (a) 52.18% 70.34% 56.90% 35.71% 56.83% 67.51%\\nFigure 6 (b) 52.30% 70.30% 56.92% 35.76% 56.95% 67.74%\\nFigure 6 (c) 52.33% 70.56% 56.91% 35.90% 57.06% 67.50%\\nFigure 6 (d) 52.17% 70.32% 56.82% 35.33% 57.06% 68.09%\\nFigure 6 (e) 52.23% 70.20% 56.81% 35.34% 56.97% 66.88%\\nvolution block that conforms to our design strategy, we ad-\\nditionally design a reversed dark block for the experiment,\\nwhose architecture is shown in Figure 7. Since the CSP-\\nDarknet with dark block and reversed dark block has exactly\\nthe same amount of parameters and operations, it is fair to\\ncompare. The experiment results illustrated in Table 5 fully\\nconﬁrm that the proposed planned re-parameterized model\\nis equally effective on residual-based model. We ﬁnd that\\nthe design of RepCSPResNet [85] also ﬁt our design pat-\\ntern.\\nFigure 7: Reversed CSPDarknet. We reverse the position of 1 × 1\\nand 3 × 3 convolutional layer in dark block to ﬁt our planned re-\\nparameterized model design strategy.\\nTable 5: Ablation study on planned RepResidual model.\\nModel AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nbase (YOLOR-W6) 54.82% 72.39% 59.95% 39.68% 59.38% 68.30%\\nRepCSP 54.67% 72.50% 59.58% 40.22% 59.61% 67.87%\\nRCSP 54.36% 71.95% 59.54% 40.15% 59.02% 67.44%\\nRepRCSP 54.85% 72.51% 60.08% 40.53% 59.52% 68.06%\\nbase (YOLOR-CSP) 50.81% 69.47% 55.28% 33.74% 56.01% 65.38%\\nRepRCSP 50.91% 69.54% 55.55% 34.44% 55.74% 65.46%\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 8: Objectness map predicted by different methods at auxiliary head and lead head.\\n5.4.3 Proposed assistant loss for auxiliary head\\nIn the assistant loss for auxiliary head experiments, we com-\\npare the general independent label assignment for lead head\\nand auxiliary head methods, and we also compare the two\\nproposed lead guided label assignment methods. We show\\nall comparison results in Table 6. From the results listed in\\nTable 6, it is clear that any model that increases assistant\\nloss can signiﬁcantly improve the overall performance. In\\naddition, our proposed lead guided label assignment strat-\\negy receives better performance than the general indepen-\\ndent label assignment strategy in AP, AP 50, and AP75. As\\nfor our proposed coarse for assistant and ﬁne for lead label\\nassignment strategy, it results in best results in all cases. In\\nFigure 8 we show the objectness map predicted by different\\nmethods at auxiliary head and lead head. From Figure 8 we\\nﬁnd that if auxiliary head learns lead guided soft label, it\\nwill indeed help lead head to extract the residual informa-\\ntion from the consistant targets.\\nTable 6: Ablation study on proposed auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\nindependent 1280 55.8% 73.4% 60.9%\\nlead guided 1280 55.9% 73.5% 61.0%\\ncoarse-to-ﬁne lead guided 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nIn Table 7 we further analyze the effect of the proposed\\ncoarse-to-ﬁne lead guided label assignment method on the\\ndecoder of auxiliary head. That is, we compared the results\\nof with/without the introduction of upper bound constraint.\\nJudging from the numbers in the Table, the method of con-\\nstraining the upper bound of objectness by the distance from\\nthe center of the object can achieve better performance.\\nTable 7: Ablation study on constrained auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\naux without constraint 1280 55.9% 73.5% 61.0%\\naux with constraint 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nSince the proposed YOLOv7 uses multiple pyramids to\\njointly predict object detection results, we can directly con-\\nnect auxiliary head to the pyramid in the middle layer for\\ntraining. This type of training can make up for informa-\\ntion that may be lost in the next level pyramid prediction.\\nFor the above reasons, we designed partial auxiliary head\\nin the proposed E-ELAN architecture. Our approach is to\\nconnect auxiliary head after one of the sets of feature map\\nbefore merging cardinality, and this connection can make\\nthe weight of the newly generated set of feature map not\\ndirectly updated by assistant loss. Our design allows each\\npyramid of lead head to still get information from objects\\nwith different sizes. Table 8 shows the results obtained us-\\ning two different methods, i.e., coarse-to-ﬁne lead guided\\nand partial coarse-to-ﬁne lead guided methods. Obviously,\\nthe partial coarse-to-ﬁne lead guided method has a better\\nauxiliary effect.\\nTable 8: Ablation study on partial auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6E) 1280 56.3% 74.0% 61.5%\\naux 1280 56.5% 74.0% 61.6%\\npartial aux 1280 56.8% 74.4% 62.1%\\nimprovement - +0.5 +0.4 +0.6\\n6. Conclusions\\nIn this paper we propose a new architecture of real-\\ntime object detector and the corresponding model scaling\\nmethod. Furthermore, we ﬁnd that the evolving process\\nof object detection methods generates new research top-\\nics. During the research process, we found the replace-\\nment problem of re-parameterized module and the alloca-\\ntion problem of dynamic label assignment. To solve the\\nproblem, we propose the trainable bag-of-freebies method\\nto enhance the accuracy of object detection. Based on the\\nabove, we have developed the YOLOv7 series of object de-\\ntection systems, which receives the state-of-the-art results.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Table 9: More comparison (batch=1, no-TRT, without extra object detection training data)\\nModel #Param. FLOPs Size FPSV 100 APtest / APval APtest\\n50 APtest\\n75\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7%\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - -\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - -\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8%\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7%\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - -\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1%\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - -\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6%\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9%\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - -\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5%\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9%\\nF-RCNN-R101-FPN+ [5] 60.0M 246.0G 1333 20 - / 44.0% - -\\nDeformable DETR [100] 40.0M 173.0G - 19 - / 46.2% - -\\nSwin-B (C-M-RCNN) [52] 145.0M 982.0G 1333 11.6 - / 51.9% - -\\nDETR DC5-R101 [5] 60.0M 253.0G 1333 10 - / 44.9% - -\\nEfﬁcientDet-D7x [74] 77.0M 410.0G 1536 6.5 55.1% / 54.4% 72.4% 58.4%\\nDual-Swin-T (C-M-RCNN) [47] 113.8M 836.0G 1333 6.5 - / 53.6% - -\\nViT-Adapter-B [7] 122.0M 997.0G - 4.4 - / 50.8% - -\\nDual-Swin-B (HTC) [47] 235.0M - 1600 2.5 58.7% / 58.4% - -\\nDual-Swin-L (HTC) [47] 453.0M - 1600 1.5 59.4% / 59.1% - -\\nModel #Param. FLOPs Size FPSA100 APtest / APval APtest\\n50 APtest\\n75\\nDN-Deformable-DETR [41] 48.0M 265.0G 1333 23.0 - / 48.6% - -\\nConvNeXt-B (C-M-RCNN) [53] - 964.0G 1280 11.5 - / 54.0% 73.1% 58.8%\\nSwin-B (C-M-RCNN) [52] - 982.0G 1280 10.7 - / 53.0% 71.8% 57.5%\\nDINO-5scale (R50) [89] 47.0M 860.0G 1333 10.0 - / 51.0% - -\\nConvNeXt-L (C-M-RCNN) [53] - 1354.0G 1280 10.0 - / 54.8% 73.8% 59.8%\\nSwin-L (C-M-RCNN) [52] - 1382.0G 1280 9.2 - / 53.9% 72.4% 58.8%\\nConvNeXt-XL (C-M-RCNN) [53] - 1898.0G 1280 8.6 - / 55.2% 74.2% 59.9%\\n8. More comparison\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS and\\nhas the highest accuracy 56.8% AP test-dev / 56.8% AP\\nmin-val among all known real-time object detectors with 30\\nFPS or higher on GPU V100. YOLOv7-E6 object detector\\n(56 FPS V100, 55.9% AP) outperforms both transformer-\\nbased detector SWIN-L Cascade-Mask R-CNN (9.2 FPS\\nA100, 53.9% AP) by 509% in speed and 2% in accuracy,\\nand convolutional-based detector ConvNeXt-XL Cascade-\\nMask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed\\nand 0.7% AP in accuracy, as well as YOLOv7 outperforms:\\nYOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, De-\\nformable DETR, DINO-5scale-R50, ViT-Adapter-B and\\nmany other object detectors in speed and accuracy. More\\nover, we train YOLOv7 only on MS COCO dataset from\\nscratch without using any other datasets or pre-trained\\nweights.\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 9: Comparison with other object detectors.\\nFigure 10: Comparison with other real-time object detectors.\\nTable 10: Comparison of different setting.\\nModel Presicion IoU threshold AP val\\nYOLOv7-X FP16 (default) 0.65 (default) 52.9%\\nYOLOv7-X FP32 0.65 53.0%\\nYOLOv7-X FP16 0.70 53.0%\\nYOLOv7-X FP32 0.70 53.1%\\nimprovement - - +0.2%\\n* Similar to meituan/YOLOv6 and PPYOLOE, our model could\\nget higher AP when set higher IoU threshold.\\nThe maximum accuracy of the YOLOv7-E6E (56.8%\\nAP) real-time model is +13.7% AP higher than the cur-\\nrent most accurate meituan/YOLOv6-s model (43.1% AP)\\non COCO dataset. Our YOLOv7-tiny (35.2% AP, 0.4\\nms) model is +25% faster and +0.2% AP higher than\\nmeituan/YOLOv6-n (35.0% AP, 0.5 ms) under identical\\nconditions on COCO dataset and V100 GPU with batch=32.\\nFigure 11: Comparison with other real-time object detectors.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] anonymous. Designing network design strategies. anony-\\nmous submission, 2022. 3\\n[2] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus\\nCubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens,\\nand Barret Zoph. Revisiting ResNets: Improved training\\nand scaling strategies.Advances in Neural Information Pro-\\ncessing Systems (NeurIPS), 34, 2021. 2\\n[3] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020.\\n2, 6, 7\\n[4] Yue Cao, Thomas Andrew Geddes, Jean Yee Hwa Yang,\\nand Pengyi Yang. Ensemble deep learning in bioinformat-\\nics. Nature Machine Intelligence, 2(9):500–508, 2020. 2\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\\nEnd-to-end object detection with transformers. In Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV), pages 213–229, 2020. 10\\n[6] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intel-\\nligence (TPAMI), 43(11):3782–3798, 2020. 2\\n[7] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\\ndense predictions. arXiv preprint arXiv:2205.08534, 2022.\\n10\\n[8] Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae\\nLee. Gaussian YOLOv3: An accurate and fast object detec-\\ntor using localization uncertainty for autonomous driving.\\nIn Proceedings of the IEEE/CVF International Conference\\non Computer Vision (ICCV), pages 502–511, 2019. 5\\n[9] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:\\nUnifying object detection heads with attentions. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 7373–7382, 2021.\\n2\\n[10] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi\\nHuang, Jungong Han, and Guiguang Ding. Re-\\nparameterizing your optimizers rather than architectures.\\narXiv preprint arXiv:2205.15242, 2022. 2\\n[11] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong\\nHan. ACNet: Strengthening the kernel skeletons for pow-\\nerful CNN via asymmetric convolution blocks. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV), pages 1911–1920, 2019. 2\\n[12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding. Diverse branch block: Building a con-\\nvolution as an inception-like unit. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 10886–10895, 2021. 2\\n[13] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong\\nHan, Guiguang Ding, and Jian Sun. RepVGG: Making\\nVGG-style convnets great again. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 13733–13742, 2021. 2, 4\\n[14] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong\\nHan, Guiguang Ding, and Jian Sun. Scaling up your ker-\\nnels to 31x31: Revisiting large kernel design in CNNs. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), 2022. 2\\n[15] Piotr Doll ´ar, Mannat Singh, and Ross Girshick. Fast and\\naccurate model scaling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pages 924–932, 2021. 2, 3\\n[16] Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi\\nLin. Simple training strategies and model scaling for object\\ndetection. arXiv preprint arXiv:2107.00057, 2021. 2\\n[17] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV), pages 3490–3499,\\n2021. 2, 5\\n[18] Di Feng, Christian Haase-Sch ¨utz, Lars Rosenbaum, Heinz\\nHertlein, Claudius Glaeser, Fabian Timm, Werner Wies-\\nbeck, and Klaus Dietmayer. Deep multi-modal object de-\\ntection and semantic segmentation for autonomous driv-\\ning: Datasets, methods, and challenges. IEEE Transac-\\ntions on Intelligent Transportation Systems , 22(3):1341–\\n1360, 2020. 1\\n[19] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,\\nDmitry P Vetrov, and Andrew G Wilson. Loss sur-\\nfaces, mode connectivity, and fast ensembling of DNNs.\\nAdvances in Neural Information Processing Systems\\n(NeurIPS), 31, 2018. 2\\n[20] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and\\nJian Sun. OTA: Optimal transport assignment for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n303–312, 2021. 2, 5\\n[21] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430, 2021. 1, 2, 7, 10\\n[22] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. NAS-FPN:\\nLearning scalable feature pyramid architecture for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7036–7045, 2019. 2\\n[23] Jocher Glenn. YOLOv5 release v6.1. https://github.com/\\nultralytics/yolov5/releases/tag/v6.1, 2022. 2, 7, 10\\n[24] Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Ex-\\npandNets: Linear over-parameterization to train compact\\nconvolutional networks. Advances in Neural Information\\nProcessing Systems (NeurIPS), 33:1298–1310, 2020. 2\\n[25] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\\nXu, and Chang Xu. GhostNet: More features from cheap\\noperations. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1580–1589, 2020. 1\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 770–778, 2016. 1, 4, 5\\n[27] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for Mo-\\nbileNetV3. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1314–1324, 2019. 1\\n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient con-\\nvolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861, 2017. 1\\n[29] Mu Hu, Junyi Feng, Jiashen Hua, Baisheng Lai, Jian-\\nqiang Huang, Xiaojin Gong, and Xiansheng Hua. On-\\nline convolutional re-parameterization. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2022. 2\\n[30] Miao Hu, Yali Li, Lu Fang, and Shengjin Wang. A 2-FPN:\\nAttention aggregation based feature pyramid network for\\ninstance segmentation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pages 15343–15352, 2021. 2\\n[31] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E\\nHopcroft, and Kilian Q Weinberger. Snapshot ensembles:\\nTrain 1, get m for free. International Conference on Learn-\\ning Representations (ICLR), 2017. 2\\n[32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 2, 4, 5\\n[33] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\\nDmitry Vetrov, and Andrew Gordon Wilson. Averaging\\nweights leads to wider optima and better generalization. In\\nConference on Uncertainty in Artiﬁcial Intelligence (UAI),\\n2018. 2\\n[34] Paul F Jaeger, Simon AA Kohl, Sebastian Bickel-\\nhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter\\nSchlemmer, and Klaus H Maier-Hein. Retina U-Net: Em-\\nbarrassingly simple exploitation of segmentation supervi-\\nsion for medical object detection. In Machine Learning for\\nHealth Workshop, pages 171–183, 2020. 1\\n[35] Hakan Karaoguz and Patric Jensfelt. Object detection ap-\\nproach for robot grasp detection. In IEEE International\\nConference on Robotics and Automation (ICRA) , pages\\n4953–4959, 2019. 1\\n[36] Kang Kim and Hee Seok Lee. Probabilistic anchor as-\\nsignment with iou prediction for object detection. In Pro-\\nceedings of the European conference on computer vision\\n(ECCV), pages 355–371, 2020. 5\\n[37] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\\nDoll´ar. Panoptic feature pyramid networks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 6399–6408, 2019. 2\\n[38] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Arti-\\nﬁcial Intelligence and Statistics, pages 562–570, 2015. 5\\n[39] Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok\\nBae, and Jongyoul Park. An energy and GPU-computation\\nefﬁcient backbone network for real-time object detection.\\nIn Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition Workshops (CVPRW),\\npages 0–0, 2019. 2, 3\\n[40] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and\\nXiaogang Wang. GS3D: An efﬁcient 3d object detection\\nframework for autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 1019–1028, 2019. 1\\n[41] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M\\nNi, and Lei Zhang. DN-DETR: Accelerate detr training\\nby introducing query denoising. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 13619–13627, 2022. 10\\n[42] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 9387–\\n9396, 2022. 2, 5\\n[43] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,\\nand Jian Yang. Generalized focal loss v2: Learning reliable\\nlocalization quality estimation for dense object detection. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), pages 11632–11641,\\n2021. 5\\n[44] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin\\nHu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal\\nloss: Learning qualiﬁed and distributed bounding boxes for\\ndense object detection. Advances in Neural Information\\nProcessing Systems (NeurIPS), 33:21002–21012, 2020. 5\\n[45] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\\nExploring plain vision transformer backbones for object de-\\ntection. arXiv preprint arXiv:2203.16527, 2022. 2\\n[46] Zhuoling Li, Minghui Dong, Shiping Wen, Xiang Hu, Pan\\nZhou, and Zhigang Zeng. CLU-CNNs: Object detection for\\nmedical images. Neurocomputing, 350:53–59, 2019. 1\\n[47] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNetV2: A composite backbone network architecture for ob-\\nject detection. arXiv preprint arXiv:2107.00420, 2021. 5,\\n10\\n[48] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song\\nHan. Memory-efﬁcient patch-based inference for tiny deep\\nlearning. Advances in Neural Information Processing Sys-\\ntems (NeurIPS), 34:2346–2358, 2021. 1\\n[49] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song\\nHan, et al. MCUNet: Tiny deep learning on IoT de-\\nvices. Advances in Neural Information Processing Systems\\n(NeurIPS), 33:11711–11722, 2020. 1\\n[50] Yuxuan Liu, Lujia Wang, and Ming Liu. YOLOStereo3D:\\nA step back to 2D for efﬁcient stereo 3D detection. In\\nIEEE International Conference on Robotics and Automa-\\ntion (ICRA), pages 13018–13024, 2021. 5\\n[51] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='et al. Swin transformer v2: Scaling up capacity and res-\\nolution. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\\n[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV), pages 10012–10022, 2021. 10\\n[53] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for\\nthe 2020s. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n11976–11986, 2022. 10\\n[54] Rangi Lyu. NanoDet-Plus. https://github.com/RangiLyu/\\nnanodet/releases/tag/v1.0.0-alpha-1, 2021. 1, 2\\n[55] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian\\nSun. ShufﬂeNet V2: Practical guidelines for efﬁcient CNN\\narchitecture design. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 116–131, 2018.\\n1, 3\\n[56] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unifying\\nclassiﬁcation and localisation in object detection. Advances\\nin Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 2\\n[57] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and in-\\nstance segmentation. In Proceedings of the IEEE/CVF In-\\nternational Conference on Computer Vision (ICCV), pages\\n3009–3018, 2021. 2\\n[58] Shuvo Kumar Paul, Muhammed Tawﬁq Chowdhury,\\nMircea Nicolescu, Monica Nicolescu, and David Feil-\\nSeifer. Object detection and pose estimation from rgb and\\ndepth data for real-time, adaptive robotic grasping. In Ad-\\nvances in Computer Vision and Computational Biology ,\\npages 121–142. 2021. 1\\n[59] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. De-\\ntectoRS: Detecting objects with recursive feature pyramid\\nand switchable atrous convolution. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 10213–10224, 2021. 2\\n[60] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n10428–10436, 2020. 2\\n[61] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Uniﬁed, real-time object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n779–788, 2016. 2, 5\\n[62] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 2\\n[63] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767, 2018. 1, 2\\n[64] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding\\nbox regression. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 658–666, 2019. 2\\n[65] Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and\\nSaehoon Kim. Sparse DETR: Efﬁcient end-to-end ob-\\nject detection with learnable sparsity. arXiv preprint\\narXiv:2111.14330, 2021. 5\\n[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey\\nZhmoginov, and Liang-Chieh Chen. MobileNetV2: In-\\nverted residuals and linear bottlenecks. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 4510–4520, 2018. 1\\n[67] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection\\nfrom scratch with deep supervision. IEEE Transactions\\non Pattern Analysis and Machine Intelligence (TPAMI) ,\\n42(2):398–412, 2019. 5\\n[68] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556, 2014. 4\\n[69] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\\nChanghu Wang, et al. Sparse R-CNN: End-to-end ob-\\nject detection with learnable proposals. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 14454–14463, 2021. 2\\n[70] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 1–9, 2015. 5\\n[71] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception\\narchitecture for computer vision. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 2818–2826, 2016. 2\\n[72] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking\\nmodel scaling for convolutional neural networks. In Inter-\\nnational Conference on Machine Learning (ICML) , pages\\n6105–6114, 2019. 2, 3\\n[73] Mingxing Tan and Quoc Le. EfﬁcientNetv2: Smaller mod-\\nels and faster training. In International Conference on Ma-\\nchine Learning (ICML), pages 10096–10106, 2021. 2\\n[74] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcient-\\nDet: Scalable and efﬁcient object detection. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 10781–10790, 2020. 2, 10\\n[75] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. Advances in Neural\\nInformation Processing Systems (NeurIPS), 30, 2017. 2, 6\\n[76] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. InProceed-\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV), pages 9627–9636, 2019. 2\\n[77] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI), 44(4):1922–1933, 2022. 2\\n[78] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff\\nZhu, Oncel Tuzel, and Anurag Ranjan. An im-\\nproved one millisecond mobile backbone. arXiv preprint\\narXiv:2206.04040, 2022. 2\\n[79] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 13029–13038, 2021. 2, 3, 6, 7\\n[80] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSP-\\nNet: A new backbone that can enhance learning capabil-\\nity of CNN. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition Workshops\\n(CVPRW), pages 390–391, 2020. 1\\n[81] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Uniﬁed network for\\nmultiple tasks. arXiv preprint arXiv:2105.04206, 2021. 1,\\n2, 6, 7, 10\\n[82] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 15849–15858, 2021. 2, 5\\n[83] Bichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai,\\nPeizhao Zhang, Matthew Yu, Jialiang Wang, Yingyan Lin,\\nand Peter Vajda. FBNetv5: Neural architecture search for\\nmultiple tasks in one run.arXiv preprint arXiv:2111.10007,\\n2021. 1\\n[84] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin,\\nGabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans,\\nMingxing Tan, Vikas Singh, and Bo Chen. MobileDets:\\nSearching for object detection architectures for mobile ac-\\ncelerators. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n3825–3834, 2021. 1\\n[85] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao\\nChang, Cheng Cui, Kaipeng Deng, Guanzhong Wang,\\nQingqing Dang, Shengyu Wei, Yuning Du, et al. PP-\\nYOLOE: An evolved version of YOLO. arXiv preprint\\narXiv:2203.16250, 2022. 2, 7, 8, 10\\n[86] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam.\\n3D-MAN: 3D multi-frame attention network for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1863–1872, 2021. 5\\n[87] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor\\nDarrell. Deep layer aggregation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 2403–2412, 2018. 1\\n[88] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng\\nCui, Wei Ji, Qingqing Dang, Kaipeng Deng, Guanzhong\\nWang, Yuning Du, et al. PP-PicoDet: A better real-\\ntime object detector on mobile devices. arXiv preprint\\narXiv:2111.00902, 2021. 1\\n[89] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\\nZhu, Lionel M Ni, and Heung-Yeung Shum. DINO: DETR\\nwith improved denoising anchor boxes for end-to-end ob-\\nject detection. arXiv preprint arXiv:2203.03605, 2022. 10\\n[90] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-\\nderhauf. VarifocalNet: An IoU-aware dense object detector.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), pages 8514–8523,\\n2021. 5\\n[91] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z Li. Bridging the gap between anchor-based and\\nanchor-free detection via adaptive training sample selec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 9759–\\n9768, 2020. 5\\n[92] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian\\nSun. ShufﬂeNet: An extremely efﬁcient convolutional neu-\\nral network for mobile devices. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 6848–6856, 2018. 1\\n[93] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan\\nYuan, Ping Luo, Wenyu Liu, and Xinggang Wang. BYTE-\\nTrack: Multi-object tracking by associating every detection\\nbox. arXiv preprint arXiv:2110.06864, 2021. 1\\n[94] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. FAIRMOT: On the fairness of detec-\\ntion and re-identiﬁcation in multiple object tracking. Inter-\\nnational Journal of Computer Vision, 129(11):3069–3087,\\n2021. 1\\n[95] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\\nvolume 34, pages 12993–13000, 2020. 2\\n[96] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV), pages 85–94, 2019. 2\\n[97] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Ob-\\njects as points. arXiv preprint arXiv:1904.07850, 2019. 1,\\n2\\n[98] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima\\nTajbakhsh, and Jianming Liang. UNet++: A nested U-\\nNet architecture for medical image segmentation. In\\nDeep Learning in Medical Image Analysis and Multimodal\\nLearning for Clinical Decision Support, 2018. 5\\n[99] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496, 2020. 2, 5\\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable DETR: Deformable trans-\\nformers for end-to-end object detection. In Proceedings of\\nthe International Conference on Learning Representations\\n(ICLR), 2021. 10\\n15')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Split documents into smaller chunks for RAG.\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    chunks = splitter.split_documents(documents)\n",
        "    print(f\"📄 {len(documents)} docs → 🔹 {len(chunks)} chunks\")\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "G90rRnnEVbNz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=split_documents(all_pdf_documents)\n",
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue014_VsVbQi",
        "outputId": "3313b227-8c07-4119-eb0e-31a8ce5f9032"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 36 docs → 🔹 194 chunks\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang1 Hui Chen2∗ Lihao Liu1 Kai Chen1\\nZijia Lin1 Jungong Han3 Guiguang Ding1∗\\n1School of Software, Tsinghua University 2BNRist, Tsinghua University\\n3Department of Automation, Tsinghua University\\nwa22@mails.tsinghua.edu.cn huichen@mail.tsinghua.edu.cn linzijia07@tsinghua.org.cn\\n{louisliu2048,chenkai2010.9,jungonghan77}@gmail.com dinggg@tsinghua.edu.cn\\n/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\\n/uni00000016/uni0000001a/uni00000011/uni00000018\\n/uni00000017/uni00000013/uni00000011/uni00000013'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni00000016/uni0000001a/uni00000011/uni00000018\\n/uni00000017/uni00000013/uni00000011/uni00000013\\n/uni00000017/uni00000015/uni00000011/uni00000018\\n/uni00000017/uni00000018/uni00000011/uni00000013\\n/uni00000017/uni0000001a/uni00000011/uni00000018\\n/uni00000018/uni00000013/uni00000011/uni00000013\\n/uni00000018/uni00000015/uni00000011/uni00000018\\n/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c\\n/uni00000016/uni0000001a/uni00000011/uni00000018\\n/uni00000017/uni00000013/uni00000011/uni00000013\\n/uni00000017/uni00000015/uni00000011/uni00000018\\n/uni00000017/uni00000018/uni00000011/uni00000013\\n/uni00000017/uni0000001a/uni00000011/uni00000018\\n/uni00000018/uni00000013/uni00000011/uni00000013\\n/uni00000018/uni00000015/uni00000011/uni00000018\\n/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='designs, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training\\nof YOLOs, which brings the competitive performance and low inference latency\\nsimultaneously. Moreover, we introduce the holistic efficiency-accuracy driven'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='of YOLOs, which brings the competitive performance and low inference latency\\nsimultaneously. Moreover, we introduce the holistic efficiency-accuracy driven\\nmodel design strategy for YOLOs. We comprehensively optimize various compo-\\nnents of YOLOs from both the efficiency and accuracy perspectives, which greatly\\nreduces the computational overhead and enhances the capability. The outcome\\nof our effort is a new generation of YOLO series for real-time end-to-end object\\ndetection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves\\nthe state-of-the-art performance and efficiency across various model scales. For\\n∗Corresponding author.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\\narXiv:2405.14458v2  [cs.CV]  30 Oct 2024'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='example, our YOLOv10-S is 1.8 × faster than RT-DETR-R18 under the simi-\\nlar AP on COCO, meanwhile enjoying 2.8× smaller number of parameters and\\nFLOPs. Compared with YOLOv9-C, YOLOv10-B has 46% less latency and 25%\\nfewer parameters for the same performance. Code and models are available at\\nhttps://github.com/THU-MIG/yolov10.\\n1 Introduction\\nReal-time object detection has always been a focal point of research in the area of computer vision,\\nwhich aims to accurately predict the categories and positions of objects in an image under low\\nlatency. It is widely adopted in various practical applications, including autonomous driving [ 3],\\nrobot navigation [12], and object tracking [72], etc. In recent years, researchers have concentrated\\non devising CNN-based object detectors to achieve real-time detection [ 19, 23, 48, 49, 50, 57,\\n13]. Among them, YOLOs have gained increasing popularity due to their adept balance between'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='on devising CNN-based object detectors to achieve real-time detection [ 19, 23, 48, 49, 50, 57,\\n13]. Among them, YOLOs have gained increasing popularity due to their adept balance between\\nperformance and efficiency [2, 20, 29, 20, 21, 65, 60, 70, 8, 71, 17, 29]. The detection pipeline of\\nYOLOs consists of two parts: the model forward process and the NMS post-processing. However,\\nboth of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.\\nSpecifically, YOLOs usually employ one-to-many label assignment strategy during training, whereby\\none ground-truth object corresponds to multiple positive samples. Despite yielding superior perfor-\\nmance, this approach necessitates NMS to select the best positive prediction during inference. This\\nslows down the inference speed and renders the performance sensitive to the hyperparameters of NMS,\\nthereby preventing YOLOs from achieving optimal end-to-end deployment [78]. One line to tackle'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='slows down the inference speed and renders the performance sensitive to the hyperparameters of NMS,\\nthereby preventing YOLOs from achieving optimal end-to-end deployment [78]. One line to tackle\\nthis issue is to adopt the recently introduced end-to-end DETR architectures [4, 81, 73, 30, 36, 42, 67].\\nFor example, RT-DETR [78] presents an efficient hybrid encoder and uncertainty-minimal query\\nselection, propelling DETRs into the realm of real-time applications. Nevertheless, when considering\\nonly the forward process of model during deployment, the efficiency of the DETRs still has room\\nfor improvements compared with YOLOs. Another line is to explore end-to-end detection for CNN-\\nbased detectors, which typically leverages one-to-one assignment strategies to suppress the redundant\\npredictions [6, 55, 66, 80, 17]. However, they usually introduce additional inference overhead or\\nachieve suboptimal performance for YOLOs.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='predictions [6, 55, 66, 80, 17]. However, they usually introduce additional inference overhead or\\nachieve suboptimal performance for YOLOs.\\nFurthermore, the model architecture design remains a fundamental challenge for YOLOs, which\\nexhibits an important impact on the accuracy and speed [50, 17, 71, 8]. To achieve more efficient\\nand effective model architectures, researchers have explored different design strategies. Various\\nprimary computational units are presented for the backbone to enhance the feature extraction ability,\\nincluding DarkNet [48, 49, 50], CSPNet [2], EfficientRep [29] and ELAN [62, 64], etc. For the neck,\\nPAN [37], BiC [29], GD [60] and RepGFPN [71], etc., are explored to enhance the multi-scale feature\\nfusion. Besides, model scaling strategies [62, 61] and re-parameterization [11, 29] techniques are also\\ninvestigated. While these efforts have achieved notable advancements, a comprehensive inspection for'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='investigated. While these efforts have achieved notable advancements, a comprehensive inspection for\\nvarious components in YOLOs from both the efficiency and accuracy perspectives is still lacking. As\\na result, there still exists considerable computational redundancy within YOLOs, leading to inefficient\\nparameter utilization and suboptimal efficiency. Besides, the resulting constrained model capability\\nalso leads to inferior performance, leaving ample room for accuracy improvements.\\nIn this work, we aim to address these issues and further advance the accuracy-speed boundaries of\\nYOLOs. We target both the post-processing and the model architecture throughout the detection\\npipeline. To this end, we first tackle the problem of redundant predictions in the post-processing\\nby presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label\\nassignments and consistent matching metric. It allows the model to enjoy rich and harmonious'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='by presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label\\nassignments and consistent matching metric. It allows the model to enjoy rich and harmonious\\nsupervision during training while eliminating the need for NMS during inference, leading to com-\\npetitive performance with high efficiency. Secondly, we propose the holistic efficiency-accuracy\\ndriven model design strategy for the model architecture by performing the comprehensive inspection\\nfor various components in YOLOs. For efficiency, we propose the lightweight classification head,\\nspatial-channel decoupled downsampling, and rank-guided block design, to reduce the manifested\\ncomputational redundancy and achieve more efficient architecture. For accuracy, we explore the\\nlarge-kernel convolution and present the effective partial self-attention module to enhance the model\\ncapability, harnessing the potential for performance improvements under low cost.\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Based on these approaches, we succeed in achieving a new family of real-time end-to-end detectors\\nwith different model scales, i.e., YOLOv10-N / S / M / B / L / X. Extensive experiments on standard\\nbenchmarks for object detection, i.e., COCO [35], demonstrate that our YOLOv10 can significantly\\noutperform previous state-of-the-art models in terms of computation-accuracy trade-offs across\\nvarious model scales. As shown in Fig. 1, our YOLOv10-S / X are 1.8× / 1.3× faster than RT-DETR-\\nR18 / R101, respectively, under the similar performance. Compared with YOLOv9-C, YOLOv10-B\\nachieves a 46% reduction in latency with the same performance. Moreover, YOLOv10 exhibits highly\\nefficient parameter utilization. Our YOLOv10-L / X outperforms YOLOv8-L / X by 0.3 AP and\\n0.5 AP, with 1.8× and 2.3× smaller number of parameters, respectively. YOLOv10-M achieves the\\nsimilar AP compared with YOLOv9-M / YOLO-MS, with 23% / 31% fewer parameters, respectively.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='0.5 AP, with 1.8× and 2.3× smaller number of parameters, respectively. YOLOv10-M achieves the\\nsimilar AP compared with YOLOv9-M / YOLO-MS, with 23% / 31% fewer parameters, respectively.\\nWe hope that our work can inspire further studies and advancements in the field.\\n2 Related Work\\nReal-time object detectors. Real-time object detection aims to classify and locate objects under low\\nlatency, which is crucial for real-world applications. Over the past years, substantial efforts have been\\ndirected towards developing efficient detectors [19, 57, 48, 34, 79, 75, 32, 31, 41]. Particularly, the\\nYOLO series [48, 49, 50, 2, 20, 29, 62, 21, 65] stand out as the mainstream ones. YOLOv1, YOLOv2,\\nand YOLOv3 identify the typical detection architecture consisting of three parts, i.e., backbone, neck,\\nand head [48, 49, 50]. YOLOv4 [2] and YOLOv5 [20] introduce the CSPNet [63] design to replace\\nDarkNet [47], coupled with data augmentation strategies, enhanced PAN, and a greater variety of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='and head [48, 49, 50]. YOLOv4 [2] and YOLOv5 [20] introduce the CSPNet [63] design to replace\\nDarkNet [47], coupled with data augmentation strategies, enhanced PAN, and a greater variety of\\nmodel scales, etc. YOLOv6 [29] presents BiC and SimCSPSPPF for neck and backbone, respectively,\\nwith anchor-aided training and self-distillation strategy. YOLOv7 [62] introduces E-ELAN for rich\\ngradient flow path and explores several trainable bag-of-freebies methods. YOLOv8 [21] presents C2f\\nbuilding block for effective feature extraction and fusion. Gold-YOLO [60] provides the advanced\\nGD mechanism to boost the multi-scale feature fusion capability. YOLOv9 [65] proposes GELAN to\\nimprove the architecture and PGI to augment the training process.\\nEnd-to-end object detectors. End-to-end object detection has emerged as a paradigm shift from\\ntraditional pipelines, offering streamlined architectures [53]. DETR [4] introduces the transformer'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='End-to-end object detectors. End-to-end object detection has emerged as a paradigm shift from\\ntraditional pipelines, offering streamlined architectures [53]. DETR [4] introduces the transformer\\narchitecture and adopts Hungarian loss to achieve one-to-one matching prediction, thereby eliminating\\nhand-crafted components and post-processing. Since then, various DETR variants have been proposed\\nto enhance its performance and efficiency [42, 67, 56, 30, 36, 28, 5, 77, 82]. Deformable-DETR [81]\\nleverages multi-scale deformable attention module to accelerate the convergence speed. DINO [73]\\nintegrates contrastive denoising, mix query selection, and look forward twice scheme into DETRs.\\nRT-DETR [78] further designs the efficient hybrid encoder and proposes the uncertainty-minimal\\nquery selection to improve both the accuracy and latency. Another line to achieve end-to-end object\\ndetection is based CNN detectors. Learnable NMS [24] and relation networks [26] present another'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='query selection to improve both the accuracy and latency. Another line to achieve end-to-end object\\ndetection is based CNN detectors. Learnable NMS [24] and relation networks [26] present another\\nnetwork to remove duplicated predictions for detectors. OneNet [55] and DeFCN [66] propose one-\\nto-one matching strategies to enable end-to-end object detection with fully convolutional networks.\\nFCOSpss [80] introduces a positive sample selector to choose the optimal sample for prediction.\\n3 Methodology\\n3.1 Consistent Dual Assignments for NMS-free Training\\nDuring training, YOLOs [21, 65, 29, 70] usually leverage TAL [15] to allocate multiple positive sam-\\nples for each instance. The adoption of one-to-many assignment yields plentiful supervisory signals,\\nfacilitating the optimization and achieving superior performance. However, it necessitates YOLOs\\nto rely on the NMS post-processing, which causes the suboptimal inference efficiency for deploy-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='facilitating the optimization and achieving superior performance. However, it necessitates YOLOs\\nto rely on the NMS post-processing, which causes the suboptimal inference efficiency for deploy-\\nment. While previous works [55, 66, 80, 6] explore one-to-one matching to suppress the redundant\\npredictions, they usually introduce additional inference overhead or yield suboptimal performance.\\nIn this work, we present a NMS-free training strategy for YOLOs with dual label assignments and\\nconsistent matching metric, achieving both high efficiency and competitive performance.\\nDual label assignments. Unlike one-to-many assignment, one-to-one matching assigns only one\\nprediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak\\nsupervision, which causes suboptimal accuracy and convergence speed [ 82]. Fortunately, this\\ndeficiency can be compensated for by the one-to-many assignment [6]. To achieve this, we introduce\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Backbone PAN\\nRegression\\nClassification\\nOne-to-many Head\\nDual Label Assignments Consistent Match. Metric\\n𝑚 = 𝑠 ⋅ 𝑝𝛼 ⋅ IoU \\u0de0𝑏, 𝑏\\n𝛽\\n \\nRegression\\nClassification\\nOne-to-one Head\\nInput\\n(a) (b)\\n1\\n23\\n4\\n6\\n5\\nFigure 2: (a) Consistent dual assignments for NMS-free training. (b) Frequency of one-to-one\\nassignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs αo2m=0.5 and\\nβo2m=6 by default [21]. For consistency, αo2o=0.5; βo2o=6. For inconsistency, αo2o=0.5; βo2o =2.\\ndual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown\\nin Fig. 2.(a), we incorporate another one-to-one head for YOLOs. It retains the identical structure\\nand adopts the same optimization objectives as the original one-to-many branch but leverages the\\none-to-one matching to obtain label assignments. During training, two heads are jointly optimized\\nwith the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='one-to-one matching to obtain label assignments. During training, two heads are jointly optimized\\nwith the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-\\nto-many assignment. During inference, we discard the one-to-many head and utilize the one-to-one\\nhead to make predictions. This enables YOLOs for the end-to-end deployment without incurring any\\nadditional inference cost. Besides, in the one-to-one matching, we adopt the top one selection, which\\nachieves the same performance as Hungarian matching [4] with less extra training time.\\nConsistent matching metric. During assignments, both one-to-one and one-to-many approaches\\nleverage a metric to quantitatively assess the level of concordance between predictions and instances.\\nTo achieve prediction aware matching for both branches, we employ a uniform matching metric,i.e.,\\nm(α, β) = s · pα · IoU(ˆb, b)β, (1)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='To achieve prediction aware matching for both branches, we employ a uniform matching metric,i.e.,\\nm(α, β) = s · pα · IoU(ˆb, b)β, (1)\\nwhere p is the classification score, ˆb and b denote the bounding box of prediction and instance,\\nrespectively. s represents the spatial prior indicating whether the anchor point of prediction is within\\nthe instance [21, 65, 29, 70]. α and β are two important hyperparameters that balance the impact\\nof the semantic prediction task and the location regression task. We denote the one-to-many and\\none-to-one metrics as mo2m=m(αo2m, βo2m) and mo2o=m(αo2o, βo2o), respectively. These metrics\\ninfluence the label assignments and supervision information for the two heads.\\nIn dual label assignments, the one-to-many branch provides much richer supervisory signals than\\none-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that\\nof one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='one-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that\\nof one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many\\nhead’s optimization. As a result, the one-to-one head can provide improved quality of samples during\\ninference, leading to better performance. To this end, we first analyze the supervision gap between the\\ntwo heads. Due to the randomness during training, we initiate our examination in the beginning with\\ntwo heads initialized with the same values and producing the same predictions, i.e., one-to-one head\\nand one-to-many head generate the same p and IoU for each prediction-instance pair. We note that the\\nregression targets of two branches do not conflict, as matched predictions share the same targets and\\nunmatched predictions are ignored. The supervision gap thus lies in the different classification targets.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='unmatched predictions are ignored. The supervision gap thus lies in the different classification targets.\\nGiven an instance, we denote its largest IoU with predictions as u∗, and the largest one-to-many and\\none-to-one matching scores as m∗\\no2m and m∗\\no2o, respectively. Suppose that one-to-many branch yields\\nthe positive samples Ω and one-to-one branch selects i-th prediction with the metric mo2o,i=m∗\\no2o, we\\ncan then derive the classification target to2m,j=u∗ · mo2m,j\\nm∗\\no2m\\n≤ u∗ for j ∈ Ω and to2o,i=u∗ · mo2o,i\\nm∗\\no2o\\n=u∗\\nfor task aligned loss as in [21, 65, 29, 70, 15]. The supervision gap between two branches can thus\\nbe derived by the 1-Wasserstein distance [46] of different classification objectives,i.e.,\\nA = to2o,i − I(i ∈ Ω)to2m,i +\\nX\\nk∈Ω\\\\{i}\\nto2m,k, (2)\\nWe can observe that the gap decreases as to2m,i increases, i.e., i ranks higher within Ω. It reaches the\\nminimum when to2m,i=u∗, i.e., i is the best positive sample in Ω, as shown in Fig. 2.(a). To achieve'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='minimum when to2m,i=u∗, i.e., i is the best positive sample in Ω, as shown in Fig. 2.(a). To achieve\\nthis, we present the consistent matching metric, i.e., αo2o=r · αo2m and βo2o=r · βo2m, which implies\\nmo2o=mr\\no2m. Therefore, the best positive sample for one-to-many head is also the best for one-to-one\\nhead. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, we\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='take r=1, by default, i.e., αo2o=αo2m and βo2o=βo2m. To verify the improved supervision alignment,\\nwe count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results\\nafter training. As shown in Fig. 2.(b), the alignment is improved under the consistent matching metric.\\nFor a more comprehensive understanding of the mathematical proof, please refer to the appendix.\\nDiscussion with other counter-parts. Similarly, previous works [28, 5, 77, 54, 6, 82, 45] explore the\\ndifferent assignments to accelerate the training convergence and improve the performance for different\\nnetworks. For example, H-DETR [28], Group-DETR [5], and MS-DETR [77] introduce one-to-many\\nmatching in conjunction with the original one-to-one matching by hybrid or multiple group label\\nassignments, to improve upon DETR. Differently, to achieve the one-to-many matching, they usually\\nintroduce extra queries or repeat ground truths for bipartite matching, or select top several queries'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='assignments, to improve upon DETR. Differently, to achieve the one-to-many matching, they usually\\nintroduce extra queries or repeat ground truths for bipartite matching, or select top several queries\\nfrom the matching scores, while we adopt the prediction aware assignment that incorporates the\\nspatial prior. Besides, LRANet [54] employs the dense assignment and sparse assignment branches\\nfor training, which all belong to the one-to-many assignment, while we adopt the one-to-many and\\none-to-one branches. DEYO [ 45, 43, 44] investigates the step-by-step training with one-to-many\\nmatching in the first stage for convolutional encoder and one-to-one matching in the second stage for\\ntransformer decoder, while we avoid the transformer decoder for end-to-end inference. Compared\\nwith works [6, 80] which incorporate dual assignments for CNN-based detectors, we further analyze\\nthe supervision gap between the two heads and present the consistent matching metric for YOLOs to'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='with works [6, 80] which incorporate dual assignments for CNN-based detectors, we further analyze\\nthe supervision gap between the two heads and present the consistent matching metric for YOLOs to\\nreduce the theoretical supervision gap. It improves performance through better supervision alignment\\nand eliminates the need for hyper-parameter tuning.\\n3.2 Holistic Efficiency-Accuracy Driven Model Design\\nIn addition to the post-processing, the model architectures of YOLOs also pose great challenges to the\\nefficiency-accuracy trade-offs [50, 8, 29]. Although previous works explore various design strategies,\\nthe comprehensive inspection for various components in YOLOs is still lacking. Consequently, the\\nmodel architecture exhibits non-negligible computational redundancy and constrained capability,\\nwhich impedes its potential for achieving high efficiency and performance. Here, we aim to holistically\\nperform model designs for YOLOs from both efficiency and accuracy perspectives.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='which impedes its potential for achieving high efficiency and performance. Here, we aim to holistically\\nperform model designs for YOLOs from both efficiency and accuracy perspectives.\\nEfficiency driven model design. The components in YOLO consist of the stem, downsampling\\nlayers, stages with basic building blocks, and the head. The stem incurs few computational cost and\\nwe thus perform efficiency driven model design for other three parts.\\n(1) Lightweight classification head. The classification and regression heads usually share the same\\narchitecture in YOLOs. However, they exhibit notable disparities in computational overhead. For\\nexample, the FLOPs and parameter count of the classification head (5.95G/1.51M) are 2.5× and 2.4×\\nthose of the regression head (2.34G/0.64M) in YOLOv8-S, respectively. However, after analyzing\\nthe impact of classification error and the regression error (seeing Tab. 6), we find that the regression'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='those of the regression head (2.34G/0.64M) in YOLOv8-S, respectively. However, after analyzing\\nthe impact of classification error and the regression error (seeing Tab. 6), we find that the regression\\nhead undertakes more significance for the performance of YOLOs. Consequently, we can reduce the\\noverhead of classification head without worrying about hurting the performance greatly. Therefore,\\nwe simply adopt a lightweight architecture for the classification head, which consists of two depthwise\\nseparable convolutions [25, 9] with the kernel size of 3×3 followed by a 1×1 convolution.\\n(2) Spatial-channel decoupled downsampling. YOLOs typically leverage regular 3 ×3 standard\\nconvolutions with stride of 2, achieving spatial downsampling (fromH × W to H\\n2 × W\\n2 ) and channel\\ntransformation (from C to 2C) simultaneously. This introduces non-negligible computational cost of\\nO(9\\n2 HW C2) and parameter count ofO(18C2). Instead, we propose to decouple the spatial reduction'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='transformation (from C to 2C) simultaneously. This introduces non-negligible computational cost of\\nO(9\\n2 HW C2) and parameter count ofO(18C2). Instead, we propose to decouple the spatial reduction\\nand channel increase operations, enabling more efficient downsampling. Specifically, we firstly\\nleverage the pointwise convolution to modulate the channel dimension and then utilize the depthwise\\nconvolution to perform spatial downsampling. This reduces the computational cost to O(2HW C2 +\\n9\\n2 HW C) and the parameter count to O(2C2 +18C). Meanwhile, it maximizes information retention\\nduring downsampling, leading to competitive performance with latency reduction.\\n(3) Rank-guided block design. YOLOs usually employ the same basic building block for all stages [29,\\n65], e.g., the bottleneck block in YOLOv8 [21]. To thoroughly examine such homogeneous design for\\nYOLOs, we utilize the intrinsic rank [33, 16] to analyze the redundancy2 of each stage. Specifically,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='65], e.g., the bottleneck block in YOLOv8 [21]. To thoroughly examine such homogeneous design for\\nYOLOs, we utilize the intrinsic rank [33, 16] to analyze the redundancy2 of each stage. Specifically,\\nwe calculate the numerical rank of the last convolution in the last basic block in each stage, which\\ncounts the number of singular values larger than a threshold. Fig. 3.(a) presents the results of\\n2A lower rank implies greater redundancy, while a higher rank signifies more condensed information.\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='1×1\\nC\\n×𝑁 +\\n3×3 DW\\n1×1\\n3×3 DW\\n1×1\\n3×3 DW\\nSplit\\n1×1\\nCIB\\nCIB\\n1×1\\nSplit\\nC\\nMHSA\\n+\\nFFN\\n+\\n1×1\\n×𝑁𝑝𝑠𝑎\\n(a) (b) (c)\\nFigure 3: (a) The intrinsic ranks across stages and models in YOLOv8. The stage in the backbone\\nand neck is numbered in the order of model forward process. The numerical rank r is normalized\\nto r/Co for y-axis and its threshold is set to λmax/2, by default, where Co denotes the number of\\noutput channels and λmax is the largest singular value. It can be observed that deep stages and large\\nmodels exhibit lower intrinsic rank values. (b) The compact inverted block (CIB). (c) The partial\\nself-attention module (PSA).\\nYOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This\\nobservation suggests that simply applying the same block design for all stages is suboptimal for the\\nbest capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='observation suggests that simply applying the same block design for all stages is suboptimal for the\\nbest capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme\\nwhich aims to decrease the complexity of stages that are shown to be redundant using compact\\narchitecture design. We first present a compact inverted block (CIB) structure, which adopts the cheap\\ndepthwise convolutions for spatial mixing and cost-effective pointwise convolutions for channel\\nmixing, as shown in Fig. 3.(b). It can serve as the efficient basic building block, e.g., embedded in the\\nELAN structure [64, 21] (Fig. 3.(b)). Then, we advocate a rank-guided block allocation strategy to\\nachieve the best efficiency while maintaining competitive capacity. Specifically, given a model, we\\nsort its all stages based on their intrinsic ranks in ascending order. We further inspect the performance\\nvariation of replacing the basic block in the leading stage with CIB. If there is no performance'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='variation of replacing the basic block in the leading stage with CIB. If there is no performance\\ndegradation compared with the given model, we proceed with the replacement of the next stage and\\nhalt the process otherwise. Consequently, we can implement adaptive compact block designs across\\nstages and model scales, achieving higher efficiency without compromising performance. Due to the\\npage limit, we provide the details of the algorithm in the appendix.\\nAccuracy driven model design. We further explore the large-kernel convolution and self-attention\\nfor accuracy driven design, aiming to boost the performance under minimal cost.\\n(1) Large-kernel convolution. Employing large-kernel depthwise convolution is an effective way\\nto enlarge the receptive field and enhance the model’s capability [ 10, 40, 39]. However, simply\\nleveraging them in all stages may introduce contamination in shallow features used for detecting'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='to enlarge the receptive field and enhance the model’s capability [ 10, 40, 39]. However, simply\\nleveraging them in all stages may introduce contamination in shallow features used for detecting\\nsmall objects, while also introducing significantI/O overhead and latency in high-resolution stages [8].\\nTherefore, we propose to leverage the large-kernel depthwise convolutions in CIB within the deep\\nstages. Specifically, we increase the kernel size of the second 3×3 depthwise convolution in the CIB to\\n7×7, following [39]. Additionally, we employ the structural reparameterization technique [11, 10, 59]\\nto bring another 3 ×3 depthwise convolution branch to alleviate the optimization issue without\\ninference overhead. Furthermore, as the model size increases, its receptive field naturally expands,\\nwith the benefit of using large-kernel convolutions diminishing. Therefore, we only adopt large-kernel\\nconvolution for small model scales.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='with the benefit of using large-kernel convolutions diminishing. Therefore, we only adopt large-kernel\\nconvolution for small model scales.\\n(2) Partial self-attention (PSA). Self-attention [58] is widely employed in various visual tasks due\\nto its remarkable global modeling capability [38, 14, 76]. However, it exhibits high computational\\ncomplexity and memory footprint. To address this, in light of the prevalent attention head redun-\\ndancy [69], we present an efficient partial self-attention (PSA) module design, as shown in Fig. 3.(c).\\nSpecifically, we evenly partition the features across channels into two parts after the 1×1 convolution.\\nWe only feed one part into the NPSA blocks comprised of multi-head self-attention module (MHSA)\\nand feed-forward network (FFN). Two parts are then concatenated and fused by a 1×1 convolution.\\nBesides, we follow [22] to assign the dimensions of the query and key to half of that of the value in'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='and feed-forward network (FFN). Two parts are then concatenated and fused by a 1×1 convolution.\\nBesides, we follow [22] to assign the dimensions of the query and key to half of that of the value in\\nMHSA and replace the LayerNorm [1] with BatchNorm [27] for fast inference. Furthermore, PSA is\\nonly placed after the Stage 4 with the lowest resolution, avoiding the excessive overhead from the\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models.\\nLatencyf denotes the latency in the forward process of model without post-processing. † means the\\nresults of YOLOv10 with the original one-to-many training using NMS. All results below are without\\nthe additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\\nModel #Param.(M) FLOPs(G) AP val(%) Latency(ms) Latency f (ms)\\nYOLOv6-3.0-N [29] 4.7 11.4 37.0 2.69 1.76\\nGold-YOLO-N [60] 5.6 12.1 39.6 2.92 1.82\\nYOLOv8-N [21] 3.2 8.7 37.3 6.16 1.77\\nYOLOv10-N (Ours) 2.3 6.7 38.5 / 39.5† 1.84 1.79\\nYOLOv6-3.0-S [29] 18.5 45.3 44.3 3.42 2.35\\nGold-YOLO-S [60] 21.5 46.0 45.4 3.82 2.73\\nYOLO-MS-XS [8] 4.5 17.4 43.4 8.23 2.80\\nYOLO-MS-S [8] 8.1 31.2 46.2 10.12 4.83\\nYOLOv8-S [21] 11.2 28.6 44.9 7.07 2.33\\nYOLOv9-S [65] 7.1 26.4 46.7 - -\\nRT-DETR-R18 [78] 20.0 60.0 46.5 4.58 4.49\\nYOLOv10-S (Ours) 7.2 21.6 46.3 / 46.8† 2.49 2.39\\nYOLOv6-3.0-M [29] 34.9 85.8 49.1 5.63 4.56'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv9-S [65] 7.1 26.4 46.7 - -\\nRT-DETR-R18 [78] 20.0 60.0 46.5 4.58 4.49\\nYOLOv10-S (Ours) 7.2 21.6 46.3 / 46.8† 2.49 2.39\\nYOLOv6-3.0-M [29] 34.9 85.8 49.1 5.63 4.56\\nGold-YOLO-M [60] 41.3 87.5 49.8 6.38 5.45\\nYOLO-MS [8] 22.2 80.2 51.0 12.41 7.30\\nYOLOv8-M [21] 25.9 78.9 50.6 9.50 5.09\\nYOLOv9-M [65] 20.0 76.3 51.1 - -\\nRT-DETR-R34 [78] 31.0 92.0 48.9 6.32 6.21\\nRT-DETR-R50m [78] 36.0 100.0 51.3 6.90 6.84\\nYOLOv10-M (Ours) 15.4 59.1 51.1 / 51.3† 4.74 4.63\\nYOLOv6-3.0-L [29] 59.6 150.7 51.8 9.02 7.90\\nGold-YOLO-L [60] 75.1 151.7 51.8 10.65 9.78\\nYOLOv9-C [65] 25.3 102.1 52.5 10.57 6.13\\nYOLOv10-B (Ours) 19.1 92.0 52.5 / 52.7† 5.74 5.67\\nYOLOv8-L [21] 43.7 165.2 52.9 12.39 8.06\\nRT-DETR-R50 [78] 42.0 136.0 53.1 9.20 9.07\\nYOLOv10-L (Ours) 24.4 120.3 53.2 / 53.4† 7.28 7.21\\nYOLOv8-X [21] 68.2 257.8 53.9 16.86 12.83\\nRT-DETR-R101 [78] 76.0 259.0 54.3 13.71 13.58\\nYOLOv10-X (Ours) 29.5 160.4 54.4 / 54.4† 10.70 10.60'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10-L (Ours) 24.4 120.3 53.2 / 53.4† 7.28 7.21\\nYOLOv8-X [21] 68.2 257.8 53.9 16.86 12.83\\nRT-DETR-R101 [78] 76.0 259.0 54.3 13.71 13.58\\nYOLOv10-X (Ours) 29.5 160.4 54.4 / 54.4† 10.70 10.60\\nquadratic computational complexity of self-attention. In this way, the global representation learning\\nability can be incorporated into YOLOs with low computational costs, which well enhances the\\nmodel’s capability and leads to improved performance.\\n4 Experiments\\n4.1 Implementation Details\\nWe select YOLOv8 [21] as our baseline model, due to its commendable latency-accuracy balance\\nand its availability in various model sizes. We employ the consistent dual assignments for NMS-free\\ntraining and perform holistic efficiency-accuracy driven model design based on it, which brings our\\nYOLOv10 models. YOLOv10 has the same variants as YOLOv8, i.e., N / S / M / L / X. Besides, we\\nderive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10 models. YOLOv10 has the same variants as YOLOv8, i.e., N / S / M / L / X. Besides, we\\nderive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We\\nverify the proposed detector on COCO [35] under the same training-from-scratch setting [21, 65, 62].\\nMoreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following [78].\\n4.2 Comparison with state-of-the-arts\\nAs shown in Tab. 1, our YOLOv10 achieves the state-of-the-art performance and end-to-end latency\\nacross various model scales. We first compare YOLOv10 with our baseline models, i.e., YOLOv8.\\nOn N / S / M / L / X five variants, our YOLOv10 achieves 1.2% / 1.4% / 0.5% / 0.3% / 0.5% AP\\nimprovements, with 28% / 36% / 41% / 44% / 57% fewer parameters, 23% / 24% / 25% / 27% / 38%\\nless calculations, and 70% / 65% / 50% / 41% / 37% lower latencies. Compared with other YOLOs,\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\\n# Model NMS-free. Efficiency. Accuracy. #Param.(M) FLOPs(G) AP val(%) Latency(ms)\\n1\\nYOLOv10-S\\n11.2 28.6 44.9 7.07\\n2 ✓ 11.2 28.6 44.3 2.44\\n3 ✓ ✓ 6.2 20.8 44.5 2.31\\n4 ✓ ✓ ✓ 7.2 21.6 46.3 2.49\\n5\\nYOLOv10-M\\n25.9 78.9 50.6 9.50\\n6 ✓ 25.9 78.9 50.3 5.22\\n7 ✓ ✓ 14.1 58.1 50.4 4.57\\n8 ✓ ✓ ✓ 15.4 59.1 51.1 4.74\\nTable 3: Dual assign.\\no2m o2o AP Latency\\n✓ 44.9 7.07\\n✓ 43.4 2.44\\n✓ ✓ 44.3 2.44\\nTable 4: Matching metric.\\nαo2o βo2o APval αo2o βo2o APval\\n0.5 2.0 42.7 0.25 3.0 44.3\\n0.5 4.0 44.2 0.25 6.0 43.5\\n0.5 6.0 44.3 1.0 6.0 43.9\\n0.5 8.0 44.0 1.0 12.0 44.3\\nTable 5: Efficiency. for YOLOv10-S/M.\\n# Model #Param FLOPs AP val Latency\\n1 base. 11.2/25.9 28.6/78.9 44.3/50.3 2.44/5.22\\n2 +cls. 9.9/23.2 23.5/67.7 44.2/50.2 2.39/5.07\\n3 +downs. 8.0/19.7 22.2/65.0 44.4/50.4 2.36/4.97\\n4 +block. 6.2/14.1 20.8/58.1 44.5/50.4 2.31/4.57\\nYOLOv10 also exhibits superior trade-offs between accuracy and computational cost. Specifically,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='3 +downs. 8.0/19.7 22.2/65.0 44.4/50.4 2.36/4.97\\n4 +block. 6.2/14.1 20.8/58.1 44.5/50.4 2.31/4.57\\nYOLOv10 also exhibits superior trade-offs between accuracy and computational cost. Specifically,\\nfor lightweight and small models, YOLOv10-N / S outperforms YOLOv6-3.0-N / S by 1.5 AP and 2.0\\nAP, with 51% / 61% fewer parameters and 41% / 52% less computations, respectively. For medium\\nmodels, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46% / 62% latency\\nreduction under the same or better performance, respectively. For large models, compared with\\nGold-YOLO-L, our YOLOv10-L shows 68% fewer parameters and 32% lower latency, along with\\na significant improvement of 1.4% AP. Furthermore, compared with RT-DETR, YOLOv10 obtains\\nsignificant performance and latency improvements. Notably, YOLOv10-S / X achieves 1.8 × and\\n1.3× faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='significant performance and latency improvements. Notably, YOLOv10-S / X achieves 1.8 × and\\n1.3× faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance.\\nThese results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach.\\nWe consider the performance and the latency of model forward process (Latencyf ) in this situation,\\nfollowing [62, 21, 60]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance\\nand efficiency across different model scales, indicating the effectiveness of our architectural designs.\\n4.3 Model Analyses\\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It\\ncan be observed that our NMS-free training with consistent dual assignments significantly reduces\\nthe end-to-end latency of YOLOv10-S by 4.63ms, while maintaining competitive performance of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='can be observed that our NMS-free training with consistent dual assignments significantly reduces\\nthe end-to-end latency of YOLOv10-S by 4.63ms, while maintaining competitive performance of\\n44.3% AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters\\nand 20.8 GFlOPs, with a considerable latency reduction of 0.65ms for YOLOv10-M, well showing\\nits effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements\\nof 1.8 AP and 0.7 AP for YOLOv10-S and YOLOv10-M, alone with only 0.18ms and 0.17ms latency\\noverhead, respectively, which well demonstrates its superiority.\\nAnalyses for NMS-free training.\\n• Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can\\nbring both rich supervision of one-to-many (o2m) branch during training and high efficiency of\\none-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., #1 in'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='bring both rich supervision of one-to-many (o2m) branch during training and high efficiency of\\none-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., #1 in\\nTab. 2. Specifically, we introduce baselines for training with only o2m branch and only o2o branch,\\nrespectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\\n• Consistent matching metric. We introduce consistent matching metric to make the one-to-one head\\nmore harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., #1 in\\nTab. 2, under differentαo2o and βo2o. As shown in Tab. 4, the proposed consistent matching metric,\\ni.e., αo2o=r · αo2m and βo2o=r · βo2m, can achieve the optimal performance, where αo2m=0.5 and\\nβo2m=6.0 in the one-to-many head [21]. Such an improvement can be attributed to the reduction\\nof the supervision gap (Eq. (2)), which provides improved supervision alignment between two'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='βo2m=6.0 in the one-to-many head [21]. Such an improvement can be attributed to the reduction\\nof the supervision gap (Eq. (2)), which provides improved supervision alignment between two\\nbranches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive\\nhyper-parameter tuning, which is appealing in practical scenarios.\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10-N\\n YOLOv10-S\\n YOLOv10-M\\n YOLOv10-B\\n YOLOv10-L\\n YOLOv10-X\\n0.2\\n0.3\\n0.4\\nFigure 4: The average cosine similarity of each anchor point’s extracted features with all others.\\nTable 6: cls. results.\\nbase. +cls.\\nAPval 44.3 44.2\\nAPval\\nw/o c 59.9 59.9\\nAPval\\nw/o r 64.5 64.2\\nTable 7: Results of d.s.\\nModel AP val Latency\\nbase. 43.7 2.33\\nours 44.4 2.36\\nTable 8: Results of CIB.\\nModel AP val Latency\\nIRB 43.7 2.30\\nIRB-DW 44.2 2.30\\nours 44.5 2.31\\nTable 9: Rank-guided.\\nStages with CIB AP val\\nempty 44.4\\n8 44.5\\n8,4, 44.5\\n8,4,7 44.3\\n• Performance gap compared with one-to-many training. Although achieving superior end-to-end\\nperformance under NMS-free training, we observe that there still exists the performance gap\\ncompared with the original one-to-many training using NMS, as shown in Tab. 3 and Tab. 1.\\nBesides, we note that the gap diminishes as the model size increases. Therefore, we reasonably\\nconcludes that such a gap can be attributed to the limitations in the model capability. Notably,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Besides, we note that the gap diminishes as the model size increases. Therefore, we reasonably\\nconcludes that such a gap can be attributed to the limitations in the model capability. Notably,\\nunlike the original one-to-many training using NMS, the NMS-free training necessitates more\\ndiscriminative features for one-to-one matching. In the case of the YOLOv10-N model, its limited\\ncapacity results in extracted features that lack sufficient discriminability, leading to a more notable\\nperformance gap of 1.0% AP. In contrast, the YOLOv10-X model, which possesses stronger\\ncapability and more discriminative features, shows no performance gap between two training\\nstrategies. In Fig. 4, we visualize the average cosine similarity of each anchor point’s extracted\\nfeatures with those of all other anchor points on the COCO val set. We observe that as the model\\nsize increases, the feature similarity between anchor points exhibits a downward trend, which'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='features with those of all other anchor points on the COCO val set. We observe that as the model\\nsize increases, the feature similarity between anchor points exhibits a downward trend, which\\nbenefits the one-to-one matching. Based on this insight, we will explore approaches to further\\nreduce the gap and achieve higher end-to-end performance in the future work.\\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the\\nefficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model\\nwithout efficiency-accuracy driven model design,i.e., #2/#6 in Tab. 2. As shown in Tab. 5, each design\\ncomponent, including lightweight classification head, spatial-channel decoupled downsampling, and\\nrank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency.\\nImportantly, these improvements are achieved while maintaining competitive performance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='rank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency.\\nImportantly, these improvements are achieved while maintaining competitive performance.\\n• Lightweight classification head. We analyze the impact of category and localization errors of\\npredictions on the performance, based on the YOLOv10-S of #1 and #2 in Tab. 5, like [ 7].\\nSpecifically, we match the predictions to the instances by the one-to-one assignment. Then,\\nwe substitute the predicted category score with instance labels, resulting in AP val\\nw/o c with no\\nclassification errors. Similarly, we replace the predicted locations with those of instances, yielding\\nAPval\\nw/o rwith no regression errors. As shown in Tab. 6, AP val\\nw/o ris much higher than AP val\\nw/o c,\\nrevealing that eliminating the regression errors achieves greater improvement. The performance\\nbottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='revealing that eliminating the regression errors achieves greater improvement. The performance\\nbottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification\\nhead can allow higher efficiency without compromising the performance.\\n• Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency,\\nwhere the channel dimensions are first increased by pointwise convolution (PW) and the resolution\\nis then reduced by depthwise convolution (DW) for maximal information retention. We compare it\\nwith the baseline way of spatial reduction by DW followed by channel modulation by PW, based\\non the YOLOv10-S of #3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the\\n0.7% AP improvement by enjoying less information loss during downsampling.\\n• Compact inverted block (CIB).We introduce CIB as the compact basic building block. We verify its'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='0.7% AP improvement by enjoying less information loss during downsampling.\\n• Compact inverted block (CIB).We introduce CIB as the compact basic building block. We verify its\\neffectiveness based on the YOLOv10-S of #4 in the Tab. 5. Specifically, we introduce the inverted\\nresidual block [51] (IRB) as the baseline, which achieves the suboptimal 43.7% AP, as shown in\\nTab. 8. We then append a 3×3 depthwise convolution (DW) after it, denoted as “IRB-DW”, which\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 10: Accuracy. for S/M.\\n# Model AP val Latency\\n1 base. 44.5/50.4 2.31/4.57\\n2 +L.k. 44.9/- 2.34/-\\n3 +PSA 46.3/51.1 2.49/4.74\\nTable 11: L.k. results.\\nModel AP val Latency\\nk.s.=5 44.7 2.32\\nk.s.=7 44.9 2.34\\nk.s.=9 44.9 2.37\\nw/o rep. 44.8 2.34\\nTable 12: L.k. usage.\\nw/o L.k. w/ L.k.\\nN 36.3 36.6\\nS 44.5 44.9\\nM 50.4 50.4\\nTable 13: PSA results.\\nModel AP val Latency\\nPSA 46.3 2.49\\nTrans. 46.0 2.54\\nNPSA = 1 46.3 2.49\\nNPSA = 2 46.5 2.59\\nbrings 0.5% AP improvement. Compared with “IRB-DW”, our CIB further achieves 0.3% AP\\nimprovement by prepending another DW with minimal overhead, indicating its superiority.\\n• Rank-guided block design. We introduce the rank-guided block design to adaptively integrate\\ncompact block design for improving the model efficiency. We verify its benefit based on the\\nYOLOv10-S of #3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks\\nare Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10-S of #3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks\\nare Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the\\nbottleneck block in each stage with the efficient CIB, we observe the performance degradation\\nstarting from Stage 7. In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can\\nthus adopt the efficient block design without compromising the performance. These results indicate\\nthat rank-guided block design can serve as an effective strategy for higher model efficiency.\\nAnalyses for accuracy driven model design. We present the results of gradually integrating the\\naccuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model\\nafter incorporating efficiency driven design, i.e., #3/#7 in Tab. 2. As shown in Tab. 10, the adoption\\nof large-kernel convolution and PSA module leads to the considerable performance improvements'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='after incorporating efficiency driven design, i.e., #3/#7 in Tab. 2. As shown in Tab. 10, the adoption\\nof large-kernel convolution and PSA module leads to the considerable performance improvements\\nof 0.4% AP and 1.4% AP for YOLOv10-S under minimal latency increase of 0.03ms and 0.15ms,\\nrespectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\\n• Large-kernel convolution. We first investigate the effect of different kernel sizes based on the\\nYOLOv10-S of #2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size\\nincreases and stagnates around the kernel size of 7×7, indicating the benefit of large perception field.\\nBesides, removing the reparameterization branch during training achieves 0.1% AP degradation,\\nshowing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel\\nconvolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel\\nconvolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no\\nimprovements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We\\nthus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\\n• Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the\\nglobal modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10-\\nS of #3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN,\\nas the baseline, denoted as “Trans.”. As shown in Tab. 13, compared with it, PSA brings 0.3% AP\\nimprovement with 0.05ms latency reduction. The performance enhancement may be attributed to\\nthe alleviation of optimization problem [68, 10] in self-attention, by mitigating the redundancy'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='improvement with 0.05ms latency reduction. The performance enhancement may be attributed to\\nthe alleviation of optimization problem [68, 10] in self-attention, by mitigating the redundancy\\nin attention heads. Moreover, we investigate the impact of different NPSA. As shown in Tab. 13,\\nincreasing NPSA to 2 obtains 0.2% AP improvement but with 0.1ms latency overhead. Therefore,\\nwe set NPSA to 1, by default, to enhance the model capability while maintaining high efficiency.\\n5 Conclusion\\nIn this paper, we target both the post-processing and model architecture throughout the detection\\npipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMS-\\nfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the\\nholistic efficiency-accuracy driven model design strategy, improving the performance-efficiency trade-\\noffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency trade-\\noffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments\\nshow that YOLOv10 achieves the state-of-the-art performance and latency compared with other\\nadvanced detectors, well demonstrating its superiority.\\n6 Acknowledgments\\nThis work was supported by National Natural Science Foundation of China (Nos. 61925107,\\n62271281) and Beijing Natural Science Foundation (No. L223023).\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed\\nand accuracy of object detection, 2020.\\n[3] Daniel Bogdoll, Maximilian Nitsche, and J Marius Zöllner. Anomaly detection in autonomous\\ndriving: A survey. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 4488–4499, 2022.\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In European conference on\\ncomputer vision, pages 213–229. Springer, 2020.\\n[5] Qiang Chen, Xiaokang Chen, Jian Wang, Shan Zhang, Kun Yao, Haocheng Feng, Junyu Han,\\nErrui Ding, Gang Zeng, and Jingdong Wang. Group detr: Fast detr training with group-wise one-\\nto-many assignment. In Proceedings of the IEEE/CVF International Conference on Computer'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Errui Ding, Gang Zeng, and Jingdong Wang. Group detr: Fast detr training with group-wise one-\\nto-many assignment. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision, pages 6633–6642, 2023.\\n[6] Yiqun Chen, Qiang Chen, Qinghao Hu, and Jian Cheng. Date: Dual assignment for end-to-end\\nfully convolutional object detection. arXiv preprint arXiv:2211.13859, 2022.\\n[7] Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, and Jian Cheng. Enhancing\\nyour trained detrs with box refinement. arXiv preprint arXiv:2307.11828, 2023.\\n[8] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng.\\nYolo-ms: rethinking multi-scale representation learning for real-time object detection. arXiv\\npreprint arXiv:2308.05480, 2023.\\n[9] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceed-\\nings of the IEEE conference on computer vision and pattern recognition , pages 1251–1258,\\n2017.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[9] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceed-\\nings of the IEEE conference on computer vision and pattern recognition , pages 1251–1258,\\n2017.\\n[10] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to\\n31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition, pages 11963–11975, 2022.\\n[11] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun.\\nRepvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pages 13733–13742, 2021.\\n[12] Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel\\nFernando Tello Gamarra. Mobile robot navigation using an object recognition software with\\nrgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):1290–1305, 2019.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Fernando Tello Gamarra. Mobile robot navigation using an object recognition software with\\nrgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):1290–1305, 2019.\\n[13] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet:\\nKeypoint triplets for object detection. In Proceedings of the IEEE/CVF international conference\\non computer vision, pages 6569–6578, 2019.\\n[14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\\nimage synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 12873–12883, 2021.\\n[15] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. Tood: Task-aligned\\none-stage object detection. In 2021 IEEE/CVF International Conference on Computer Vision\\n(ICCV), pages 3490–3499. IEEE Computer Society, 2021.\\n[16] Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='(ICCV), pages 3490–3499. IEEE Computer Society, 2021.\\n[16] Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha.\\nRank diminishing in deep neural networks. Advances in Neural Information Processing Systems,\\n35:33054–33065, 2022.\\n[17] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in\\n2021. arXiv preprint arXiv:2107.08430, 2021.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[18] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V\\nLe, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance\\nsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 2918–2928, 2021.\\n[19] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer\\nvision, pages 1440–1448, 2015.\\n[20] Jocher Glenn. Yolov5 release v7.0.https: // github. com/ ultralytics/ yolov5/ tree/\\nv7. 0, 2022.\\n[21] Jocher Glenn. Yolov8. https: // github. com/ ultralytics/ ultralytics/ tree/\\nmain , 2023.\\n[22] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé\\nJégou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference.\\nIn Proceedings of the IEEE/CVF international conference on computer vision, pages 12259–\\n12269, 2021.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='In Proceedings of the IEEE/CVF international conference on computer vision, pages 12259–\\n12269, 2021.\\n[23] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of\\nthe IEEE international conference on computer vision, pages 2961–2969, 2017.\\n[24] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition , pages\\n4507–4515, 2017.\\n[25] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias\\nWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural\\nnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\\n[26] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 3588–3597, 2018.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='detection. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 3588–3597, 2018.\\n[27] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\\nby reducing internal covariate shift. In International conference on machine learning, pages\\n448–456. pmlr, 2015.\\n[28] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang,\\nand Han Hu. Detrs with hybrid matching. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition, pages 19702–19712, 2023.\\n[29] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming\\nXu, and Xiangxiang Chu. Yolov6 v3.0: A full-scale reloading.arXiv preprint arXiv:2301.05586,\\n2023.\\n[30] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate\\ndetr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='2023.\\n[30] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate\\ndetr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition, pages 13619–13627, 2022.\\n[31] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss\\nv2: Learning reliable localization quality estimation for dense object detection. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 11632–11641,\\n2021.\\n[32] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang.\\nGeneralized focal loss: Learning qualified and distributed bounding boxes for dense object\\ndetection. Advances in Neural Information Processing Systems, 33:21002–21012, 2020.\\n[33] Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design\\nfor gpu-efficient networks. arXiv preprint arXiv:2006.14090, 2020.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[33] Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design\\nfor gpu-efficient networks. arXiv preprint arXiv:2006.14090, 2020.\\n[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense\\nobject detection. In Proceedings of the IEEE international conference on computer vision ,\\npages 2980–2988, 2017.\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13, pages 740–755. Springer, 2014.\\n[36] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang.\\nDab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329,\\n2022.\\n[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for\\ninstance segmentation. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 8759–8768, 2018.\\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\\nof the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\\n[39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\\nXie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition, pages 11976–11986, 2022.\\n[40] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive\\nfield in deep convolutional neural networks. Advances in neural information processing systems,\\n29, 2016.\\n[41] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong\\nZhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors.\\narXiv preprint arXiv:2212.07784, 2022.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Zhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors.\\narXiv preprint arXiv:2212.07784, 2022.\\n[42] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and\\nJingdong Wang. Conditional detr for fast training convergence. InProceedings of the IEEE/CVF\\ninternational conference on computer vision, pages 3651–3660, 2021.\\n[43] Haodong Ouyang. Deyov2: Rank feature with greedy matching for end-to-end object detection.\\narXiv preprint arXiv:2306.09165, 2023.\\n[44] Haodong Ouyang. Deyov3: Detr with yolo for real-time object detection. arXiv preprint\\narXiv:2309.11851, 2023.\\n[45] Haodong Ouyang. Deyo: Detr with yolo for end-to-end object detection. arXiv preprint\\narXiv:2402.16370, 2024.\\n[46] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review\\nof statistics and its application, 6:405–431, 2019.\\n[47] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='of statistics and its application, 6:405–431, 2019.\\n[47] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/\\ndarknet/, 2013–2016.\\n[48] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\\nreal-time object detection. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), June 2016.\\n[49] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), July 2017.\\n[50] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.\\n[51] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\\nMobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 4510–4520, 2018.\\n[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='on computer vision and pattern recognition, pages 4510–4520, 2018.\\n[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and\\nJian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings\\nof the IEEE/CVF international conference on computer vision, pages 8430–8439, 2019.\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[53] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in\\ncrowded scenes. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 2325–2333, 2016.\\n[54] Yuchen Su, Zhineng Chen, Zhiwen Shao, Yuning Du, Zhilong Ji, Jinfeng Bai, Yong Zhou,\\nand Yu-Gang Jiang. Lranet: Towards accurate and efficient scene text detection with low-rank\\napproximation network. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 38, pages 4979–4987, 2024.\\n[55] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What\\nmakes for end-to-end object detection? In International Conference on Machine Learning,\\npages 9934–9944. PMLR, 2021.\\n[56] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka,\\nLei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with\\nlearnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with\\nlearnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 14454–14463, 2021.\\n[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object\\ndetector. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):1922–1933,\\n2020.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems, 30, 2017.\\n[59] Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, and Guiguang Ding. Repvit: Revisiting mobile\\ncnn from vit perspective. arXiv preprint arXiv:2307.09283, 2023.\\n[60] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai\\nHan. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[60] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai\\nHan. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in\\nNeural Information Processing Systems, 36, 2024.\\n[61] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling\\ncross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and\\npattern recognition, pages 13029–13038, 2021.\\n[62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-\\nfreebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 7464–7475, 2023.\\n[63] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and\\nI-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. InProceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition workshops , pages'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. InProceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition workshops , pages\\n390–391, 2020.\\n[64] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh. Designing network design strategies\\nthrough gradient path analysis. arXiv preprint arXiv:2211.04800, 2022.\\n[65] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to\\nlearn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024.\\n[66] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end\\nobject detection with fully convolutional network. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pages 15849–15858, 2021.\\n[67] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for\\ntransformer-based detector. In Proceedings of the AAAI conference on artificial intelligence,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[67] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for\\ntransformer-based detector. In Proceedings of the AAAI conference on artificial intelligence,\\nvolume 36, pages 2567–2575, 2022.\\n[68] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\\nIntroducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international\\nconference on computer vision, pages 22–31, 2021.\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[69] Haiyang Xu, Zhichao Zhou, Dongliang He, Fu Li, and Jingdong Wang. Vision transformer with\\nattention map hallucination and ffn compaction. arXiv preprint arXiv:2306.10875, 2023.\\n[70] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong\\nWang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. Pp-yoloe: An evolved version of yolo.\\narXiv preprint arXiv:2203.16250, 2022.\\n[71] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, and Xiuyu Sun. Damo-yolo:\\nA report on real-time object detection design. arXiv preprint arXiv:2211.15444, 2022.\\n[72] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr:\\nEnd-to-end multiple-object tracking with transformer. In European Conference on Computer\\nVision, pages 659–675. Springer, 2022.\\n[73] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-\\nYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[73] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-\\nYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.\\narXiv preprint arXiv:2203.03605, 2022.\\n[74] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\\n[75] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between\\nanchor-based and anchor-free detection via adaptive training sample selection. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 9759–9768,\\n2020.\\n[76] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu,\\nGang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic\\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 12083–12093, 2022.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 12083–12093, 2022.\\n[77] Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, and Jingdong\\nWang. Ms-detr: Efficient detr training with mixed supervision. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 17027–17036, 2024.\\n[78] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu,\\nand Jie Chen. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069,\\n2023.\\n[79] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou\\nloss: Faster and better learning for bounding box regression. In Proceedings of the AAAI\\nconference on artificial intelligence, volume 34, pages 12993–13000, 2020.\\n[80] Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms.\\nIEEE Transactions on Multimedia, 2023.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='[80] Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms.\\nIEEE Transactions on Multimedia, 2023.\\n[81] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\\n2020.\\n[82] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training.\\nIn Proceedings of the IEEE/CVF international conference on computer vision, pages 6748–6758,\\n2023.\\n15'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='A Appendix\\nA.1 Implementation Details\\nFollowing [21, 62, 65], all YOLOv10 models are trained from scratch using the SGD optimizer for\\n500 epochs. The SGD momentum and weight decay are set to 0.937 and 5×10−4, respectively. The\\ninitial learning rate is 1×10−2 and it decays linearly to 1×10−4. For data augmentation, we adopt the\\nMosaic [2, 20], Mixup [74] and copy-paste augmentation [18], etc., like [21, 65]. Tab. 14 presents the\\ndetailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase\\nthe width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the\\nSPPF module [21] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion\\nratio of 2 for the inverted bottleneck block structure. Following [65, 62], we report the standard mean\\naverage precision (AP) across different object scales and IoU thresholds on the COCO dataset [35].'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='average precision (AP) across different object scales and IoU thresholds on the COCO dataset [35].\\nMoreover, we follow [78] to establish the end-to-end speed benchmark. Since the execution time of\\nNMS is affected by the input, we thus measure the latency on the COCO val set with the batch size\\nof 1, like [78]. We adopt the same NMS hyperparameters used by the detectors during their validation.\\nThe TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is\\nomitted. We report the average latency across all images.\\nTable 14: Hyper-parameters of YOLOv10.\\nhyper-parameter YOLOv10-N/S/M/B/L/X\\nepochs 500\\noptimizer SGD\\nmomentum 0.937\\nweight decay 5 ×10−4\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\ninitial learning rate 10 −2\\nfinal learning rate 10 −4\\nlearning rate schedule linear decay\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\nHSV hue augmentation 0.015'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='learning rate schedule linear decay\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\nHSV hue augmentation 0.015\\ntranslation augmentation 0.1\\nscale augmentation 0.5/0.5/0.9/0.9/0.9/0.9\\nmosaic augmentation 1.0\\nMixup augmentation 0.0/0.0/0.1/0.1/0.15/0.15\\ncopy-paste augmentation 0.0/0.0/0.1/0.1/0.3/0.3\\nclose mosaic epochs 10\\nA.2 Details of Consistent Matching Metric\\nWe provide the detailed derivation of consistent matching metric here.\\nAs mentioned in the paper, we suppose that the one-to-many positive samples isΩ and the one-to-\\none branch selects i-th prediction. We can then leverage the normalized metric [ 15] to obtain the\\nclassification target for task alignment learning [21, 15, 65, 29, 70], i.e., to2m,j = u∗ · mo2m,j\\nm∗\\no2m\\n≤ u∗\\nfor j ∈ Ω and to2o,i = u∗ · mo2o,i\\nm∗\\no2o\\n= u∗. We can thus derive the supervision gap between two\\nbranches by the 1-Wasserstein distance [46] of the different classification targets, i.e.,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='o2m\\n≤ u∗\\nfor j ∈ Ω and to2o,i = u∗ · mo2o,i\\nm∗\\no2o\\n= u∗. We can thus derive the supervision gap between two\\nbranches by the 1-Wasserstein distance [46] of the different classification targets, i.e.,\\nA = |(1 − to2o,i) − (1 − I(i ∈ Ω)to2m,i)| +\\nX\\nk∈Ω\\\\{i}\\n|1 − (1 − to2m,k)|\\n= |to2o,i − I(i ∈ Ω)to2m,i| +\\nX\\nk∈Ω\\\\{i}\\nto2m,k\\n= to2o,i − I(i ∈ Ω)to2m,i +\\nX\\nk∈Ω\\\\{i}\\nto2m,k,\\n(3)\\n16'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='where I(·) is the indicator function. We denote the classification targets of the predictions in Ω as\\n{ˆt1, ˆt2, ...,ˆt|Ω|} in descending order, with ˆt1 ≥ ˆt2 ≥ ... ≥ ˆt|Ω|. We can then replace to2o,i with u∗\\nand obtain:\\nA = u∗ − I(i ∈ Ω)to2m,i +\\nX\\nk∈Ω\\\\{i}\\nto2m,k\\n= u∗ +\\nX\\nk∈Ω\\nto2m,k − 2 · I(i ∈ Ω)to2m,i\\n= u∗ +\\nX|Ω|\\nk=1\\nˆtk − 2 · I(i ∈ Ω)to2m,i\\n(4)\\nWe further discuss the supervision gap in two scenarios, i.e.,\\n1. Supposing i ̸∈ Ω, we can obtain:\\nA = u∗ +\\nX|Ω|\\nk=1\\nˆtk (5)\\n2. Supposing i ∈ Ω, we denote to2m,i = ˆtn and obtain:\\nA = u∗ +\\nX|Ω|\\nk=1\\nˆtk − 2 · ˆtn (6)\\nDue to ˆtn ≥ 0, the second case can lead to smaller supervision gap. Besides, we can observe that A\\ndecreases as ˆtn increases, indicating that n decreases and the ranking of i within Ω improves. Due\\nto ˆtn ≤ ˆt1, A thus achieves the minimum when ˆtn = ˆt1, i.e., i is the best positive sample in Ω with\\nmo2m,i = m∗\\no2m and to2m,i = u∗ · mo2m,i\\nm∗\\no2m\\n= u∗.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='to ˆtn ≤ ˆt1, A thus achieves the minimum when ˆtn = ˆt1, i.e., i is the best positive sample in Ω with\\nmo2m,i = m∗\\no2m and to2m,i = u∗ · mo2m,i\\nm∗\\no2m\\n= u∗.\\nFurthermore, we prove that we can achieve the minimized supervision gap by the consistent matching\\nmetric. We suppose αo2m > 0 and βo2m > 0, which are common in [21, 65, 29, 15, 70]. Similarly,\\nwe assume αo2o > 0 and βo2o > 0. We can obtain r1 = αo2o\\nαo2m\\n> 0 and r2 = βo2o\\nβo2m\\n> 0, and then\\nderive mo2o by\\nmo2o = s · pαo2o · IoU(ˆb, b)βo2o\\n= s · pr1·αo2m · IoU(ˆb, b)r2·βo2m\\n= s · (pαo2m · IoU(ˆb, b)βo2m)r1 · IoU(ˆb, b)(r2−r1)·βo2m\\n= mr1\\no2m · IoU(ˆb, b)(r2−r1)·βo2m\\n(7)\\nTo achieve mo2m,i = m∗\\no2m and mo2o,i = m∗\\no2o, we can make mo2o monotonically increase with\\nmo2m by assigning (r2 − r1) = 0, i.e.,\\nmo2o = mr1\\no2m · IoU(ˆb, b)0·βo2m\\n= mr1\\no2m\\n(8)\\nSupposing r1 = r2 = r, we can thus derive the consistent matching metric, i.e., αo2o = r · αo2m and\\nβo2o = r · βo2m. By simply taking r = 1, we obtain αo2o = αo2m and βo2o = βo2m.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='= mr1\\no2m\\n(8)\\nSupposing r1 = r2 = r, we can thus derive the consistent matching metric, i.e., αo2o = r · αo2m and\\nβo2o = r · βo2m. By simply taking r = 1, we obtain αo2o = αo2m and βo2o = βo2m.\\nA.3 Details of Rank-Guided Block Design\\nWe present the details of the algorithm of rank-guided block design in Algo. 1. Besides, to calculate\\nthe numerical rank of the convolution, we reshape its weight to the shape of (Co, K2 ×Ci), where Co\\nand Ci denote the number of output and input channels, and K means the kernel size, respectively.\\nA.4 Training Cost Analyses\\nIn addition to the inference efficiency analyses, we also investigate the training cost of our YOLOv10\\nmodels. We compare with other YOLO variants and measure the training throughput on 8 NVIDIA\\n3090 GPUs using the official codebases. Tab. 15 presents the comparison results based on the medium\\nmodel scale. We observe that despite having 500 training epochs, YOLOv10 achieves a high training'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='3090 GPUs using the official codebases. Tab. 15 presents the comparison results based on the medium\\nmodel scale. We observe that despite having 500 training epochs, YOLOv10 achieves a high training\\nthroughput, making its training cost affordable. We also note that the one-to-many head in the\\nNMS-free training will introduce the extra overhead for YOLOv10. To investigate this, we measure\\nthe training cost of YOLOv10 with only the one-to-one head, which is denoted as “YOLOv10-o2o”.\\nAs shown in Tab. 15, YOLOv10-M results in a small increase in the training time over “YOLOv10-\\nM-o2o”, about 18s each epoch, which is affordable. To fairly verify the benefit of the one-to-many\\nhead in NMS-free training, we also adopt longer 550 training epochs for “YOLOv10-M-o2o”, which\\n17'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Algorithm 1: Rank-guided block design\\nInput: Intrinsic ranks R for all stages S; Original Network Θ; CIB θcib;\\nOutput: New network Θ∗ with CIB for certain stages.\\n1 t ← 0;\\n2 Θ0 ← Θ; Θ∗ ← Θ0;\\n3 ap0 ← AP(T(Θ0)) ; // T:training the network; AP:evaluating the AP performance.\\n4 while S ̸= ∅ do\\n5 st ← argmins∈S R;\\n6 Θt+1 ← Replace(Θt, θcib, st) ; // Replace the block in Stage st of Θt with CIB θcib.\\n7 apt+1 ← AP(T(Θt+1));\\n8 if apt+1 ≥ ap0 then\\n9 Θ∗ ← Θt+1; S ← S \\\\ {st};\\n10 else\\n11 return Θ∗;\\n12 end\\n13 end\\n14 return Θ∗;\\nTable 15: Training cost analyses on 8 NVIDIA 3090 GPUs.\\nModel Epoch Speed (epoch/hour) Time (hour)\\nYOLOv6-3.0-M 300 7.2 41.7\\nYOLOv8-M 500 18.3 27.3\\nYOLOv9-M 500 12.3 40.7\\nGold-YOLO-M 300 4.7 63.8\\nYOLO-MS 300 7.1 42.3\\nYOLOv10-M-o2o 500 18.8 26.7\\nYOLOv10-M 500 17.2 29.1\\nTable 16: Latency with NMS.\\nModel Latency\\nYOLOv10-N 6.19ms\\nYOLOv10-S 7.15ms\\nYOLOv10-M 9.03ms\\nYOLOv10-B 10.04ms\\nYOLOv10-L 11.52ms\\nYOLOv10-X 14.67ms'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10-M-o2o 500 18.8 26.7\\nYOLOv10-M 500 17.2 29.1\\nTable 16: Latency with NMS.\\nModel Latency\\nYOLOv10-N 6.19ms\\nYOLOv10-S 7.15ms\\nYOLOv10-M 9.03ms\\nYOLOv10-B 10.04ms\\nYOLOv10-L 11.52ms\\nYOLOv10-X 14.67ms\\nleads to a similar training time (29.3 vs. 29.1 hours) but still yields inferior performance (48.9% vs.\\n51.1% AP) compared with YOLOv10-M.\\nA.5 More Results on COCO\\nWe measure the latency of YOLOv10 with the original one-to-many training using NMS and report\\nthe results on COCO in Tab. 16. Besides, we report the detailed performance of YOLOv10, including\\nAPval\\n50 and APval\\n75 at different IoU thresholds, as well as AP val\\nsmall, APval\\nmedium, and APval\\nlarge across\\ndifferent scales, in Tab. 17. We also present the comparisons with more lightweight detectors,\\nincluding DAMO-YOLO [ 71], YOLOv7 [ 62], and DEYO [ 45], in Tab. 18. It shows that our\\nYOLOv10 also achieves superior performance and efficiency trade-offs. Additionally, in experiments,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='including DAMO-YOLO [ 71], YOLOv7 [ 62], and DEYO [ 45], in Tab. 18. It shows that our\\nYOLOv10 also achieves superior performance and efficiency trade-offs. Additionally, in experiments,\\nwe follow previous works [21, 65] to train the models for 500 epochs. We also conduct experiments\\nto train the models for 300 epochs and present the comparison results with YOLOv6 [ 29], Gold-\\nYOLO [60], and YOLO-MS [8] which adopt 300 epochs, in Tab. 19. We observe that our YOLOv10\\nalso exhibits better performance and inference latency. We also note that despite trained for 500\\nepochs, YOLOv10 has less training cost compared with these models as presented in Tab. 15.\\nA.6 Inference Efficiency Comparison on CPU\\nWe present the speed comparison results of YOLOv10 and others on CPU (Intel Xeon Skylake, IBRS)\\nusing OpenVINO in Fig. 5. We observe that YOLOv10 also shows state-of-the-art trade-offs in terms\\nof performance and efficiency.\\nA.7 More Analyses for Holistic Efficiency-Accuracy Driven Model Design'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='using OpenVINO in Fig. 5. We observe that YOLOv10 also shows state-of-the-art trade-offs in terms\\nof performance and efficiency.\\nA.7 More Analyses for Holistic Efficiency-Accuracy Driven Model Design\\nWe note that reducing the latency of YOLOv10-S (#2 in Tab. 2) is particularly challenging due to its\\nsmall model scale. However, as shown in Tab. 2, our efficiency driven model design still achieves a\\n5.3% reduction in latency without compromising performance. This provides substantial support for\\nthe further accuracy driven model design. YOLOv10-S achieves a better latency-accuracy trade-off\\n18'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 17: Detailed performance of YOLOv10 on COCO.\\nModel AP val(%) AP val\\n50 (%) AP val\\n75 (%) AP val\\nsmall(%) AP val\\nmedium(%) AP val\\nlarge(%)\\nYOLOv10-N 38.5 53.8 41.7 18.9 42.4 54.6\\nYOLOv10-S 46.3 63.0 50.4 26.8 51.0 63.8\\nYOLOv10-M 51.1 68.1 55.8 33.8 56.5 67.0\\nYOLOv10-B 52.5 69.6 57.2 35.1 57.8 68.5\\nYOLOv10-L 53.2 70.1 58.1 35.8 58.5 69.4\\nYOLOv10-X 54.4 71.3 59.3 37.0 59.8 70.9\\nTable 18: Comparisons with more lightweight detectors.\\nModel #Param.(M) FLOPs(G) AP val(%) Latency(ms)\\nDEYO-tiny [45] 4.0 8.0 37.6 2.01\\nYOLOv10-N 2.3 6.7 38.5 1.84\\nDAMO-YOLO-T [71] 8.5 18.1 42.0 2.21\\nDAMO-YOLO-S [71] 16.3 37.8 46.0 3.18\\nDEYO-S [45] 14.0 26.0 45.8 3.34\\nYOLOv10-S 7.2 21.6 46.3 2.49\\nDAMO-YOLO-M [71] 28.2 61.8 49.2 4.57\\nDAMO-YOLO-L [71] 42.1 97.3 50.8 6.48\\nDEYO-M [45] 33.0 78.0 50.7 7.14\\nYOLOv10-M 15.4 59.1 51.1 4.74\\nYOLOv7 [62] 36.9 104.7 51.2 17.03\\nYOLOv10-B 19.1 92.0 52.5 5.74\\nYOLOv7-X [62] 71.3 189.9 52.9 21.45\\nDEYO-L [45] 51.0 155.0 52.7 10.00\\nYOLOv10-L 24.4 120.3 53.2 7.28'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='YOLOv10-M 15.4 59.1 51.1 4.74\\nYOLOv7 [62] 36.9 104.7 51.2 17.03\\nYOLOv10-B 19.1 92.0 52.5 5.74\\nYOLOv7-X [62] 71.3 189.9 52.9 21.45\\nDEYO-L [45] 51.0 155.0 52.7 10.00\\nYOLOv10-L 24.4 120.3 53.2 7.28\\nDEYO-X [45] 78.0 242.0 53.7 15.38\\nYOLOv10-X 29.5 160.4 54.4 10.70\\nwith our holistic efficiency-accuracy driven model design, showing a 2.0% AP improvement with only\\n0.05ms latency overhead. Besides, for YOLOv10-M (#6 in Tab. 2), which has a larger model scale\\nand more redundancy, our efficiency driven model design results in a considerable 12.5% latency\\nreduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a\\nnotable 0.8% AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48ms.\\nThese results well demonstrate the effectiveness of our design strategy across different model scales.\\nTable 19: Performance comparisons under 300 training epochs.\\nModel #Param.(M) FLOPs(G) AP val(%) Latency(ms)\\nYOLOv6-3.0-N [29] 4.7 11.4 37.0 2.69'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Table 19: Performance comparisons under 300 training epochs.\\nModel #Param.(M) FLOPs(G) AP val(%) Latency(ms)\\nYOLOv6-3.0-N [29] 4.7 11.4 37.0 2.69\\nGold-YOLO-N [60] 5.6 12.1 39.6 2.92\\nYOLOv10-N (Ours) 2.3 6.7 37.7 1.84\\nYOLOv6-3.0-S [29] 18.5 45.3 44.3 3.42\\nGold-YOLO-S [60] 21.5 46.0 45.4 3.82\\nYOLO-MS-XS [8] 4.5 17.4 43.4 8.23\\nYOLOv10-S (Ours) 7.2 21.6 45.6 2.49\\nYOLOv6-3.0-M [29] 34.9 85.8 49.1 5.63\\nGold-YOLO-M [60] 41.3 87.5 49.8 6.38\\nYOLOv10-M (Ours) 15.4 59.1 50.3 4.74\\nYOLOv6-3.0-L [29] 59.6 150.7 51.8 9.02\\nGold-YOLO-L [60] 75.1 151.7 51.8 10.65\\nYOLO-MS [8] 22.2 80.2 51.0 12.41\\nYOLOv10-B (Ours) 19.1 92.0 51.6 5.74\\nYOLOv10-L (Ours) 24.4 120.3 52.4 7.28\\nYOLOv10-X (Ours) 29.5 160.4 53.6 10.70\\n19'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni00000052/uni00000051/uni00000003/uni00000026/uni00000033/uni00000038/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\\n/uni00000016/uni0000001a/uni00000011/uni00000018\\n/uni00000017/uni00000013/uni00000011/uni00000013\\n/uni00000017/uni00000015/uni00000011/uni00000018\\n/uni00000017/uni00000018/uni00000011/uni00000013\\n/uni00000017/uni0000001a/uni00000011/uni00000018\\n/uni00000018/uni00000013/uni00000011/uni00000013\\n/uni00000018/uni00000015/uni00000011/uni00000018\\n/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 5: Performance and efficiency comparisons on CPU.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Figure 5: Performance and efficiency comparisons on CPU.\\nFigure 6: Visualization results under complex and challenging scenarios.\\nA.8 Visualization Results\\nFig. 6 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It\\ncan be observed that YOLOv10 can achieve precise detection under various difficult conditions, such\\nas low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely\\npacked objects, such as bottle, cup, and person. These results indicate its superior performance.\\nA.9 Contribution, Limitation, and Broader Impact\\nContribution. In summary, our contributions are three folds as follows:\\n1. We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label\\nassignments way is designed to provide rich supervision by one-to-many branch during training\\nand high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='assignments way is designed to provide rich supervision by one-to-many branch during training\\nand high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious\\nsupervision between two branches, we innovatively propose the consistent matching metric, which\\ncan well reduce the theoretical supervision gap and lead to improved performance.\\n20'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='2. We propose a holistic efficiency-accuracy driven model design strategy for the model architecture\\nof YOLOs. We present novel lightweight classification head, spatial-channel decoupled down-\\nsampling, and rank-guided block design, which greatly reduce the computational redundancy and\\nachieve high efficiency. We further introduce the large-kernel convolution and innovative partial\\nself-attention module, which effectively enhance the performance under low cost.\\n3. Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object\\ndetector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art\\nperformance and efficiency trade-offs compared with other advanced detectors.\\nLimitation. Due to the limited computational resources, we do not investigate the pretraining\\nof YOLOv10 on large-scale datasets, e.g., Objects365 [ 52]. Besides, although we can achieve'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='Limitation. Due to the limited computational resources, we do not investigate the pretraining\\nof YOLOv10 on large-scale datasets, e.g., Objects365 [ 52]. Besides, although we can achieve\\ncompetitive end-to-end performance using the one-to-one head under NMS-free training, there still\\nexists a performance gap compared with the original one-to-many training using NMS, especially\\nnoticeable in small models. For example, in YOLOv10-N and YOLOv10-S, the performance of\\none-to-many training with NMS outperforms that of NMS-free training by 1.0% AP and 0.5% AP,\\nrespectively. We will explore ways to further reduce the gap and achieve higher performance for\\nYOLOv10 in the future work.\\nBroader impact. The YOLOs can be widely applied in various real-world applications, including\\nmedical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these\\nfields and improve the efficiency. However, we acknowledge the potential for malicious use of our'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-31T00:20:38+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-31T00:20:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'file_2.pdf', 'file_type': 'pdf'}, page_content='medical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these\\nfields and improve the efficiency. However, we acknowledge the potential for malicious use of our\\nmodels. We will make every effort to prevent this.\\n21'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='FPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\nReal-time object detection is a very important topic in\\ncomputer vision, as it is often a necessary component in\\ncomputer vision systems. For example, multi-object track-\\ning [94, 93], autonomous driving [40, 18], robotics [35, 58],\\nmedical image analysis [34, 46], etc. The computing de-\\nvices that execute real-time object detection is usually some\\nmobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='mobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute\\nstick (Intel), Jetson AI edge devices (Nvidia), the edge TPU\\n(Google), the neural processing engine (Qualcomm), the AI\\nprocessing unit (MediaTek), and the AI SoCs (Kneron), are\\nall NPUs. Some of the above mentioned edge devices focus\\non speeding up different operations such as vanilla convolu-\\ntion, depth-wise convolution, or MLP operations. In this pa-\\nper, the real-time object detector we proposed mainly hopes\\nthat it can support both mobile GPU and GPU devices from\\nthe edge to the cloud.\\nIn recent years, the real-time object detector is still de-\\nveloped for different edge device. For example, the devel-\\nFigure 1: Comparison with other real-time object detectors, our\\nproposed methods achieve state-of-the-arts performance.\\nopment of MCUNet [49, 48] and NanoDet [54] focused on'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 1: Comparison with other real-time object detectors, our\\nproposed methods achieve state-of-the-arts performance.\\nopment of MCUNet [49, 48] and NanoDet [54] focused on\\nproducing low-power single-chip and improving the infer-\\nence speed on edge CPU. As for methods such as YOLOX\\n[21] and YOLOR [81], they focus on improving the infer-\\nence speed of various GPUs. More recently, the develop-\\nment of real-time object detector has focused on the de-\\nsign of efﬁcient architecture. As for real-time object de-\\ntectors that can be used on CPU [54, 88, 84, 83], their de-\\nsign is mostly based on MobileNet [28, 66, 27], ShufﬂeNet\\n[92, 55], or GhostNet [25]. Another mainstream real-time\\nobject detectors are developed for GPU [81, 21, 97], they\\nmostly use ResNet [26], DarkNet [63], or DLA [87], and\\nthen use the CSPNet [80] strategy to optimize the architec-\\nture. The development direction of the proposed methods in\\nthis paper are different from that of the current mainstream'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='then use the CSPNet [80] strategy to optimize the architec-\\nture. The development direction of the proposed methods in\\nthis paper are different from that of the current mainstream\\nreal-time object detectors. In addition to architecture op-\\ntimization, our proposed methods will focus on the opti-\\nmization of the training process. Our focus will be on some\\noptimized modules and optimization methods which may\\nstrengthen the training cost for improving the accuracy of\\nobject detection, but without increasing the inference cost.\\nWe call the proposed modules and optimization methods\\ntrainable bag-of-freebies.\\n1\\narXiv:2207.02696v1  [cs.CV]  6 Jul 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Recently, model re-parameterization [13, 12, 29] and dy-\\nnamic label assignment [20, 17, 42] have become important\\ntopics in network training and object detection. Mainly af-\\nter the above new concepts are proposed, the training of\\nobject detector evolves many new issues. In this paper, we\\nwill present some of the new issues we have discovered and\\ndevise effective methods to address them. For model re-\\nparameterization, we analyze the model re-parameterization\\nstrategies applicable to layers in different networks with the\\nconcept of gradient propagation path, and propose planned\\nre-parameterized model. In addition, when we discover that\\nwith dynamic label assignment technology, the training of\\nmodel with multiple output layers will generate new issues.\\nThat is: “How to assign dynamic targets for the outputs of\\ndifferent branches?” For this problem, we propose a new\\nlabel assignment method called coarse-to-ﬁne lead guided\\nlabel assignment.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='That is: “How to assign dynamic targets for the outputs of\\ndifferent branches?” For this problem, we propose a new\\nlabel assignment method called coarse-to-ﬁne lead guided\\nlabel assignment.\\nThe contributions of this paper are summarized as fol-\\nlows: (1) we design several trainable bag-of-freebies meth-\\nods, so that real-time object detection can greatly improve\\nthe detection accuracy without increasing the inference\\ncost; (2) for the evolution of object detection methods, we\\nfound two new issues, namely how re-parameterized mod-\\nule replaces original module, and how dynamic label as-\\nsignment strategy deals with assignment to different output\\nlayers. In addition, we also propose methods to address the\\ndifﬁculties arising from these issues; (3) we propose “ex-\\ntend” and “compound scaling” methods for the real-time\\nobject detector that can effectively utilize parameters and\\ncomputation; and (4) the method we proposed can effec-\\ntively reduce about 40% parameters and 50% computation'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='object detector that can effectively utilize parameters and\\ncomputation; and (4) the method we proposed can effec-\\ntively reduce about 40% parameters and 50% computation\\nof state-of-the-art real-time object detector, and has faster\\ninference speed and higher detection accuracy.\\n2. Related work\\n2.1. Real-time object detectors\\nCurrently state-of-the-art real-time object detectors are\\nmainly based on YOLO [61, 62, 63] and FCOS [76, 77],\\nwhich are [3, 79, 81, 21, 54, 85, 23]. Being able to become\\na state-of-the-art real-time object detector usually requires\\nthe following characteristics: (1) a faster and stronger net-\\nwork architecture; (2) a more effective feature integration\\nmethod [22, 97, 37, 74, 59, 30, 9, 45]; (3) a more accurate\\ndetection method [76, 77, 69]; (4) a more robust loss func-\\ntion [96, 64, 6, 56, 95, 57]; (5) a more efﬁcient label assign-\\nment method [99, 20, 17, 82, 42]; and (6) a more efﬁcient\\ntraining method. In this paper, we do not intend to explore'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='tion [96, 64, 6, 56, 95, 57]; (5) a more efﬁcient label assign-\\nment method [99, 20, 17, 82, 42]; and (6) a more efﬁcient\\ntraining method. In this paper, we do not intend to explore\\nself-supervised learning or knowledge distillation methods\\nthat require additional data or large model. Instead, we will\\ndesign new trainable bag-of-freebies method for the issues\\nderived from the state-of-the-art methods associated with\\n(4), (5), and (6) mentioned above.\\n2.2. Model re-parameterization\\nModel re-parametrization techniques [71, 31, 75, 19, 33,\\n11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple compu-\\ntational modules into one at inference stage. The model\\nre-parameterization technique can be regarded as an en-\\nsemble technique, and we can divide it into two cate-\\ngories, i.e., module-level ensemble and model-level ensem-\\nble. There are two common practices for model-level re-\\nparameterization to obtain the ﬁnal inference model. One\\nis to train multiple identical models with different train-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ble. There are two common practices for model-level re-\\nparameterization to obtain the ﬁnal inference model. One\\nis to train multiple identical models with different train-\\ning data, and then average the weights of multiple trained\\nmodels. The other is to perform a weighted average of the\\nweights of models at different iteration number. Module-\\nlevel re-parameterization is a more popular research issue\\nrecently. This type of method splits a module into multi-\\nple identical or different module branches during training\\nand integrates multiple branched modules into a completely\\nequivalent module during inference. However, not all pro-\\nposed re-parameterized module can be perfectly applied to\\ndifferent architectures. With this in mind, we have devel-\\noped new re-parameterization module and designed related\\napplication strategies for various architectures.\\n2.3. Model scaling\\nModel scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way\\nto scale up or down an already designed model and make'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='application strategies for various architectures.\\n2.3. Model scaling\\nModel scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way\\nto scale up or down an already designed model and make\\nit ﬁt in different computing devices. The model scaling\\nmethod usually uses different scaling factors, such as reso-\\nlution (size of input image), depth (number of layer), width\\n(number of channel), and stage (number of feature pyra-\\nmid), so as to achieve a good trade-off for the amount of\\nnetwork parameters, computation, inference speed, and ac-\\ncuracy. Network architecture search (NAS) is one of the\\ncommonly used model scaling methods. NAS can automat-\\nically search for suitable scaling factors from search space\\nwithout deﬁning too complicated rules. The disadvantage\\nof NAS is that it requires very expensive computation to\\ncomplete the search for model scaling factors. In [15], the\\nresearcher analyzes the relationship between scaling factors\\nand the amount of parameters and operations, trying to di-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='complete the search for model scaling factors. In [15], the\\nresearcher analyzes the relationship between scaling factors\\nand the amount of parameters and operations, trying to di-\\nrectly estimate some rules, and thereby obtain the scaling\\nfactors required by model scaling. Checking the literature,\\nwe found that almost all model scaling methods analyze in-\\ndividual scaling factor independently, and even the methods\\nin the compound scaling category also optimized scaling\\nfactor independently. The reason for this is because most\\npopular NAS architectures deal with scaling factors that are\\nnot very correlated. We observed that all concatenation-\\nbased models, such as DenseNet [32] or V oVNet [39], will\\nchange the input width of some layers when the depth of\\nsuch models is scaled. Since the proposed architecture is\\nconcatenation-based, we have to design a new compound\\nscaling method for this model.\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 2: Extended efﬁcient layer aggregation networks. The proposed extended ELAN (E-ELAN) does not change the gradient transmis-\\nsion path of the original architecture at all, but use group convolution to increase the cardinality of the added features, and combine the\\nfeatures of different groups in a shufﬂe and merge cardinality manner. This way of operation can enhance the features learned by different\\nfeature maps and improve the use of parameters and calculations.\\n3. Architecture\\n3.1. Extended efﬁcient layer aggregation networks\\nIn most of the literature on designing the efﬁcient ar-\\nchitectures, the main considerations are no more than the\\nnumber of parameters, the amount of computation, and the\\ncomputational density. Starting from the characteristics of\\nmemory access cost, Ma et al. [55] also analyzed the in-\\nﬂuence of the input/output channel ratio, the number of\\nbranches of the architecture, and the element-wise opera-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='memory access cost, Ma et al. [55] also analyzed the in-\\nﬂuence of the input/output channel ratio, the number of\\nbranches of the architecture, and the element-wise opera-\\ntion on the network inference speed. Doll´ar et al. [15] addi-\\ntionally considered activation when performing model scal-\\ning, that is, to put more consideration on the number of el-\\nements in the output tensors of convolutional layers. The\\ndesign of CSPV oVNet [79] in Figure 2 (b) is a variation of\\nV oVNet [39]. In addition to considering the aforementioned\\nbasic designing concerns, the architecture of CSPV oVNet\\n[79] also analyzes the gradient path, in order to enable the\\nweights of different layers to learn more diverse features.\\nThe gradient analysis approach described above makes in-\\nferences faster and more accurate. ELAN [1] in Figure 2 (c)\\nconsiders the following design strategy – “How to design an\\nefﬁcient network?.” They came out with a conclusion: By'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ferences faster and more accurate. ELAN [1] in Figure 2 (c)\\nconsiders the following design strategy – “How to design an\\nefﬁcient network?.” They came out with a conclusion: By\\ncontrolling the shortest longest gradient path, a deeper net-\\nwork can learn and converge effectively. In this paper, we\\npropose Extended-ELAN (E-ELAN) based on ELAN and\\nits main architecture is shown in Figure 2 (d).\\nRegardless of the gradient path length and the stacking\\nnumber of computational blocks in large-scale ELAN, it has\\nreached a stable state. If more computational blocks are\\nstacked unlimitedly, this stable state may be destroyed, and\\nthe parameter utilization rate will decrease. The proposed\\nE-ELAN uses expand, shufﬂe, merge cardinality to achieve\\nthe ability to continuously enhance the learning ability of\\nthe network without destroying the original gradient path.\\nIn terms of architecture, E-ELAN only changes the archi-\\ntecture in computational block, while the architecture of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='the network without destroying the original gradient path.\\nIn terms of architecture, E-ELAN only changes the archi-\\ntecture in computational block, while the architecture of\\ntransition layer is completely unchanged. Our strategy is\\nto use group convolution to expand the channel and car-\\ndinality of computational blocks. We will apply the same\\ngroup parameter and channel multiplier to all the compu-\\ntational blocks of a computational layer. Then, the feature\\nmap calculated by each computational block will be shuf-\\nﬂed into g groups according to the set group parameter g,\\nand then concatenate them together. At this time, the num-\\nber of channels in each group of feature map will be the\\nsame as the number of channels in the original architec-\\nture. Finally, we add g groups of feature maps to perform\\nmerge cardinality. In addition to maintaining the original\\nELAN design architecture, E-ELAN can also guide differ-\\nent groups of computational blocks to learn more diverse\\nfeatures.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='merge cardinality. In addition to maintaining the original\\nELAN design architecture, E-ELAN can also guide differ-\\nent groups of computational blocks to learn more diverse\\nfeatures.\\n3.2. Model scaling for concatenation-based models\\nThe main purpose of model scaling is to adjust some at-\\ntributes of the model and generate models of different scales\\nto meet the needs of different inference speeds. For ex-\\nample the scaling model of EfﬁcientNet [72] considers the\\nwidth, depth, and resolution. As for the scaled-YOLOv4\\n[79], its scaling model is to adjust the number of stages. In\\n[15], Doll´ar et al. analyzed the inﬂuence of vanilla convolu-\\ntion and group convolution on the amount of parameter and\\ncomputation when performing width and depth scaling, and\\nused this to design the corresponding model scaling method.\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 3: Model scaling for concatenation-based models. From (a) to (b), we observe that when depth scaling is performed on\\nconcatenation-based models, the output width of a computational block also increases. This phenomenon will cause the input width\\nof the subsequent transmission layer to increase. Therefore, we propose (c), that is, when performing model scaling on concatenation-\\nbased models, only the depth in a computational block needs to be scaled, and the remaining of transmission layer is performed with\\ncorresponding width scaling.\\nThe above methods are mainly used in architectures such as\\nPlainNet or ResNet. When these architectures are in execut-\\ning scaling up or scaling down, the in-degree and out-degree\\nof each layer will not change, so we can independently an-\\nalyze the impact of each scaling factor on the amount of\\nparameters and computation. However, if these methods\\nare applied to the concatenation-based architecture, we will'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='alyze the impact of each scaling factor on the amount of\\nparameters and computation. However, if these methods\\nare applied to the concatenation-based architecture, we will\\nﬁnd that when scaling up or scaling down is performed on\\ndepth, the in-degree of a translation layer which is immedi-\\nately after a concatenation-based computational block will\\ndecrease or increase, as shown in Figure 3 (a) and (b).\\nIt can be inferred from the above phenomenon that\\nwe cannot analyze different scaling factors separately for\\na concatenation-based model but must be considered to-\\ngether. Take scaling-up depth as an example, such an ac-\\ntion will cause a ratio change between the input channel and\\noutput channel of a transition layer, which may lead to a de-\\ncrease in the hardware usage of the model. Therefore, we\\nmust propose the corresponding compound model scaling\\nmethod for a concatenation-based model. When we scale\\nthe depth factor of a computational block, we must also cal-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='must propose the corresponding compound model scaling\\nmethod for a concatenation-based model. When we scale\\nthe depth factor of a computational block, we must also cal-\\nculate the change of the output channel of that block. Then,\\nwe will perform width factor scaling with the same amount\\nof change on the transition layers, and the result is shown\\nin Figure 3 (c). Our proposed compound scaling method\\ncan maintain the properties that the model had at the initial\\ndesign and maintains the optimal structure.\\n4. Trainable bag-of-freebies\\n4.1. Planned re-parameterized convolution\\nAlthough RepConv [13] has achieved excellent perfor-\\nmance on the VGG [68], when we directly apply it to\\nResNet [26] and DenseNet [32] and other architectures,\\nits accuracy will be signiﬁcantly reduced. We use gradi-\\nent ﬂow propagation paths to analyze how re-parameterized\\nconvolution should be combined with different network.\\nWe also designed planned re-parameterized convolution ac-\\ncordingly.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ent ﬂow propagation paths to analyze how re-parameterized\\nconvolution should be combined with different network.\\nWe also designed planned re-parameterized convolution ac-\\ncordingly.\\nFigure 4: Planned re-parameterized model. In the proposed\\nplanned re-parameterized model, we found that a layer with resid-\\nual or concatenation connections, its RepConv should not have\\nidentity connection. Under these circumstances, it can be replaced\\nby RepConvN that contains no identity connections.\\nRepConv actually combines 3 × 3 convolution, 1 × 1\\nconvolution, and identity connection in one convolutional\\nlayer. After analyzing the combination and correspond-\\ning performance of RepConv and different architectures,\\nwe ﬁnd that the identity connection in RepConv destroys\\nthe residual in ResNet and the concatenation in DenseNet,\\nwhich provides more diversity of gradients for different fea-\\nture maps. For the above reasons, we use RepConv with-\\nout identity connection (RepConvN) to design the architec-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='which provides more diversity of gradients for different fea-\\nture maps. For the above reasons, we use RepConv with-\\nout identity connection (RepConvN) to design the architec-\\nture of planned re-parameterized convolution. In our think-\\ning, when a convolutional layer with residual or concate-\\nnation is replaced by re-parameterized convolution, there\\nshould be no identity connection. Figure 4 shows an exam-\\nple of our designed “planned re-parameterized convolution”\\nused in PlainNet and ResNet. As for the complete planned\\nre-parameterized convolution experiment in residual-based\\nmodel and concatenation-based model, it will be presented\\nin the ablation study session.\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 5: Coarse for auxiliary and ﬁne for lead head label assigner. Compare with normal model (a), the schema in (b) has auxiliary head.\\nDifferent from the usual independent label assigner (c), we propose (d) lead head guided label assigner and (e) coarse-to-ﬁne lead head\\nguided label assigner. The proposed label assigner is optimized by lead head prediction and the ground truth to get the labels of training\\nlead head and auxiliary head at the same time. The detailed coarse-to-ﬁne implementation method and constraint design details will be\\nelaborated in Apendix.\\n4.2. Coarse for auxiliary and ﬁne for lead loss\\nDeep supervision [38] is a technique that is often used\\nin training deep networks. Its main concept is to add\\nextra auxiliary head in the middle layers of the network,\\nand the shallow network weights with assistant loss as the\\nguide. Even for architectures such as ResNet [26] and\\nDenseNet [32] which usually converge well, deep supervi-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='and the shallow network weights with assistant loss as the\\nguide. Even for architectures such as ResNet [26] and\\nDenseNet [32] which usually converge well, deep supervi-\\nsion [70, 98, 67, 47, 82, 65, 86, 50] can still signiﬁcantly\\nimprove the performance of the model on many tasks. Fig-\\nure 5 (a) and (b) show, respectively, the object detector ar-\\nchitecture “without” and “with” deep supervision. In this\\npaper, we call the head responsible for the ﬁnal output as\\nthe lead head, and the head used to assist training is called\\nauxiliary head.\\nNext we want to discuss the issue of label assignment. In\\nthe past, in the training of deep network, label assignment\\nusually refers directly to the ground truth and generate hard\\nlabel according to the given rules. However, in recent years,\\nif we take object detection as an example, researchers often\\nuse the quality and distribution of prediction output by the\\nnetwork, and then consider together with the ground truth to'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='if we take object detection as an example, researchers often\\nuse the quality and distribution of prediction output by the\\nnetwork, and then consider together with the ground truth to\\nuse some calculation and optimization methods to generate\\na reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].\\nFor example, YOLO [61] use IoU of prediction of bounding\\nbox regression and ground truth as the soft label of object-\\nness. In this paper, we call the mechanism that considers\\nthe network prediction results together with the ground truth\\nand then assigns soft labels as “label assigner.”\\nDeep supervision needs to be trained on the target ob-\\njectives regardless of the circumstances of auxiliary head or\\nlead head. During the development of soft label assigner re-\\nlated techniques, we accidentally discovered a new deriva-\\ntive issue, i.e., “How to assign soft label to auxiliary head\\nand lead head ?” To the best of our knowledge, the relevant'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='lated techniques, we accidentally discovered a new deriva-\\ntive issue, i.e., “How to assign soft label to auxiliary head\\nand lead head ?” To the best of our knowledge, the relevant\\nliterature has not explored this issue so far. The results of\\nthe most popular method at present is as shown in Figure 5\\n(c), which is to separate auxiliary head and lead head, and\\nthen use their own prediction results and the ground truth\\nto execute label assignment. The method proposed in this\\npaper is a new label assignment method that guides both\\nauxiliary head and lead head by the lead head prediction.\\nIn other words, we use lead head prediction as guidance to\\ngenerate coarse-to-ﬁne hierarchical labels, which are used\\nfor auxiliary head and lead head learning, respectively. The\\ntwo proposed deep supervision label assignment strategies\\nare shown in Figure 5 (d) and (e), respectively.\\nLead head guided label assigner is mainly calculated\\nbased on the prediction result of the lead head and the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='are shown in Figure 5 (d) and (e), respectively.\\nLead head guided label assigner is mainly calculated\\nbased on the prediction result of the lead head and the\\nground truth, and generate soft label through the optimiza-\\ntion process. This set of soft labels will be used as the tar-\\nget training model for both auxiliary head and lead head.\\nThe reason to do this is because lead head has a relatively\\nstrong learning capability, so the soft label generated from it\\nshould be more representative of the distribution and corre-\\nlation between the source data and the target. Furthermore,\\nwe can view such learning as a kind of generalized residual\\nlearning. By letting the shallower auxiliary head directly\\nlearn the information that lead head has learned, lead head\\nwill be more able to focus on learning residual information\\nthat has not yet been learned.\\nCoarse-to-ﬁne lead head guided label assigner also\\nused the predicted result of the lead head and the ground'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='will be more able to focus on learning residual information\\nthat has not yet been learned.\\nCoarse-to-ﬁne lead head guided label assigner also\\nused the predicted result of the lead head and the ground\\ntruth to generate soft label. However, in the process we gen-\\nerate two different sets of soft label, i.e., coarse label and\\nﬁne label, where ﬁne label is the same as the soft label gen-\\nerated by lead head guided label assigner, and coarse label\\nis generated by allowing more grids to be treated as posi-\\ntive target by relaxing the constraints of the positive sample\\nassignment process. The reason for this is that the learning\\nability of an auxiliary head is not as strong as that of a lead\\nhead, and in order to avoid losing the information that needs\\nto be learned, we will focus on optimizing the recall of aux-\\niliary head in the object detection task. As for the output\\nof lead head, we can ﬁlter the high precision results from\\nthe high recall results as the ﬁnal output. However, we must'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='iliary head in the object detection task. As for the output\\nof lead head, we can ﬁlter the high precision results from\\nthe high recall results as the ﬁnal output. However, we must\\nnote that if the additional weight of coarse label is close to\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparison of baseline object detectors.\\nModel #Param. FLOPs Size AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nYOLOv4 [3] 64.4M 142.8G 640 49.7% 68.2% 54.3% 32.9% 54.8% 63.7%\\nYOLOR-u5 (r6.1) [81] 46.5M 109.1G 640 50.2% 68.7% 54.6% 33.2% 55.5% 63.7%\\nYOLOv4-CSP [79] 52.9M 120.4G 640 50.3% 68.6% 54.9% 34.2% 55.6% 65.1%\\nYOLOR-CSP [81] 52.9M 120.4G 640 50.8% 69.5% 55.3% 33.7% 56.0% 65.4%\\nYOLOv7 36.9M 104.7G 640 51.2% 69.7% 55.5% 35.2% 56.0% 66.7%\\nimprovement -43% -15% - +0.4 +0.2 +0.2 +1.5 = +1.3\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 52.7% 71.3% 57.4% 36.3% 57.5% 68.3%\\nYOLOv7-X 71.3M 189.9G 640 52.9% 71.1% 57.5% 36.9% 57.7% 68.6%\\nimprovement -36% -19% - +0.2 -0.2 +0.1 +0.6 +0.2 +0.3\\nYOLOv4-tiny [79] 6.1 6.9 416 24.9% 42.1% 25.7% 8.7% 28.4% 39.2%\\nYOLOv7-tiny 6.2 5.8 416 35.2% 52.8% 37.3% 15.7% 38.0% 53.4%\\nimprovement +2% -19% - +10.3 +10.7 +11.6 +7.0 +9.6 +14.2\\nYOLOv4-tiny-3l [79] 8.7 5.2 320 30.8% 47.3% 32.2% 10.9% 31.9% 51.5%'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOv7-tiny 6.2 5.8 416 35.2% 52.8% 37.3% 15.7% 38.0% 53.4%\\nimprovement +2% -19% - +10.3 +10.7 +11.6 +7.0 +9.6 +14.2\\nYOLOv4-tiny-3l [79] 8.7 5.2 320 30.8% 47.3% 32.2% 10.9% 31.9% 51.5%\\nYOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%\\nimprovement = -11% - +0.7 +0.5 +0.9 -1.6 +1.6 +0.7\\nthat of ﬁne label, it may produce bad prior at ﬁnal predic-\\ntion. Therefore, in order to make those extra coarse positive\\ngrids have less impact, we put restrictions in the decoder,\\nso that the extra coarse positive grids cannot produce soft'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='tion. Therefore, in order to make those extra coarse positive\\ngrids have less impact, we put restrictions in the decoder,\\nso that the extra coarse positive grids cannot produce soft\\nlabel perfectly. The mechanism mentioned above allows\\nthe importance of ﬁne label and coarse label to be dynam-\\nically adjusted during the learning process, and makes the\\noptimizable upper bound of ﬁne label always higher than\\ncoarse label.\\n4.3. Other trainable bag-of-freebies\\nIn this section we will list some trainable bag-of-\\nfreebies. These freebies are some of the tricks we used\\nin training, but the original concepts were not proposed\\nby us. The training details of these freebies will be elab-\\norated in the Appendix, including (1) Batch normalization\\nin conv-bn-activation topology: This part mainly connects\\nbatch normalization layer directly to convolutional layer.\\nThe purpose of this is to integrate the mean and variance\\nof batch normalization into the bias and weight of convolu-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='batch normalization layer directly to convolutional layer.\\nThe purpose of this is to integrate the mean and variance\\nof batch normalization into the bias and weight of convolu-\\ntional layer at the inference stage. (2) Implicit knowledge\\nin YOLOR [81] combined with convolution feature map in\\naddition and multiplication manner: Implicit knowledge in\\nYOLOR can be simpliﬁed to a vector by pre-computing at\\nthe inference stage. This vector can be combined with the\\nbias and weight of the previous or subsequent convolutional\\nlayer. (3) EMA model: EMA is a technique used in mean\\nteacher [75], and in our system we use EMA model purely\\nas the ﬁnal inference model.\\n5. Experiments\\n5.1. Experimental setup\\nWe use Microsoft COCO dataset to conduct experiments\\nand validate our object detection method. All our experi-\\nments did not use pre-trained models. That is, all models\\nwere trained from scratch. During the development pro-\\ncess, we used train 2017 set for training, and then used val'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ments did not use pre-trained models. That is, all models\\nwere trained from scratch. During the development pro-\\ncess, we used train 2017 set for training, and then used val\\n2017 set for veriﬁcation and choosing hyperparameters. Fi-\\nnally, we show the performance of object detection on the\\ntest 2017 set and compare it with the state-of-the-art object\\ndetection algorithms. Detailed training parameter settings\\nare described in Appendix.\\nWe designed basic model for edge GPU, normal GPU,\\nand cloud GPU, and they are respectively called YOLOv7-\\ntiny, YOLOv7, and YOLOv7-W6. At the same time, we\\nalso use basic model for model scaling for different ser-\\nvice requirements and get different types of models. For\\nYOLOv7, we do stack scaling on neck, and use the pro-\\nposed compound scaling method to perform scaling-up of\\nthe depth and width of the entire model, and use this to ob-\\ntain YOLOv7-X. As for YOLOv7-W6, we use the newly\\nproposed compound scaling method to obtain YOLOv7-E6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='the depth and width of the entire model, and use this to ob-\\ntain YOLOv7-X. As for YOLOv7-W6, we use the newly\\nproposed compound scaling method to obtain YOLOv7-E6\\nand YOLOv7-D6. In addition, we use the proposed E-\\nELAN for YOLOv7-E6, and thereby complete YOLOv7-\\nE6E. Since YOLOv7-tiny is an edge GPU-oriented archi-\\ntecture, it will use leaky ReLU as activation function. As\\nfor other models we use SiLU as activation function. We\\nwill describe the scaling factor of each model in detail in\\nAppendix.\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Table 2: Comparison of state-of-the-art real-time object detectors.\\nModel #Param. FLOPs Size FPS APtest / APval APtest\\n50 APtest\\n75 APtest\\nS APtest\\nM APtest\\nL\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - - - - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - - - - -\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - - - - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - - - - -\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6% 23.2% 46.4% 56.9%\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0% 28.6% 52.9% 63.8%\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6% 31.4% 55.3% 66.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5% 33.3% 56.3% 66.4%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - - - - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - - - - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - - - - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - - - - -'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - - - - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - - - - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - - - - -\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - - - - -\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7% 31.7% 55.3% 64.7%\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9% 33.7% 57.1% 66.8%\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7% 18.8% 42.4% 51.9%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9% 31.8% 55.5% 65.0%\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8% 33.8% 57.1% 67.4%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - - - - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - - - - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - - - - -\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - - - - -\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - - - - -'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - - - - -\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - - - - -\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - - - - -\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9% 36.1% 57.7% 65.6%\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5% 37.7% 59.1% 67.1%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1% 38.4% 59.7% 67.7%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9% 38.9% 60.4% 68.7%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1% 37.3% 58.7% 67.1%\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2% 38.0% 59.9% 68.4%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8% 38.8% 60.1% 69.5%\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%\\n1 Our FLOPs is calaculated by rectangle input resolution like 640 × 640 or 1280 × 1280.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%\\n1 Our FLOPs is calaculated by rectangle input resolution like 640 × 640 or 1280 × 1280.\\n2 Our inference time is estimated by using letterbox resize input image to make its long side equals to 640 or 1280.\\n5.2. Baselines\\nWe choose previous version of YOLO [3, 79] and state-\\nof-the-art object detector YOLOR [81] as our baselines. Ta-\\nble 1 shows the comparison of our proposed YOLOv7 mod-\\nels and those baseline that are trained with the same settings.\\nFrom the results we see that if compared with YOLOv4,\\nYOLOv7 has 75% less parameters, 36% less computation,\\nand brings 1.5% higher AP. If compared with state-of-the-\\nart YOLOR-CSP, YOLOv7 has 43% fewer parameters, 15%\\nless computation, and 0.4% higher AP. In the performance\\nof tiny model, compared with YOLOv4-tiny-31, YOLOv7-\\ntiny reduces the number of parameters by 39% and the\\namount of computation by 49%, but maintains the same AP.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='of tiny model, compared with YOLOv4-tiny-31, YOLOv7-\\ntiny reduces the number of parameters by 39% and the\\namount of computation by 49%, but maintains the same AP.\\nOn the cloud GPU model, our model can still have a higher\\nAP while reducing the number of parameters by 19% and\\nthe amount of computation by 33%.\\n5.3. Comparison with state-of-the-arts\\nWe compare the proposed method with state-of-the-art\\nobject detectors for general GPUs and Mobile GPUs, and\\nthe results are shown in Table 2. From the results in\\nTable 2 we know that the proposed method has the best\\nspeed-accuracy trade-off comprehensively. If we compare\\nYOLOv7-tiny-SiLU with YOLOv5-N (r6.1), our method\\nis 127 fps faster and 10.7% more accurate on AP. In ad-\\ndition, YOLOv7 has 51.4% AP at frame rate of 161 fps,\\nwhile PPYOLOE-L with the same AP has only 78 fps frame\\nrate. In terms of parameter usage, YOLOv7 is 41% less than\\nPPYOLOE-L. If we compare YOLOv7-X with 114 fps in-\\nference speed to YOLOv5-L (r6.1) with 99 fps inference'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='rate. In terms of parameter usage, YOLOv7 is 41% less than\\nPPYOLOE-L. If we compare YOLOv7-X with 114 fps in-\\nference speed to YOLOv5-L (r6.1) with 99 fps inference\\nspeed, YOLOv7-X can improve AP by 3.9%. If YOLOv7-\\nX is compared with YOLOv5-X (r6.1) of similar scale, the\\ninference speed of YOLOv7-X is 31 fps faster. In addi-\\ntion, in terms of the amount of parameters and computation,\\nYOLOv7-X reduces 22% of parameters and 8% of compu-\\ntation compared to YOLOv5-X (r6.1), but improves AP by\\n2.2%.\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='If we compare YOLOv7 with YOLOR using the input\\nresolution 1280, the inference speed of YOLOv7-W6 is 8\\nfps faster than that of YOLOR-P6, and the detection rate is\\nalso increased by 1% AP. As for the comparison between\\nYOLOv7-E6 and YOLOv5-X6 (r6.1), the former has 0.9%\\nAP gain than the latter, 45% less parameters and 63% less\\ncomputation, and the inference speed is increased by 47%.\\nYOLOv7-D6 has close inference speed to YOLOR-E6, but\\nimproves AP by 0.8%. YOLOv7-E6E has close inference\\nspeed to YOLOR-D6, but improves AP by 0.3%.\\n5.4. Ablation study\\n5.4.1 Proposed compound scaling method\\nTable 3 shows the results obtained when using different\\nmodel scaling strategies for scaling up. Among them, our\\nproposed compound scaling method is to scale up the depth\\nof computational block by 1.5 times and the width of tran-\\nsition block by 1.25 times. If our method is compared with\\nthe method that only scaled up the width, our method can\\nimprove the AP by 0.5% with less parameters and amount'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='sition block by 1.25 times. If our method is compared with\\nthe method that only scaled up the width, our method can\\nimprove the AP by 0.5% with less parameters and amount\\nof computation. If our method is compared with the method\\nthat only scales up the depth, our method only needs to in-\\ncrease the number of parameters by 2.9% and the amount of\\ncomputation by 1.2%, which can improve the AP by 0.2%.\\nIt can be seen from the results of Table 3 that our proposed\\ncompound scaling strategy can utilize parameters and com-\\nputation more efﬁciently.\\nTable 3: Ablation study on proposed model scaling.\\nModel #Param. FLOPs Size AP val APval\\n50 APval\\n75\\nbase (v7-X light) 47.0M 125.5G 640 51.7% 70.1% 56.0%\\nwidth only (1.25 w) 73.4M 195.5G 640 52.4% 70.9% 57.1%\\ndepth only (2.0 d) 69.3M 187.6G 640 52.7% 70.8% 57.3%\\ncompound (v7-X) 71.3M 189.9G 640 52.9% 71.1% 57.5%\\nimprovement - - - +1.2 +1.0 +1.5\\n5.4.2 Proposed planned re-parameterized model\\nIn order to verify the generality of our proposed planed'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='compound (v7-X) 71.3M 189.9G 640 52.9% 71.1% 57.5%\\nimprovement - - - +1.2 +1.0 +1.5\\n5.4.2 Proposed planned re-parameterized model\\nIn order to verify the generality of our proposed planed\\nre-parameterized model, we use it on concatenation-based\\nmodel and residual-based model respectively for veriﬁca-\\ntion. The concatenation-based model and residual-based\\nmodel we chose for veriﬁcation are 3-stacked ELAN and\\nCSPDarknet, respectively.\\nIn the experiment of concatenation-based model, we re-\\nplace the 3 ×3 convolutional layers in different positions in\\n3-stacked ELAN with RepConv, and the detailed conﬁgura-\\ntion is shown in Figure 6. From the results shown in Table 4\\nwe see that all higher AP values are present on our proposed\\nplanned re-parameterized model.\\nIn the experiment dealing with residual-based model,\\nsince the original dark block does not have a 3 × 3 con-\\nFigure 6: Planned RepConv 3-stacked ELAN. Blue circles are the\\nposition we replace Conv by RepConv.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='since the original dark block does not have a 3 × 3 con-\\nFigure 6: Planned RepConv 3-stacked ELAN. Blue circles are the\\nposition we replace Conv by RepConv.\\nTable 4: Ablation study on planned RepConcatenation model.\\nModel AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nbase (3-S ELAN) 52.26% 70.41% 56.77% 35.81% 57.00% 67.59%\\nFigure 6 (a) 52.18% 70.34% 56.90% 35.71% 56.83% 67.51%\\nFigure 6 (b) 52.30% 70.30% 56.92% 35.76% 56.95% 67.74%\\nFigure 6 (c) 52.33% 70.56% 56.91% 35.90% 57.06% 67.50%\\nFigure 6 (d) 52.17% 70.32% 56.82% 35.33% 57.06% 68.09%\\nFigure 6 (e) 52.23% 70.20% 56.81% 35.34% 56.97% 66.88%\\nvolution block that conforms to our design strategy, we ad-\\nditionally design a reversed dark block for the experiment,\\nwhose architecture is shown in Figure 7. Since the CSP-\\nDarknet with dark block and reversed dark block has exactly\\nthe same amount of parameters and operations, it is fair to\\ncompare. The experiment results illustrated in Table 5 fully'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Darknet with dark block and reversed dark block has exactly\\nthe same amount of parameters and operations, it is fair to\\ncompare. The experiment results illustrated in Table 5 fully\\nconﬁrm that the proposed planned re-parameterized model\\nis equally effective on residual-based model. We ﬁnd that\\nthe design of RepCSPResNet [85] also ﬁt our design pat-\\ntern.\\nFigure 7: Reversed CSPDarknet. We reverse the position of 1 × 1\\nand 3 × 3 convolutional layer in dark block to ﬁt our planned re-\\nparameterized model design strategy.\\nTable 5: Ablation study on planned RepResidual model.\\nModel AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nbase (YOLOR-W6) 54.82% 72.39% 59.95% 39.68% 59.38% 68.30%\\nRepCSP 54.67% 72.50% 59.58% 40.22% 59.61% 67.87%\\nRCSP 54.36% 71.95% 59.54% 40.15% 59.02% 67.44%\\nRepRCSP 54.85% 72.51% 60.08% 40.53% 59.52% 68.06%\\nbase (YOLOR-CSP) 50.81% 69.47% 55.28% 33.74% 56.01% 65.38%\\nRepRCSP 50.91% 69.54% 55.55% 34.44% 55.74% 65.46%\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 8: Objectness map predicted by different methods at auxiliary head and lead head.\\n5.4.3 Proposed assistant loss for auxiliary head\\nIn the assistant loss for auxiliary head experiments, we com-\\npare the general independent label assignment for lead head\\nand auxiliary head methods, and we also compare the two\\nproposed lead guided label assignment methods. We show\\nall comparison results in Table 6. From the results listed in\\nTable 6, it is clear that any model that increases assistant\\nloss can signiﬁcantly improve the overall performance. In\\naddition, our proposed lead guided label assignment strat-\\negy receives better performance than the general indepen-\\ndent label assignment strategy in AP, AP 50, and AP75. As\\nfor our proposed coarse for assistant and ﬁne for lead label\\nassignment strategy, it results in best results in all cases. In\\nFigure 8 we show the objectness map predicted by different\\nmethods at auxiliary head and lead head. From Figure 8 we'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='assignment strategy, it results in best results in all cases. In\\nFigure 8 we show the objectness map predicted by different\\nmethods at auxiliary head and lead head. From Figure 8 we\\nﬁnd that if auxiliary head learns lead guided soft label, it\\nwill indeed help lead head to extract the residual informa-\\ntion from the consistant targets.\\nTable 6: Ablation study on proposed auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\nindependent 1280 55.8% 73.4% 60.9%\\nlead guided 1280 55.9% 73.5% 61.0%\\ncoarse-to-ﬁne lead guided 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nIn Table 7 we further analyze the effect of the proposed\\ncoarse-to-ﬁne lead guided label assignment method on the\\ndecoder of auxiliary head. That is, we compared the results\\nof with/without the introduction of upper bound constraint.\\nJudging from the numbers in the Table, the method of con-\\nstraining the upper bound of objectness by the distance from'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='of with/without the introduction of upper bound constraint.\\nJudging from the numbers in the Table, the method of con-\\nstraining the upper bound of objectness by the distance from\\nthe center of the object can achieve better performance.\\nTable 7: Ablation study on constrained auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\naux without constraint 1280 55.9% 73.5% 61.0%\\naux with constraint 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nSince the proposed YOLOv7 uses multiple pyramids to\\njointly predict object detection results, we can directly con-\\nnect auxiliary head to the pyramid in the middle layer for\\ntraining. This type of training can make up for informa-\\ntion that may be lost in the next level pyramid prediction.\\nFor the above reasons, we designed partial auxiliary head\\nin the proposed E-ELAN architecture. Our approach is to\\nconnect auxiliary head after one of the sets of feature map'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='For the above reasons, we designed partial auxiliary head\\nin the proposed E-ELAN architecture. Our approach is to\\nconnect auxiliary head after one of the sets of feature map\\nbefore merging cardinality, and this connection can make\\nthe weight of the newly generated set of feature map not\\ndirectly updated by assistant loss. Our design allows each\\npyramid of lead head to still get information from objects\\nwith different sizes. Table 8 shows the results obtained us-\\ning two different methods, i.e., coarse-to-ﬁne lead guided\\nand partial coarse-to-ﬁne lead guided methods. Obviously,\\nthe partial coarse-to-ﬁne lead guided method has a better\\nauxiliary effect.\\nTable 8: Ablation study on partial auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6E) 1280 56.3% 74.0% 61.5%\\naux 1280 56.5% 74.0% 61.6%\\npartial aux 1280 56.8% 74.4% 62.1%\\nimprovement - +0.5 +0.4 +0.6\\n6. Conclusions\\nIn this paper we propose a new architecture of real-\\ntime object detector and the corresponding model scaling'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='partial aux 1280 56.8% 74.4% 62.1%\\nimprovement - +0.5 +0.4 +0.6\\n6. Conclusions\\nIn this paper we propose a new architecture of real-\\ntime object detector and the corresponding model scaling\\nmethod. Furthermore, we ﬁnd that the evolving process\\nof object detection methods generates new research top-\\nics. During the research process, we found the replace-\\nment problem of re-parameterized module and the alloca-\\ntion problem of dynamic label assignment. To solve the\\nproblem, we propose the trainable bag-of-freebies method\\nto enhance the accuracy of object detection. Based on the\\nabove, we have developed the YOLOv7 series of object de-\\ntection systems, which receives the state-of-the-art results.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Table 9: More comparison (batch=1, no-TRT, without extra object detection training data)\\nModel #Param. FLOPs Size FPSV 100 APtest / APval APtest\\n50 APtest\\n75\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7%\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - -\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - -\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8%\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7%\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - -'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - -\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1%\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - -\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6%\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9%\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - -\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5%\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8%'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='YOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9%\\nF-RCNN-R101-FPN+ [5] 60.0M 246.0G 1333 20 - / 44.0% - -\\nDeformable DETR [100] 40.0M 173.0G - 19 - / 46.2% - -\\nSwin-B (C-M-RCNN) [52] 145.0M 982.0G 1333 11.6 - / 51.9% - -\\nDETR DC5-R101 [5] 60.0M 253.0G 1333 10 - / 44.9% - -\\nEfﬁcientDet-D7x [74] 77.0M 410.0G 1536 6.5 55.1% / 54.4% 72.4% 58.4%\\nDual-Swin-T (C-M-RCNN) [47] 113.8M 836.0G 1333 6.5 - / 53.6% - -\\nViT-Adapter-B [7] 122.0M 997.0G - 4.4 - / 50.8% - -\\nDual-Swin-B (HTC) [47] 235.0M - 1600 2.5 58.7% / 58.4% - -\\nDual-Swin-L (HTC) [47] 453.0M - 1600 1.5 59.4% / 59.1% - -\\nModel #Param. FLOPs Size FPSA100 APtest / APval APtest\\n50 APtest\\n75'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Dual-Swin-B (HTC) [47] 235.0M - 1600 2.5 58.7% / 58.4% - -\\nDual-Swin-L (HTC) [47] 453.0M - 1600 1.5 59.4% / 59.1% - -\\nModel #Param. FLOPs Size FPSA100 APtest / APval APtest\\n50 APtest\\n75\\nDN-Deformable-DETR [41] 48.0M 265.0G 1333 23.0 - / 48.6% - -\\nConvNeXt-B (C-M-RCNN) [53] - 964.0G 1280 11.5 - / 54.0% 73.1% 58.8%\\nSwin-B (C-M-RCNN) [52] - 982.0G 1280 10.7 - / 53.0% 71.8% 57.5%\\nDINO-5scale (R50) [89] 47.0M 860.0G 1333 10.0 - / 51.0% - -\\nConvNeXt-L (C-M-RCNN) [53] - 1354.0G 1280 10.0 - / 54.8% 73.8% 59.8%\\nSwin-L (C-M-RCNN) [52] - 1382.0G 1280 9.2 - / 53.9% 72.4% 58.8%\\nConvNeXt-XL (C-M-RCNN) [53] - 1898.0G 1280 8.6 - / 55.2% 74.2% 59.9%\\n8. More comparison\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS and\\nhas the highest accuracy 56.8% AP test-dev / 56.8% AP\\nmin-val among all known real-time object detectors with 30\\nFPS or higher on GPU V100. YOLOv7-E6 object detector\\n(56 FPS V100, 55.9% AP) outperforms both transformer-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='min-val among all known real-time object detectors with 30\\nFPS or higher on GPU V100. YOLOv7-E6 object detector\\n(56 FPS V100, 55.9% AP) outperforms both transformer-\\nbased detector SWIN-L Cascade-Mask R-CNN (9.2 FPS\\nA100, 53.9% AP) by 509% in speed and 2% in accuracy,\\nand convolutional-based detector ConvNeXt-XL Cascade-\\nMask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed\\nand 0.7% AP in accuracy, as well as YOLOv7 outperforms:\\nYOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, De-\\nformable DETR, DINO-5scale-R50, ViT-Adapter-B and\\nmany other object detectors in speed and accuracy. More\\nover, we train YOLOv7 only on MS COCO dataset from\\nscratch without using any other datasets or pre-trained\\nweights.\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Figure 9: Comparison with other object detectors.\\nFigure 10: Comparison with other real-time object detectors.\\nTable 10: Comparison of different setting.\\nModel Presicion IoU threshold AP val\\nYOLOv7-X FP16 (default) 0.65 (default) 52.9%\\nYOLOv7-X FP32 0.65 53.0%\\nYOLOv7-X FP16 0.70 53.0%\\nYOLOv7-X FP32 0.70 53.1%\\nimprovement - - +0.2%\\n* Similar to meituan/YOLOv6 and PPYOLOE, our model could\\nget higher AP when set higher IoU threshold.\\nThe maximum accuracy of the YOLOv7-E6E (56.8%\\nAP) real-time model is +13.7% AP higher than the cur-\\nrent most accurate meituan/YOLOv6-s model (43.1% AP)\\non COCO dataset. Our YOLOv7-tiny (35.2% AP, 0.4\\nms) model is +25% faster and +0.2% AP higher than\\nmeituan/YOLOv6-n (35.0% AP, 0.5 ms) under identical\\nconditions on COCO dataset and V100 GPU with batch=32.\\nFigure 11: Comparison with other real-time object detectors.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] anonymous. Designing network design strategies. anony-\\nmous submission, 2022. 3\\n[2] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus\\nCubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens,\\nand Barret Zoph. Revisiting ResNets: Improved training\\nand scaling strategies.Advances in Neural Information Pro-\\ncessing Systems (NeurIPS), 34, 2021. 2\\n[3] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020.\\n2, 6, 7\\n[4] Yue Cao, Thomas Andrew Geddes, Jean Yee Hwa Yang,\\nand Pengyi Yang. Ensemble deep learning in bioinformat-\\nics. Nature Machine Intelligence, 2(9):500–508, 2020. 2\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\\nEnd-to-end object detection with transformers. In Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV), pages 213–229, 2020. 10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='las Usunier, Alexander Kirillov, and Sergey Zagoruyko.\\nEnd-to-end object detection with transformers. In Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV), pages 213–229, 2020. 10\\n[6] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intel-\\nligence (TPAMI), 43(11):3782–3798, 2020. 2\\n[7] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\\ndense predictions. arXiv preprint arXiv:2205.08534, 2022.\\n10\\n[8] Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae\\nLee. Gaussian YOLOv3: An accurate and fast object detec-\\ntor using localization uncertainty for autonomous driving.\\nIn Proceedings of the IEEE/CVF International Conference\\non Computer Vision (ICCV), pages 502–511, 2019. 5\\n[9] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='on Computer Vision (ICCV), pages 502–511, 2019. 5\\n[9] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:\\nUnifying object detection heads with attentions. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 7373–7382, 2021.\\n2\\n[10] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi\\nHuang, Jungong Han, and Guiguang Ding. Re-\\nparameterizing your optimizers rather than architectures.\\narXiv preprint arXiv:2205.15242, 2022. 2\\n[11] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong\\nHan. ACNet: Strengthening the kernel skeletons for pow-\\nerful CNN via asymmetric convolution blocks. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV), pages 1911–1920, 2019. 2\\n[12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding. Diverse branch block: Building a con-\\nvolution as an inception-like unit. In Proceedings of the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='[12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding. Diverse branch block: Building a con-\\nvolution as an inception-like unit. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 10886–10895, 2021. 2\\n[13] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong\\nHan, Guiguang Ding, and Jian Sun. RepVGG: Making\\nVGG-style convnets great again. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 13733–13742, 2021. 2, 4\\n[14] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong\\nHan, Guiguang Ding, and Jian Sun. Scaling up your ker-\\nnels to 31x31: Revisiting large kernel design in CNNs. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), 2022. 2\\n[15] Piotr Doll ´ar, Mannat Singh, and Ross Girshick. Fast and\\naccurate model scaling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pages 924–932, 2021. 2, 3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='accurate model scaling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pages 924–932, 2021. 2, 3\\n[16] Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi\\nLin. Simple training strategies and model scaling for object\\ndetection. arXiv preprint arXiv:2107.00057, 2021. 2\\n[17] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV), pages 3490–3499,\\n2021. 2, 5\\n[18] Di Feng, Christian Haase-Sch ¨utz, Lars Rosenbaum, Heinz\\nHertlein, Claudius Glaeser, Fabian Timm, Werner Wies-\\nbeck, and Klaus Dietmayer. Deep multi-modal object de-\\ntection and semantic segmentation for autonomous driv-\\ning: Datasets, methods, and challenges. IEEE Transac-\\ntions on Intelligent Transportation Systems , 22(3):1341–\\n1360, 2020. 1\\n[19] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ing: Datasets, methods, and challenges. IEEE Transac-\\ntions on Intelligent Transportation Systems , 22(3):1341–\\n1360, 2020. 1\\n[19] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,\\nDmitry P Vetrov, and Andrew G Wilson. Loss sur-\\nfaces, mode connectivity, and fast ensembling of DNNs.\\nAdvances in Neural Information Processing Systems\\n(NeurIPS), 31, 2018. 2\\n[20] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and\\nJian Sun. OTA: Optimal transport assignment for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n303–312, 2021. 2, 5\\n[21] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430, 2021. 1, 2, 7, 10\\n[22] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. NAS-FPN:\\nLearning scalable feature pyramid architecture for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Learning scalable feature pyramid architecture for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7036–7045, 2019. 2\\n[23] Jocher Glenn. YOLOv5 release v6.1. https://github.com/\\nultralytics/yolov5/releases/tag/v6.1, 2022. 2, 7, 10\\n[24] Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Ex-\\npandNets: Linear over-parameterization to train compact\\nconvolutional networks. Advances in Neural Information\\nProcessing Systems (NeurIPS), 33:1298–1310, 2020. 2\\n[25] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\\nXu, and Chang Xu. GhostNet: More features from cheap\\noperations. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1580–1589, 2020. 1\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 770–778, 2016. 1, 4, 5\\n[27] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for Mo-\\nbileNetV3. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1314–1324, 2019. 1\\n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient con-\\nvolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861, 2017. 1\\n[29] Mu Hu, Junyi Feng, Jiashen Hua, Baisheng Lai, Jian-\\nqiang Huang, Xiaojin Gong, and Xiansheng Hua. On-\\nline convolutional re-parameterization. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2022. 2\\n[30] Miao Hu, Yali Li, Lu Fang, and Shengjin Wang. A 2-FPN:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2022. 2\\n[30] Miao Hu, Yali Li, Lu Fang, and Shengjin Wang. A 2-FPN:\\nAttention aggregation based feature pyramid network for\\ninstance segmentation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pages 15343–15352, 2021. 2\\n[31] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E\\nHopcroft, and Kilian Q Weinberger. Snapshot ensembles:\\nTrain 1, get m for free. International Conference on Learn-\\ning Representations (ICLR), 2017. 2\\n[32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 2, 4, 5\\n[33] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\\nDmitry Vetrov, and Andrew Gordon Wilson. Averaging\\nweights leads to wider optima and better generalization. In'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='4700–4708, 2017. 2, 4, 5\\n[33] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\\nDmitry Vetrov, and Andrew Gordon Wilson. Averaging\\nweights leads to wider optima and better generalization. In\\nConference on Uncertainty in Artiﬁcial Intelligence (UAI),\\n2018. 2\\n[34] Paul F Jaeger, Simon AA Kohl, Sebastian Bickel-\\nhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter\\nSchlemmer, and Klaus H Maier-Hein. Retina U-Net: Em-\\nbarrassingly simple exploitation of segmentation supervi-\\nsion for medical object detection. In Machine Learning for\\nHealth Workshop, pages 171–183, 2020. 1\\n[35] Hakan Karaoguz and Patric Jensfelt. Object detection ap-\\nproach for robot grasp detection. In IEEE International\\nConference on Robotics and Automation (ICRA) , pages\\n4953–4959, 2019. 1\\n[36] Kang Kim and Hee Seok Lee. Probabilistic anchor as-\\nsignment with iou prediction for object detection. In Pro-\\nceedings of the European conference on computer vision\\n(ECCV), pages 355–371, 2020. 5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='signment with iou prediction for object detection. In Pro-\\nceedings of the European conference on computer vision\\n(ECCV), pages 355–371, 2020. 5\\n[37] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\\nDoll´ar. Panoptic feature pyramid networks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 6399–6408, 2019. 2\\n[38] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Arti-\\nﬁcial Intelligence and Statistics, pages 562–570, 2015. 5\\n[39] Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok\\nBae, and Jongyoul Park. An energy and GPU-computation\\nefﬁcient backbone network for real-time object detection.\\nIn Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition Workshops (CVPRW),\\npages 0–0, 2019. 2, 3\\n[40] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and\\nXiaogang Wang. GS3D: An efﬁcient 3d object detection'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='puter Vision and Pattern Recognition Workshops (CVPRW),\\npages 0–0, 2019. 2, 3\\n[40] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and\\nXiaogang Wang. GS3D: An efﬁcient 3d object detection\\nframework for autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 1019–1028, 2019. 1\\n[41] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M\\nNi, and Lei Zhang. DN-DETR: Accelerate detr training\\nby introducing query denoising. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 13619–13627, 2022. 10\\n[42] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 9387–\\n9396, 2022. 2, 5\\n[43] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,\\nand Jian Yang. Generalized focal loss v2: Learning reliable'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='puter Vision and Pattern Recognition (CVPR), pages 9387–\\n9396, 2022. 2, 5\\n[43] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,\\nand Jian Yang. Generalized focal loss v2: Learning reliable\\nlocalization quality estimation for dense object detection. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), pages 11632–11641,\\n2021. 5\\n[44] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin\\nHu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal\\nloss: Learning qualiﬁed and distributed bounding boxes for\\ndense object detection. Advances in Neural Information\\nProcessing Systems (NeurIPS), 33:21002–21012, 2020. 5\\n[45] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\\nExploring plain vision transformer backbones for object de-\\ntection. arXiv preprint arXiv:2203.16527, 2022. 2\\n[46] Zhuoling Li, Minghui Dong, Shiping Wen, Xiang Hu, Pan\\nZhou, and Zhigang Zeng. CLU-CNNs: Object detection for\\nmedical images. Neurocomputing, 350:53–59, 2019. 1'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='[46] Zhuoling Li, Minghui Dong, Shiping Wen, Xiang Hu, Pan\\nZhou, and Zhigang Zeng. CLU-CNNs: Object detection for\\nmedical images. Neurocomputing, 350:53–59, 2019. 1\\n[47] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNetV2: A composite backbone network architecture for ob-\\nject detection. arXiv preprint arXiv:2107.00420, 2021. 5,\\n10\\n[48] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song\\nHan. Memory-efﬁcient patch-based inference for tiny deep\\nlearning. Advances in Neural Information Processing Sys-\\ntems (NeurIPS), 34:2346–2358, 2021. 1\\n[49] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song\\nHan, et al. MCUNet: Tiny deep learning on IoT de-\\nvices. Advances in Neural Information Processing Systems\\n(NeurIPS), 33:11711–11722, 2020. 1\\n[50] Yuxuan Liu, Lujia Wang, and Ming Liu. YOLOStereo3D:\\nA step back to 2D for efﬁcient stereo 3D detection. In\\nIEEE International Conference on Robotics and Automa-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='[50] Yuxuan Liu, Lujia Wang, and Ming Liu. YOLOStereo3D:\\nA step back to 2D for efﬁcient stereo 3D detection. In\\nIEEE International Conference on Robotics and Automa-\\ntion (ICRA), pages 13018–13024, 2021. 5\\n[51] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='et al. Swin transformer v2: Scaling up capacity and res-\\nolution. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\\n[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV), pages 10012–10022, 2021. 10\\n[53] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for\\nthe 2020s. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n11976–11986, 2022. 10\\n[54] Rangi Lyu. NanoDet-Plus. https://github.com/RangiLyu/\\nnanodet/releases/tag/v1.0.0-alpha-1, 2021. 1, 2\\n[55] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian\\nSun. ShufﬂeNet V2: Practical guidelines for efﬁcient CNN\\narchitecture design. In Proceedings of the European Con-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='[55] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian\\nSun. ShufﬂeNet V2: Practical guidelines for efﬁcient CNN\\narchitecture design. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 116–131, 2018.\\n1, 3\\n[56] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unifying\\nclassiﬁcation and localisation in object detection. Advances\\nin Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 2\\n[57] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and in-\\nstance segmentation. In Proceedings of the IEEE/CVF In-\\nternational Conference on Computer Vision (ICCV), pages\\n3009–3018, 2021. 2\\n[58] Shuvo Kumar Paul, Muhammed Tawﬁq Chowdhury,\\nMircea Nicolescu, Monica Nicolescu, and David Feil-\\nSeifer. Object detection and pose estimation from rgb and\\ndepth data for real-time, adaptive robotic grasping. In Ad-\\nvances in Computer Vision and Computational Biology ,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Seifer. Object detection and pose estimation from rgb and\\ndepth data for real-time, adaptive robotic grasping. In Ad-\\nvances in Computer Vision and Computational Biology ,\\npages 121–142. 2021. 1\\n[59] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. De-\\ntectoRS: Detecting objects with recursive feature pyramid\\nand switchable atrous convolution. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 10213–10224, 2021. 2\\n[60] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n10428–10436, 2020. 2\\n[61] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Uniﬁed, real-time object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n779–788, 2016. 2, 5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Farhadi. You only look once: Uniﬁed, real-time object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n779–788, 2016. 2, 5\\n[62] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 2\\n[63] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767, 2018. 1, 2\\n[64] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding\\nbox regression. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 658–666, 2019. 2\\n[65] Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and\\nSaehoon Kim. Sparse DETR: Efﬁcient end-to-end ob-\\nject detection with learnable sparsity. arXiv preprint\\narXiv:2111.14330, 2021. 5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='[65] Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and\\nSaehoon Kim. Sparse DETR: Efﬁcient end-to-end ob-\\nject detection with learnable sparsity. arXiv preprint\\narXiv:2111.14330, 2021. 5\\n[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey\\nZhmoginov, and Liang-Chieh Chen. MobileNetV2: In-\\nverted residuals and linear bottlenecks. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 4510–4520, 2018. 1\\n[67] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection\\nfrom scratch with deep supervision. IEEE Transactions\\non Pattern Analysis and Machine Intelligence (TPAMI) ,\\n42(2):398–412, 2019. 5\\n[68] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556, 2014. 4\\n[69] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:1409.1556, 2014. 4\\n[69] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\\nChanghu Wang, et al. Sparse R-CNN: End-to-end ob-\\nject detection with learnable proposals. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 14454–14463, 2021. 2\\n[70] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 1–9, 2015. 5\\n[71] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception\\narchitecture for computer vision. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 2818–2826, 2016. 2\\n[72] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 2818–2826, 2016. 2\\n[72] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking\\nmodel scaling for convolutional neural networks. In Inter-\\nnational Conference on Machine Learning (ICML) , pages\\n6105–6114, 2019. 2, 3\\n[73] Mingxing Tan and Quoc Le. EfﬁcientNetv2: Smaller mod-\\nels and faster training. In International Conference on Ma-\\nchine Learning (ICML), pages 10096–10106, 2021. 2\\n[74] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcient-\\nDet: Scalable and efﬁcient object detection. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 10781–10790, 2020. 2, 10\\n[75] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. Advances in Neural\\nInformation Processing Systems (NeurIPS), 30, 2017. 2, 6\\n[76] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='semi-supervised deep learning results. Advances in Neural\\nInformation Processing Systems (NeurIPS), 30, 2017. 2, 6\\n[76] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. InProceed-\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='ings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV), pages 9627–9636, 2019. 2\\n[77] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI), 44(4):1922–1933, 2022. 2\\n[78] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff\\nZhu, Oncel Tuzel, and Anurag Ranjan. An im-\\nproved one millisecond mobile backbone. arXiv preprint\\narXiv:2206.04040, 2022. 2\\n[79] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 13029–13038, 2021. 2, 3, 6, 7\\n[80] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSP-\\nNet: A new backbone that can enhance learning capabil-\\nity of CNN. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition Workshops'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='Net: A new backbone that can enhance learning capabil-\\nity of CNN. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition Workshops\\n(CVPRW), pages 390–391, 2020. 1\\n[81] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Uniﬁed network for\\nmultiple tasks. arXiv preprint arXiv:2105.04206, 2021. 1,\\n2, 6, 7, 10\\n[82] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 15849–15858, 2021. 2, 5\\n[83] Bichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai,\\nPeizhao Zhang, Matthew Yu, Jialiang Wang, Yingyan Lin,\\nand Peter Vajda. FBNetv5: Neural architecture search for\\nmultiple tasks in one run.arXiv preprint arXiv:2111.10007,\\n2021. 1\\n[84] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin,\\nGabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='multiple tasks in one run.arXiv preprint arXiv:2111.10007,\\n2021. 1\\n[84] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin,\\nGabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans,\\nMingxing Tan, Vikas Singh, and Bo Chen. MobileDets:\\nSearching for object detection architectures for mobile ac-\\ncelerators. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n3825–3834, 2021. 1\\n[85] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao\\nChang, Cheng Cui, Kaipeng Deng, Guanzhong Wang,\\nQingqing Dang, Shengyu Wei, Yuning Du, et al. PP-\\nYOLOE: An evolved version of YOLO. arXiv preprint\\narXiv:2203.16250, 2022. 2, 7, 8, 10\\n[86] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam.\\n3D-MAN: 3D multi-frame attention network for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1863–1872, 2021. 5\\n[87] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='tection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1863–1872, 2021. 5\\n[87] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor\\nDarrell. Deep layer aggregation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 2403–2412, 2018. 1\\n[88] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng\\nCui, Wei Ji, Qingqing Dang, Kaipeng Deng, Guanzhong\\nWang, Yuning Du, et al. PP-PicoDet: A better real-\\ntime object detector on mobile devices. arXiv preprint\\narXiv:2111.00902, 2021. 1\\n[89] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\\nZhu, Lionel M Ni, and Heung-Yeung Shum. DINO: DETR\\nwith improved denoising anchor boxes for end-to-end ob-\\nject detection. arXiv preprint arXiv:2203.03605, 2022. 10\\n[90] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-\\nderhauf. VarifocalNet: An IoU-aware dense object detector.\\nIn Proceedings of the IEEE/CVF Conference on Computer'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='[90] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-\\nderhauf. VarifocalNet: An IoU-aware dense object detector.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), pages 8514–8523,\\n2021. 5\\n[91] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z Li. Bridging the gap between anchor-based and\\nanchor-free detection via adaptive training sample selec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 9759–\\n9768, 2020. 5\\n[92] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian\\nSun. ShufﬂeNet: An extremely efﬁcient convolutional neu-\\nral network for mobile devices. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 6848–6856, 2018. 1\\n[93] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan\\nYuan, Ping Luo, Wenyu Liu, and Xinggang Wang. BYTE-\\nTrack: Multi-object tracking by associating every detection'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='[93] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan\\nYuan, Ping Luo, Wenyu Liu, and Xinggang Wang. BYTE-\\nTrack: Multi-object tracking by associating every detection\\nbox. arXiv preprint arXiv:2110.06864, 2021. 1\\n[94] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. FAIRMOT: On the fairness of detec-\\ntion and re-identiﬁcation in multiple object tracking. Inter-\\nnational Journal of Computer Vision, 129(11):3069–3087,\\n2021. 1\\n[95] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\\nvolume 34, pages 12993–13000, 2020. 2\\n[96] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV), pages 85–94, 2019. 2\\n[97] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Ob-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdf_files/file_1.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'file_1.pdf', 'file_type': 'pdf'}, page_content='object detection. In International Conference on 3D Vision\\n(3DV), pages 85–94, 2019. 2\\n[97] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Ob-\\njects as points. arXiv preprint arXiv:1904.07850, 2019. 1,\\n2\\n[98] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima\\nTajbakhsh, and Jianming Liang. UNet++: A nested U-\\nNet architecture for medical image segmentation. In\\nDeep Learning in Medical Image Analysis and Multimodal\\nLearning for Clinical Decision Support, 2018. 5\\n[99] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496, 2020. 2, 5\\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable DETR: Deformable trans-\\nformers for end-to-end object detection. In Proceedings of\\nthe International Conference on Learning Representations\\n(ICLR), 2021. 10\\n15')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Embedding model\n",
        "emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": device},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# Build the FAISS vector store from your `chunks`\n",
        "vs = FAISS.from_documents(chunks, emb)\n",
        "\n",
        "# Save the index to disk\n",
        "index_dir = \"faiss_index\"\n",
        "vs.save_local(index_dir)\n",
        "print(f\"✅ Saved FAISS index to: {index_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_GKypV-VbTI",
        "outputId": "43da6a02-ccdb-4f8b-b0dc-cf58eba3c667"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2769724696.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  emb = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved FAISS index to: faiss_index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload\n",
        "vs = FAISS.load_local(\"faiss_index\", emb, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Query the vector DB\n",
        "query = \"What are the key contributions of YOLOv10?\"\n",
        "results = vs.similarity_search(query, k=4)  # returns top-4 chunks\n",
        "\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n[{i}] {doc.metadata.get('source_file')} • page {doc.metadata.get('page')}\")\n",
        "    print(doc.page_content[:400], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGILTXJxYpfR",
        "outputId": "35107bb4-0b27-4ab4-efc2-e8115066f5cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1] file_2.pdf • page 20\n",
            "medical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these\n",
            "fields and improve the efficiency. However, we acknowledge the potential for malicious use of our\n",
            "models. We will make every effort to prevent this.\n",
            "21 ...\n",
            "\n",
            "[2] file_2.pdf • page 6\n",
            "YOLOv10 models. YOLOv10 has the same variants as YOLOv8, i.e., N / S / M / L / X. Besides, we\n",
            "derive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We\n",
            "verify the proposed detector on COCO [35] under the same training-from-scratch setting [21, 65, 62].\n",
            "Moreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following [78].\n",
            "4.2 Comparison wi ...\n",
            "\n",
            "[3] file_2.pdf • page 6\n",
            "YOLOv10-L (Ours) 24.4 120.3 53.2 / 53.4† 7.28 7.21\n",
            "YOLOv8-X [21] 68.2 257.8 53.9 16.86 12.83\n",
            "RT-DETR-R101 [78] 76.0 259.0 54.3 13.71 13.58\n",
            "YOLOv10-X (Ours) 29.5 160.4 54.4 / 54.4† 10.70 10.60\n",
            "quadratic computational complexity of self-attention. In this way, the global representation learning\n",
            "ability can be incorporated into YOLOs with low computational costs, which well enhances the\n",
            "model’s capab ...\n",
            "\n",
            "[4] file_2.pdf • page 20\n",
            "Limitation. Due to the limited computational resources, we do not investigate the pretraining\n",
            "of YOLOv10 on large-scale datasets, e.g., Objects365 [ 52]. Besides, although we can achieve\n",
            "competitive end-to-end performance using the one-to-one head under NMS-free training, there still\n",
            "exists a performance gap compared with the original one-to-many training using NMS, especially\n",
            "noticeable in small  ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key =  userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(temperature = 0.0, model= \"gpt-3.5-turbo\", openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "p65dgRaQYpio"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)"
      ],
      "metadata": {
        "id": "YLDBpwVpYplz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(query, vs, llm, k=4, query_prefix=\"\", max_chars=30000, per_doc_limit=6000):\n",
        "    \"\"\"\n",
        "    llm: gemma2-9b-it\n",
        "    \"\"\"\n",
        "    # 1) Retrieve\n",
        "    docs = vs.similarity_search(query_prefix + query, k=k)\n",
        "\n",
        "    # 2) Build compact context\n",
        "    context = \"\\n\\n---\\n\\n\".join(\n",
        "        (d.page_content or \"\").strip() for d in docs if d.page_content\n",
        "    )\n",
        "\n",
        "    # 3) Ask Groq LLM\n",
        "    prompt = textwrap.dedent(f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer\n",
        "    \"\"\").strip()\n",
        "\n",
        "    resp = llm.invoke(prompt)\n",
        "    return (getattr(resp, \"content\", None) or str(resp)).strip()\n",
        "\n",
        "# Question\n",
        "q = \"Provide me an comparison between YOLOv7 and YOLOv10\"\n",
        "print(rag_answer(q, vs, llm, k=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3Pq8r9iYppB",
        "outputId": "e37b9c7a-6cd5-4f0a-b6a3-6317a38abe44"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv7 has 75% less parameters, 36% less computation, and brings 1.5% higher AP compared to YOLOv4. In contrast, YOLOv10 achieves 1.2% to 1.4% AP improvements with 28% to 57% fewer parameters, 23% to 38% less calculations, and 37% to 70% lower latencies compared to YOLOv8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSagOLIGYpuV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}